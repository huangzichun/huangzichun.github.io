---
layout:     post
title:      贝叶斯机器学习入坑指南
subtitle:   又是一个磨人的笔记
date:       2021-12-20
author:     HC
header-img: img/post-sample-image.jpg
catalog: true
tags:
    - 贝叶斯机器学习笔记
    - Bayesian Machine Learning
    - Bayesian Decision Theory
    - Bayesian Model Selection
    - Bayesian Logistic Regression
---



# 0x00 说在前面的话

入坑贝叶斯机器学习，说不定之后会用到，特地来总结一下学习笔记。



# 0x01 贝叶斯机器学习的基本组件

贝叶斯机器学习最基本的组件有四个，包括似然（Likelihood）、先验（Prior）、后验（Posterior）和推理（Inference）。

- **似然 $P(D\|\theta)$**：表示在给定参数$\theta$的情况下，描述已观测数据的生成机制。在这组$\theta$参数下，生成当前的观测数据的可能性有多大
- **先验$P(\theta)$**：表示对参数$\theta$的初始知识，是在观测到数据之前的参数分布。比如我们会认为掷硬币正面向上的概率是1/2
- **后验$P(\theta\|D)$**：表示在给定观测数据后，通过数据得到了更多关于参数的知识，是参数的事后分布。通常用贝叶斯公式计算得到
- **推断（Inference）**：一般表示用后验分布进行的分析，包括
  - 点估计：最"好"的一个$\theta$值是多少 （好的定义由loss function决定）
  - 做预测："遍历"参数的后验分布取值，模型在不同参数下的预测结果的均值
  - 做决策：能最小化后验损失的参数是多少
  - 贝叶斯模型选择（Bayesian Model Comparison）
  - 等等的



# 0x02 贝叶斯机器学习都在干啥

## 2.1 难在哪里

贝叶斯机器学习的关注对象是先验分布、似然和后验分布，其中

1. **难点一：先验分布如何选择**

   > 参数的先验知识从何而来？选哪种分布更方面我们计算后验分布？

2. **难点二：参数后验分布太难解**

   > 贝叶斯公式用来求后验分布的时候，分布的积分太难求解了
   >
   >  
   > $$
   > P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}= \frac{P(D|\theta)P(\theta)}{\int P(D|\theta)P(\theta) d\theta}
   > $$
   >  
   >
   > 



## 2.2 推理算法

贝叶斯机器学习为了要用后验分布，就需要解决积分问题。所以，贝叶斯机器学习干的事之一就是参数后验分布的求解啦。至今，已经有很多的算法来求解，包括

1. **MAP（Maximum A Posterior，最大后验估计）**：用点估计来近似，将积分问题，转化成求最大值的问题
2. **Gibbs采样/MCMC**：一种基于蒙特卡洛采样的算法，用序列化采样得到的样本来近似估计后验分布
3. **变分推断（Variational Inference）**：旨在将复杂的后验分布，用另一个简单的分布来近似，并用这个近似分布来代替原后验分布。



## 2.3 应用例子

贝叶斯机器学习在求解了参数后验分布之后，就开始了一大推的应用，包括

1. 常见模型：Factor Analysis / HMM / Bayesian Linear regression / Bayesian nets / Latent dirichlet allocation / NMF / probabilistic latent semantic analysis / Linear dynamical system / sparse coding / ICA等等
2. 贝叶斯无参模型（Bayesian Nonparametrics，不是没有参数，而是无穷参数）：Gaussian Process / Dirichlet process / hierarchical Dirichlet process / Indian buffet process / IBP linear-Gaussian model / beta process / Dirichlet diffusion trees / Pitman-Yor process
3. 模型选择：BIC / Laplace Approximation
4. 采样算法 与 变分推断
5. 信念传播（Belief propagation）
6. 等等

 

# 0x03 贝叶斯决策理论 (Decision Theory)

在贝叶斯决策理论的体系里，也有一些基本组件，包括下面几点。（是不是有强化学习的味道了！）

- **动作空间Action Space** $A$：一个动作的取值范围。可以类比强化学习里的策略网络的取值范围。对于点估计来说，动作空间就等于参数空间，是从参数空间里选一个参数，使得它是真实参数的最好估计值。
- **参数/状态空间 Parameter / State Space**：顾名思义，不解释了
- **采样空间 Sample Space**：采样空间是指观测数据的分布，观测数据来自于这个采样空间
- **决策规则 Decision Rule**：决策规则可以看成是强化学习里的策略，给定观测数据，输出一个动作。
- **损失函数 Loss Function ** $L(\theta, a)$：用于衡量当真实参数给定后，一个动作的好坏程度
- **似然函数 Likelihood Function** $P(D\|\theta)$：似然函数是观测数据到参数空间的连接函数，衡量在给定的参数下，生成观测数据的可能性。

在给定这些基础组件之后，**贝叶斯决策理论**就想找一个最优的决策规则 $\delta ^\*$，使得在给定观测数据$D$后，能从动作空间$A$中选出一个动作$a$，这个动作能最小化**后验经验损失**（Posterior Expected Loss, $\rho$）。这里对“大小”的衡量、动作的好坏的衡量，都是由Loss Function 控制的




$$
\rho(P(\theta\|D), a) = E[L(\theta, a) \| D] = \int_\theta L(\theta, a) P(\theta\|D) d\theta \\
\delta^\*(D) = \arg\min_{a \in A} \rho(P(\theta\|D), a) = \arg\min_{a \in A} \int_\theta L(\theta, a) P(\theta\|D) d\theta
$$




- 这样能最小化Posterior Expected Loss的动作就叫做 **Bayes Action**
- 一个总是能在给定观测数据下选择出Bayes Action的规则就叫 **Bayes Rule**
- 如果参数先验$P(\theta)$不是一个概率分布（学名叫 Improper），这时候叫 **Generalized Bayes Rule**
- 在这个定义下的点估计（动作空间=参数空间）就旨在找一个Loss function和决策规则，使得在每个参数下都能最小化Posterior Expected Loss
- 如果一个Bayes Rule，对每一个可能的观测集合都能选择出Bayes Action，最小化Posterior Expected Loss，那么这个Bayes Rule就叫**Bayes Estimator**。

  - 对于平方损失的情况下，Bayes Estimator是参数分布的后验均值
  - 对于绝对值损失的情况下，Bayes Estimator是后验中位数
  - 对于relaxed 0–1损失的情况下，Bayes Estimator近似是 maximum a posteriori (map) estimate（这就是为啥可以用MAP的原因，因为optimization比积分好算）



**eg. 平方损失**


$$
\rho(P(\theta\|D), \hat{\theta}) = E[L(\theta, \hat{\theta}) \| D] = \int (\theta-\hat{\theta})^2 P(\theta\|D) d\theta\\
= \int \theta^2 P(\theta\|D) d\theta - 2\hat{\theta} \int \theta  P(\theta\|D) d\theta + \hat{\theta}^2 \int P(\theta\|D) d\theta\\
= \int \theta^2 P(\theta\|D) d\theta - 2\hat{\theta} \int \theta  P(\theta\|D) d\theta + \hat{\theta}^2
$$


求导可以得到$\hat{\theta}=E(\theta\|D)$



# 0x04 贝叶斯模型选择



# 0x05 Bayesian Logistic Regression



# 0x06 Gaussian Process Regression

[高斯过程回归算法在这里](http://huangc.top/2018/03/11/Gaussian-Process-Regression-2018/)

# 0x07 Bayesian Quadrature



# 0x08 Gaussian Process Classification



# 0x09 Bayesian Optimization



# 0x10 Sampling



[采样算法在这里](http://huangc.top/2019/03/24/sampling-2019/)



# 0x11 Variational Inference

[变分推断算法在这里](http://huangc.top/2022/01/03/VI-2022/)



# 0x12 Dynamical Systems





