---
layout:     post
title:      Reinforcement Learning学习笔记（中）
subtitle:   入坑指南
date:       2018-05-12
author:     HC
header-img: img/post-bg-swift2.jpg
catalog: true
tags:
    - Reinforcement Learning
    - MDP
    - Policy
---



# 10. Model-based RL

上一章说的是Model-based Planning，即结合已知的dynamics，做出更好的value function的估计和更好的决策。本章的Model-based RL 是指，dynamics未知（本文特指状态转移，好像现在对reward function都不会怎么学习？reward都是专业知识？），但是我们要去学习它！学习到之后，我们又可以做planning啦。model-based RL和model-free RL的区别就是前者是去学习dynamics，然后获得value function或者policy，后者是直接去学习value function或者policy，而不依赖dynamics。所以model-based RL可以用下面的图表示，这里的地球是一个卡通图案，表示他是我们通过样本学习到的dynamics，然后整个RL过程就在这样的已知的dynamics中进行。

![](/img/rl3.png)

下面是一个model-based RL的流程图：一开始我们初始化一定的policy，用他和环境交互，获得经验数据；在使用这个经验数据学习出dynamics。当dynamics已知的时候，我们就可以planning出更好的policy。比如使用动态规划planning（policy iteration, value iteration, MC, TD等等），Stochastic optimization，MCTS等model-based planning的方法。但是，这种相互依赖的loop，一好则好，一旦学习有猜错，那就开始累计了。实际效果会限制于所学习的model/dynamics的好坏。因此，如果model学得太烂，完全就可以用model-free等等的方式

![](/img/rl4.png)

## 10.1 Planning with learned dynamics

从上面的示意图，只能看出model-based RL由三个步骤组成：

1. 用当前的policy/value，出招，得到一些反馈数据
2. 根据反馈数据，学习出一个dynamics $f$
3. 根据$f$来planning，优化policy/value

在实际操作中，会有一些其他问题：训练数据是否足够用来学习，是否能学习到一个好的模型？

### 10.1.1 Distribution mismatch

训练数据是源于真实的MDP，但是训练数据是否能很好的反应MDP，这是未知的。比如在自动驾驶问题中，如果车子一直都在左转弯绕圈，那么用这样的数据所学习的planning，不就只能左转弯了么。实际上训练数据（左转绕圈）和真实驾车场景是不一样的。如果我们用于学习dynamics的算法很厉害，拟合能力很强，那么这个问题就会更加突出，因为学到的dynamics overfits，建立在它之上的planning也就不会好。

解决这个问题的一个可行方案就是扩充训练数据集，竟可能的去避免mismatch。如下所示，在第4步增加数据扩充的步骤。

![](/img/rl8.png)

这个策略和模仿学习(imitation learning)中的DAgger算法（Dataset Aggregation）很相似（伪代码如下），对于DAgger算法的dabug就是要人来打标签，选动作。而在强化学习中，则是直接使用当前policy的动作值当label，勉强人工的过程。

![](/img/rl9.png)

但是，这个扩充数据集的方法就不适用于open loop plan了，毕竟没有交互，就没机会得到新数据。

### 10.1.2 Model error

另一个问题就是，真实的dynamics我们是不知道的。通过学习出来的dynamics很有可能是不准确的，甚至是有大bug的。比如在自动驾驶问题中，如果dynamics学得不好，在给定状态下做的planning，也许会给出错误的答案：如果学习到的dynamics告诉你前面是个左转弯（实际上是直路），那么planning会告诉你赶紧左转，但是一旦左转，那就会冲出公路咯。

![](/img/rl10.png)

这种情况下，除了数据扩充的方法外，还可以re-planning。在一段时间之后的新状态下，进行重新planning，一旦re-planing的结果发现不能再左转了（即使学习到的dynamics告诉你应该要左转），这个时候还是有改正的机会。

![](/img/rl11.png)

下面代码的第3就是re-planning（注意，之前的代码是每N次，更新一次dynamics，然后做一次planning，这里是每N次，更新一次dynamics，但是planning是做了N次的）

![](/img/rl12.png)

但是这里re-planning带来的计算量不会太恶心，因为如果每次都在做planning的话，其实不用去plan那些时间片很久以后的事，往往plan一个短期的future就很work了，这时候就可以使用上一章提到的Stochastic optimization方法了。

## 10.2 Learn a model by supervised learning

为了要学习dynamics（这里特指状态转移和reward function），可以求助由于监督学习。给定训练数据$\{s_{t},a_{t},r_{t+1},s_{t+1}\}^n$，我们要求解的是$s_{t+1}=P(S_{t+1}\mid S_{t},A_{t})$和$r_{t+1}=R(R_{t+1}\mid S_{t},A_{t})$。这时候监督学习学习的目标不在是label，而是输入$s_{t},a_{t}$，输出$s_{t+1}$和$r_{t+1}$。回想最开始我们提到的动态系统，就可以用来解他们，而且还是考虑到了数据之间的依赖情况。当然，我们也可以把这种依赖情况忽略，之间采用简单的模型去拟合：

$$
s_1,a_1 \rightarrow r_2,s_2\\
s_2,a_2 \rightarrow r_3,s_3\\
...\\
s_{T-1},a_{T-1} \rightarrow r_{T},s_{T}\\
$$

比如用回归问题来集合reward，用密度估计来拟合state。总的来说，可以用local和global两个大类的模型来拟合（比如下列）。global模型是指用一个模型来拟合所有的state和action的输出，local模型则是用多个局部的小模型来拟合特定状态和action下的输出。

1. Table Lookup Model
2. Linear Expectation model
3. linear gaussian
4. gaussian process
5. deep belief network
6. iLQR等等

### 10.2.1 Fitting global dynamic model

#### 10.2.1.1 Table Lookup Model

table lookup其实就是通过简单的数据统计，记录所有相同输入下的输出均值。可以将它视作一个全空间下的KNN回归模型，这个K等于数据点的个数。

$$
\widehat{P}_{ss'}^a=\frac{1}{N(s,a)}\sum_{t=1}^n1(S_t,A_t,S_{t+1}=s,a,s')\\
\widehat{R}_{s}^a=\frac{1}{N(s,a)}\sum_{t=1}^n1(S_t,A_t=s,a)R_t
$$

当然，这种基于表格的方式，仅仅适用于离散且维度不大的情况。并且模型的泛化能力依赖于训练数据，如果，缺少某个状态下的训练数据，那么模型肯定是学习不到这个状态的。在学习出model之后，就可以使用planning了。在model-based RL中还有一个特别的planning方法，叫做Sample-based Planning。这种方法简单有效。它先通过训练数据学习出一个模型，然后使用这个模型再模拟/采用出更多的样本，最后再使用model-free的方法（比如Q-learning, sarsa, MC等等）求出policy或者value function。比如给定state空间是{A,B}和训练episode数据（state, reward）：

$$
A,0,B,0\\
B,1\\
B,1\\
B,1\\
B,1\\
B,1\\
B,1\\
B,0
$$

我们可以通过table lookup的方法，得到下面的dynamics

![](/img/rl5.png)

那么，如果使用sample-based planning方法，我们需要对这个dynamics就行采样，比如得到一些采样episode：

$$
B,1\\
B,0\\
B,1\\
A,0,B,1\\
B,1\\
A,0,B,1\\
B,1\\
B,0
$$

最后，我们便可以在采样数据上使用model-free的方法学习policy或者value了，比如使用MC的方法（用episode的return来近似）可以得到$V(A)=1,V(B)=0.75$。一个随之而来的问题就是，采样数据不一定可靠（取决于所学习到的model），那为什么结合训练数据一起使用呢？可以的，这就是Dyna算法

#### 10.2.1.2 Dyna

既然我们有了来源于真实MDP的数据和源于学习到的MDP的数据，那么自然我们可以把他们合起来，学习一个更好的policy或者value function。Dyna就是这样的算法，他的思想可以用下面的图表示：

![](/img/rl6.png)

Dyna如果建立上sample-based planning方法上，那么采样的数据将用于planning，这里planning选用的是model-free的q-learning，真实的数据也是通过Q-learning来更新value function。具体来说dyna的伪代码如下：

![](/img/rl7.png)

第d步用真实数据更新value，e步学习dynamics，f步则是planning。Dyna的步骤很简单，而且效果也很好

### 10.2.2 Fitting local dynamic model

要找到一个global model来很好的拟合所有state和action是非常困难的，这样学到的模型本身也是非常复杂的，对于有效task来说，dynamics本身会比policy要复杂。如果要用RL来做红绿灯控制，那么去学习该路口（和其附近）的路况，一定比学习我国所有道路的路况，要容易而且有效很多。这时候，local model就会比较好。

#### 10.2.2.1 Fit with iLQR

在上一章的trajectory optimization部分介绍的LQR/iLRQ就可以用作一个local model

$$
\min_{a_1,... ,a_T} \sum_{t=1}^T c(s_t,a_t)  \mbox{ s.t. } s_t=f(s_{t-1},a_{t-1})\\
\min_{a_1,... ,a_T} c(s_1,a_1) + c(f(s_1,a_1),a_2)+...+c(f(f(...)),a_T)
$$

比如，我们假设使用linear model $f(s_{t},a_{t})\approx w_{t}s_{t}+b_{t}a_{t}$来拟合每一个local dynamics，还可以在考虑高斯noise，$P(s_{t+1}\mid s_{t},a_{t})=N(f(s_{t},a_{t}),\Sigma)$。这样在LQR中，每次迭代他都反馈回来一个线性的dynamics（x就是s，u就是a）。

![](/img/plan11.png)

所以，我们可以使用他来快速找到local dynamics

![](/img/rl14.png)

在这个设定下，图中的fit dynamics就是指去求解f中的参数w和b了，图中的$p(a_{t}\mid s_{t})$可以取不同的值

1. 使用iLQR：

$$
p(a_{t}\mid s_{t})=\delta(a_{t}=K_{t}(s_{t}-\widehat{s}_{t})+k_{t}+\widehat{a}_{t})
$$

2. 考虑noise：

$$
p(a_{t}\mid s_{t})=N(K_{t}(s_{t}-\widehat{s}_{t})+k_{t}+\widehat{a}_{t}, \Sigma_{t})
$$

其中$\Sigma_{t}=Q_{a_{t},a_{t}}^{-1}$是一个推荐取值。因为在iLQR中，如果变化动作a，导致Q（这里的Q表示cost to go，不是reward to go）变化很大的话，$Q_{aa}$的值也会变得很大。这时候表示动作a的选择比较敏感。

#### 10.2.2.2 Local model is too local

然而，local model的问题在于，local model没有普适性，local information只适用于local。比如下面的图，蓝色实线是学习到的local linear dynamics，绿线是真实的。如果一直使用蓝色实线所planning到的policy来走的话，可能会越走越错（蓝色虚线是planning的预想结果，红色虚线是真实执行的结果）。产生这样的原因就是local model overfits local state。

![](/img/rl15.png)

对于这个问题来说，如果我们约束trajectory distribution都分布得比价紧密，那么local dynamics之间也会变得比较紧密

![](/img/rl16.png)

这里trajectory的分布是通过KL散度来衡量的。其中H函数表示熵

$$
D_{KL}(P(\tau)\mid\mid \widehat{P}(\tau))=E_{P(\tau)}\left[ \sum_{t=1}^T logP(a_t\mid s_t) -log\widehat{P}(a_t\mid s_t)\right]\\
=\sum_{t=1}^TE_{p(s_t,a_t)} \left[ logP(a_t\mid s_t) -log\widehat{P}(a_t\mid s_t)\right]\\
=\sum_{t=1}^TE_{p(s_t,a_t)} \left[ -log\widehat{P}(a_t\mid s_t) - H(P(a_t\mid s_t))\right]
$$

那么，结合KL散度的约束，可以写成下面的优化问题

$$
\min_P \sum_{t=1}^T E_{P(s_t,a_t)}[c(s_t,a_t)]  \mbox{ s.t. } D_{KL}(P(\tau)\mid\mid \widehat{P}(\tau)) \leq \epsilon
$$

这个问题可以通过dual gradient descent来求解。下图的L函数表示拉格朗日

![](/img/rl17.png)

对于第一步的求解，即：

$$
\min_P \sum_{t=1}^T E_{P(s_t,a_t)}[c(s_t,a_t) - \lambda log\widehat{P}(a_t\mid s_t) - \lambda H(P(a_t\mid s_t))] -\lambda \epsilon
$$

它的求解要用到iLQR的一个等价理论：iLQR在高斯噪声下的优化问题等价于下式：

$$
\min_P \sum_{t=1}^T E_{P(s_t,a_t)}[c(s_t,a_t) - H(P(a_t\mid s_t))]
$$

所以，dual gradient descent的第一步可以通过令$\widehat{c}(s_{t},a_{t})=\frac{1}{\lambda}c(s_{t},a_{t})-log\widehat{P}(a_{t}\mid s_{t})$，然后通过iLQR来求解

![](/img/rl18.png)

最后，local和global的方法也可以合起来用，比如用贝叶斯模型来拟合local model，把global model以先验的形式加入。

## 10.3 Learn model and policy

model-based planning太慢（比如要做数据曾广，要做re-planning等等），难以给出实时响应，所以还是直接用policy比较好。比如打羽毛球时先根据环境和羽毛球的速度角度等等，planning一下落球点，然后跑过去接球。这个过程如果能得到一个好的接球policy的话，那么以后别人扔飞盘，就能不用重新planning一下，才能接了。所以policy隐含有更好的泛化能力。如果，我们能知道dynamics，那么整个序列决策过程可以用下图表示，那么就可以像deep model一样做back propagation

![](/img/rl19.png)

![](/img/rl13.png)

实际上，这个问题和前面的trajectory optimization相关，但是难度加大了。因为这里引入了policy，而policy把各个状态转换紧密耦合在一起了。所以需要在做planning的时候，还学习policy。或者说是有指向性的planning出一些样本，来引导policy的学习，也就是guided policy search。在加入policy之后，整个问题在trajectory optimization的基础上，变成了constrained trajectory optimization

$$
\min_{a_1,... ,a_T,s_1,...,s_T, \theta} \sum_{t=1}^T c(s_t,a_t)  \mbox{ s.t. } s_t=f(s_{t-1},a_{t-1}), a_t=\pi_\theta(s_t)\\
\mbox{简化：}\min_{\tau, \theta} c(\tau) \mbox{ s.t. } a_t=\pi_\theta(s_t)
$$

上面的问题可以通过增广拉格朗日法+dual gradient descent来解。使用曾广拉格朗日的原因是为了通过L2项来增加求解的稳定性。下面伪代码中的$L$就是增广拉格朗日。

$$
L(\tau,\theta,\lambda)=c(\tau)+\sum_{t=1}^T\lambda_t(\pi_\theta(s_t)-a_t) + \sum_{t=1}^T\delta_t(\pi_\theta(s_t)-a_t) ^2
$$

![](/img/rl20.png)

伪代码中的第二步，在求解policy的时候，已经不再是紧耦合的了，而是一个普通iid的监督学习，这就方便优化求解了。整个过程可以看做是一个guided policy search，第一步planning出最好的样本，然后将这个样本用于引导policy有更好的更新。当然这个过程也可以在多个trajectory之间并行，因为一条trajectory引导得到的policy，它的泛化性能还不高。比如，policy只适用于从一个state出发的情况。一个好的policy，无论从迷宫的那个地方进入，都要走得出去才可以。下图就是一个multiple trajectory的示意图，叉就是目标task，其他的表示不同的trajectory。

![](/img/rl21.png)

这时候的目标变成：

$$
\min_{\tau_1,...,\tau_N, \theta} \sum_{i=1}^N c(\tau_i) \mbox{ s.t. } a_{t,i}=\pi_\theta(s_{t,i})
$$

这里可以在第一步里并行求解（分布式机器学习还是很有用的）

![](/img/rl22.png)

# 11. Connections Between Inference and Control

# 12. Inverse Reinforcement  Learning 

# 13. Advanced Policy Gradient

# 14. Exploration and Exploitation 

# 15. Transfer Reinforcement Learning

# 16. Transfer and Multi-task Reinforcement Learning

# 17. Meta-Learing

# 18. Advanced Imitation Learning: Challenges and Open Problems

## 19.1 Imitation Learning

















**TO BE CONTINUE**


