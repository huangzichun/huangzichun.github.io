---
layout:     post
title:      Reinforcement Learning学习笔记（中）
subtitle:   入坑指南
date:       2018-05-12
author:     HC
header-img: img/post-bg-swift2.jpg
catalog: true
tags:
    - Reinforcement Learning
    - MDP
    - Policy
---



# 10. Model-based RL

上一章说的是Model-based Planning，即结合已知的dynamics，做出更好的value function的估计和更好的决策。本章的Model-based RL 是指，dynamics未知（本文特指状态转移，好像现在对reward function都不会怎么学习？reward都是专业知识？），但是我们要去学习它！学习到之后，我们又可以做planning啦。model-based RL和model-free RL的区别就是前者是去学习dynamics，然后获得value function或者policy，后者是直接去学习value function或者policy，而不依赖dynamics。所以model-based RL可以用下面的图表示，这里的地球是一个卡通图案，表示他是我们通过样本学习到的dynamics，然后整个RL过程就在这样的已知的dynamics中进行。

![](/img/rl3.png)

下面是一个model-based RL的流程图：一开始我们初始化一定的policy，用他和环境交互，获得经验数据；在使用这个经验数据学习出dynamics。当dynamics已知的时候，我们就可以planning出更好的policy。比如使用动态规划planning（policy iteration, value iteration, MC, TD等等），Stochastic optimization，MCTS等model-based planning的方法。但是，这种相互依赖的loop，一好则好，一旦学习有猜错，那就开始累计了。实际效果会限制于所学习的model/dynamics的好坏。因此，如果model学得太烂，完全就可以用model-free等等的方式

![](/img/rl4.png)

## 10.1 Planning with learned dynamics

从上面的示意图，只能看出model-based RL由三个步骤组成：

1. 用当前的policy/value，出招，得到一些反馈数据
2. 根据反馈数据，学习出一个dynamics $f$
3. 根据$f$来planning，优化policy/value

在实际操作中，会有一些其他问题：训练数据是否足够用来学习，是否能学习到一个好的模型？

### 10.1.1 Distribution mismatch

训练数据是源于真实的MDP，但是训练数据是否能很好的反应MDP，这是未知的。比如在自动驾驶问题中，如果车子一直都在左转弯绕圈，那么用这样的数据所学习的planning，不就只能左转弯了么。实际上训练数据（左转绕圈）和真实驾车场景是不一样的。如果我们用于学习dynamics的算法很厉害，拟合能力很强，那么这个问题就会更加突出，因为学到的dynamics overfits，建立在它之上的planning也就不会好。

解决这个问题的一个可行方案就是扩充训练数据集，竟可能的去避免mismatch。如下所示，在第4步增加数据扩充的步骤。

![](/img/rl8.png)

这个策略和模仿学习(imitation learning)中的DAgger算法（Dataset Aggregation）很相似（伪代码如下），对于DAgger算法的dabug就是要人来打标签，选动作。而在强化学习中，则是直接使用当前policy的动作值当label，勉强人工的过程。

![](/img/rl9.png)

但是，这个扩充数据集的方法就不适用于open loop plan了，毕竟没有交互，就没机会得到新数据。

### 10.1.2 Model error

另一个问题就是，真实的dynamics我们是不知道的。通过学习出来的dynamics很有可能是不准确的，甚至是有大bug的。比如在自动驾驶问题中，如果dynamics学得不好，在给定状态下做的planning，也许会给出错误的答案：如果学习到的dynamics告诉你前面是个左转弯（实际上是直路），那么planning会告诉你赶紧左转，但是一旦左转，那就会冲出公路咯。

![](/img/rl10.png)

这种情况下，除了数据扩充的方法外，还可以re-planning。在一段时间之后的新状态下，进行重新planning，一旦re-planing的结果发现不能再左转了（即使学习到的dynamics告诉你应该要左转），这个时候还是有改正的机会。

![](/img/rl11.png)

下面代码的第3就是re-planning（注意，之前的代码是每N次，更新一次dynamics，然后做一次planning，这里是每N次，更新一次dynamics，但是planning是做了N次的）

![](/img/rl12.png)

但是这里re-planning带来的计算量不会太恶心，因为如果每次都在做planning的话，其实不用去plan那些时间片很久以后的事，往往plan一个短期的future就很work了，这时候就可以使用上一章提到的Stochastic optimization方法了。

## 10.2 Learn a model by supervised learning

为了要学习dynamics（这里特指状态转移和reward function），可以求助由于监督学习。给定训练数据$\{s_{t},a_{t},r_{t+1},s_{t+1}\}^n$，我们要求解的是$s_{t+1}=P(S_{t+1}\mid S_{t},A_{t})$和$r_{t+1}=R(R_{t+1}\mid S_{t},A_{t})$。这时候监督学习学习的目标不在是label，而是输入$s_{t},a_{t}$，输出$s_{t+1}$和$r_{t+1}$。回想最开始我们提到的动态系统，就可以用来解他们，而且还是考虑到了数据之间的依赖情况。当然，我们也可以把这种依赖情况忽略，之间采用简单的模型去拟合：

$$
s_1,a_1 \rightarrow r_2,s_2\\
s_2,a_2 \rightarrow r_3,s_3\\
...\\
s_{T-1},a_{T-1} \rightarrow r_{T},s_{T}\\
$$

比如用回归问题来集合reward，用密度估计来拟合state。总的来说，可以用local和global两个大类的模型来拟合，比如：

1. Table Lookup Model
2. Linear Expectation model
3. linear gaussian
4. gaussian process
5. deep belief network
6. iLQR等等

### 10.2.1 Fitting global dynamic model

#### 10.2.1.1 Table Lookup Model

table lookup其实就是通过简单的数据统计，记录所有相同输入下的输出均值。可以将它视作一个全空间下的KNN回归模型，这个K等于数据点的个数。

$$
\widehat{P}_{ss'}^a=\frac{1}{N(s,a)}\sum_{t=1}^n1(S_t,A_t,S_{t+1}=s,a,s')\\
\widehat{R}_{s}^a=\frac{1}{N(s,a)}\sum_{t=1}^n1(S_t,A_t=s,a)R_t
$$

当然，这种基于表格的方式，仅仅适用于离散且维度不大的情况。并且模型的泛化能力依赖于训练数据，如果，缺少某个状态下的训练数据，那么模型肯定是学习不到这个状态的。在学习出model之后，就可以使用planning了。在model-based RL中还有一个特别的planning方法，叫做Sample-based Planning。这种方法简单有效。它先通过训练数据学习出一个模型，然后使用这个模型再模拟/采用出更多的样本，最后再使用model-free的方法（比如Q-learning, sarsa, MC等等）求出policy或者value function。比如给定state空间是{A,B}和训练episode数据（state, reward）：

$$
A,0,B,0\\
B,1\\
B,1\\
B,1\\
B,1\\
B,1\\
B,1\\
B,0
$$

我们可以通过table lookup的方法，得到下面的dynamics

![](/img/rl5.png)

那么，如果使用sample-based planning方法，我们需要对这个dynamics就行采样，比如得到一些采样episode：

$$
B,1\\
B,0\\
B,1\\
A,0,B,1\\
B,1\\
A,0,B,1\\
B,1\\
B,0
$$

最后，我们便可以在采样数据上使用model-free的方法学习policy或者value了，比如使用MC的方法（用episode的return来近似）可以得到$V(A)=1,V(B)=0.75$。一个随之而来的问题就是，采样数据不一定可靠（取决于所学习到的model），那为什么结合训练数据一起使用呢？可以的，这就是Dyna算法

#### 10.2.1.2 Dyna

既然我们有了来源于真实MDP的数据和源于学习到的MDP的数据，那么自然我们可以把他们合起来，学习一个更好的policy或者value function。Dyna就是这样的算法，他的思想可以用下面的图表示：

![](/img/rl6.png)

Dyna如果建立上sample-based planning方法上，那么采样的数据将用于planning，这里planning选用的是model-free的q-learning，真实的数据也是通过Q-learning来更新value function。具体来说dyna的伪代码如下：

![](/img/rl7.png)

第d步用真实数据更新value，e步学习dynamics，f步则是planning。Dyna的步骤很简单，而且效果也很好

### 10.2.2 Fitting local dynamic model

TODO

## 10.3 Learn model and policy

model-based planning太慢，难以实时，所以还是要policy比较好

![](/img/rl13.png)

TODO



# 11. Connections Between Inference and Control

# 12. Inverse Reinforcement  Learning 

# 13. Advanced Policy Gradient

# 14. Exploration and Exploitation 

# 15. Transfer Reinforcement Learning

# 16. Transfer and Multi-task Reinforcement Learning

# 17. Meta-Learing

# 18. Advanced Imitation Learning: Challenges and Open Problems

## 19.1 Imitation Learning

















**TO BE CONTINUE**


