---
layout:     post
title:      Hadoop相关总结
subtitle:   入坑指南
date:       2018-04-19
author:     HC
header-img: img/post-bg-android.jpg
catalog: true
tags:
    - Hadoop
    - HDFS
    - MapReduce
---

> 今天组会分享了Hadoop的相关知识点，特此总结一发

# 1.Hadoop  

Hadoop是一个于2011年，由Apache基金会所开发的分布式系统基础架构。它为我们提供了一个可靠的，可扩展的分布式计算框架。总的来说，Hadoop由四部分组成，包括:

1. 分布式计算框架MapReduce
2. 分布式文件系统HDFS
3. 作业调度和资源管理服务YARN
4. 用于支持外部组件的公用调用服务Hadoop Common

本文主要就前三部分进行总结。就分布式计算框架而言可以分为**模型并行(Model Parallelism)**，**数据并行(Data Parallelism)**和**两者的混合(Irregular Parallelism)**。模型并行化是将模型划分成子模型，分布到各个节点上（比如将线性模型参数W根据feature进行划分）；数据并行是将数据划分到不同节点上，而计算模型本身不会进行划分。根据算法迭代方法的不同，现有的分布式计算框架可以划分为**基于同步(Synchronization-based)**的和**基于异步的(Asynchronization-based)**。两者的不同在于是否等待所有节点完成计算/任务，然后在进入下一次迭代/任务。

对于Hadoop框架来说，**Hadoop是一个基于同步和数据并行(Synchronization-based and Data-distributed)的计算框架**。Hadoop是建立在HDFS之上，数据本身已经进行了分布式存储与各个节点上，所以Hadoop的MapReduce计算任务会将整个计算模型/程序发送到各个节点上运行。这暗示着Hadoop不可避免的会涉及到数据在不同节点上的调度，同时带来巨大的IO开销。并且也暗示了MapReduce计算框架不能应用于数据之间有依赖的情况。因为HDFS上的数据是基于IID划分的。另一方面，对于每一次MapReduce任务迭代，MapReduce总是等到所有节点都完成了Map操作之后，再进行Reduce操作。当某个节点执行任务特别慢的时候，整个集群会陷入等待状态。考虑到这个问题Hadoop也做了相应的处理。比如当一定比例的节点完成Map任务的时候，就可以进入Reduce阶段，而不在等待；当然也在从任务调度的层面监控作业进度，将某个节点上执行缓慢的任务放在别的节点上执行。

即便Hadoop本身有一些不足，但是也不影响Hadoop的优势：

1. **Low cost**，搭建Hadoop的成本是很小的，它能利用每一个臭皮匠
2. **Scalability**，多个臭皮匠，顶很多诸葛亮
3. **Reliability and Fault Tolerance**，Hadoop本身提供很多的容错机制，保证集群的可靠性。包括HDFS层面的冗余备份，HA机制对节点宕机的容错，YARN机制对作业调度的容错支持等等。
4. **Efficiency**，分布式计算的优势
5. **Throughput**，支持很大的吞吐量，支持一写多读

当然，Hadoop也有不能处理的场景

1. **小数据与大量小文件**。由于HDFS上数据是以块(Block)进行存储的，不足块大小的文件同样会占用一个块的大小。如果小文件太多，会占用NameNode过多的内存来维护元数据信息。
2. **修改文件的值**。HDFS不提供Value-level的数据修改，只能覆写或者追加
3. **流式作业和低延时场景**。Hadoop MapReduce是操作HDFS上的数据，所以流式数据到来之后，会涉及到在HDFS上的转存操作，然后再是读取数据，交于MapReduce计算。并且MapReduce计算框架本身也会涉及到一些数据溢写的IO操作。因此实时性要求强的场景下，Hadoop并不具有优势
4. **MapReduce多次迭代**。Hadoop对于MapReduce的迭代任务的支持并不友好，需要手动构建多个MapReduce Job。对于众多机器学习算法，算法的迭代是难以避免的。这时候Hadoop不是一个很好的选择
5. **基于图的并行计算（Graph parallel computing）**。Hadoop是基于数据并行的计算框架，底层HDFS进行数据分布式储存的时候，并没有考虑数据之间的联系，反倒是假设了数据是IID的。因此对于图数据，Hadoop也是不适合的。


# 2. HDFS

## 2.1 核心机制

HDFS是Hadoop Distributed File System的缩写，它采用了主从式的结构。根据节点角色的不同，可以主要分为**NameNode**和**DataNode**（细分还包括**Second Namenode**，**JournalNode**等等）。下图是HDFS的粗略框架

![](../img/hdfs1.png)

其中

1. **Namenode**充当班长的角色，管理着整个班级（集群）的信息。这份名单被叫做**元数据（meta-data）**。另外，客户端的访问请求都由namenode负责。另外namenode上也维护了集群的其他信息，包括命名空间，集群配置信息等等。一般来说，namenode上有两个重要文件常驻内存，提供元数据信息。这两个文件是FsImage和EditLogs。前者记录了命名空间，集群配置信息和数据块的相关信息；后者存放了对文件系统的每一次修改记录。比如新建了一个文件夹，修改了数据块的复制个数等等。Namenode需要周期性的合并两个文件，生成新的FsImage文件，以提供客户端的查询访问。
2. **Datanode**负责实际数据的存放，数据以数据块（block）的形式存储在datanode之上，每一个数据块都有多个冗余备份（**Replication**），以达到容错的作用。客户端对数据的读写操作直接作用在datanode上，但前提是客户端首先得由namenode告知在哪里读，在哪里写。
3. 图中的Rack是表示一个PC或者机柜

为了保证整个班级的正常次序，班长需要定期的点名，检查是否有童鞋上课睡觉。这里的namenode并不会主动点名，而是让各个datanode主动汇报，定期的给namenode发送信息。信息包括两部分，心跳检测（HeartBeat）和数据块汇报信息（BlockReport）。前者告诉namenode，datanode还活着；后者是该datanode上数据块的信息。

1. 如果HeartBeat和BlockReport被namenode收到了

   namenode会汇总大家的blockreport，检测文件的数据块是否有丢失，数据块的复制数是否达到要求。如果没有达到要求，集群会进入Safe Mode。

2. 如果namenode超过10min还没有收到某个datanode的HeartBeat

   namenode会将这个datanode标记为挂了，然后将原本存储在这个datanode上的数据块，重新复制到其他节点上，并且以后的计算任务也不会再发送给这个datanode了。

当然，班长namenode也可能会挂掉。一旦班长挂了，整个班级都约等于挂了。为了防止这个情况，允许集群的更好的容错能力，Hadoop 1.X启动了副班长策略，引入了**Second Namenode**。second namenode负责FsImage和EditLog两个文件的合并，减轻namenode的负担（如下图）。

![](../img/hdfs2.png)

当namenode挂掉了，second namenode上会保存有最新的FsImage文件。那么集群管理狗就可以将这份FsImage拷贝到namenode上，然后人工重启namenode。所以second namenode不提供namenode的故障处理，它仅仅是减轻namenode的压力而已。

要达到故障处理的要求，Hadoop 2.X之后提供了Hadoop HA策略。但是，注意，HA和second namenode策略不能同时使用。Hadoop HA的框架图如下：

![](../img/hdfs3.png)

Hadoop HA引入了两个Namenode，即**Active Namename**和**Standby Namenode**。但是只有一个提供集群服务，而另一个就standby。一旦active namenode挂了，standby的就立刻上线。至于一开始，哪个namenode充当active的，取决于Zookeeper。Zookeeper提供master选举的作用，这个选举实际上是一个抢占式的锁机制，两个namenode谁先到谁就是active的。

standby namenode需要和active namenode的元信息一致，才能在active one宕掉之后，立刻提供一致服务。为了让元信息一致：

1. 集群里的datanode需要同时向两个namenode发送心跳和blockreport；
2. active namenode对EditLog的修改需要同时写入JournalNodes（也就是图中的JN）。一般来说，有2N+1台 JN 来存储EditLog，每次写数据操作有大多数（>=N+1）返回成功时即认为该次写成功。同时，也容忍了最多有N台机器挂掉，如果多于N台挂掉，这个算法就失效了。
3. 任何修改操作在 Active NN上执行时，JN进程同时也会记录修改log到至少半数以上的JN中，这时 Standby NN 监测到JN 里面的同步log发生变化了会读取 JN 里面的修改log，然后同步到自己的的目录镜像树里面

最后，图中的**FailoverController**监控了NameNode的状态，并且通过心跳机制，监控自己在zookeeper中的状态。一旦active namenode挂了，则触发standby的namenode上线。

Hadoop HA机制虽然提供了故障处理，但是它任然限制了只能有一个namenode提供服务。并且，如果hdfs中有很多块，那么元数据将占用namenode大量的内存。为了处理这个问题，Hadoop 2.X提供了**HDFS Federation机制**，它允许多个namenode共用一个集群里的存储资源，每个namenode都可以单独对外提供服务。

## 2.2 数据存储

在HDFS中，数据是以数据块（Block）的形式存储，不同版本HAdoop的默认块大小不同，128M或者64M。这个值可以用户自己定义。如果一个数据文件被划分成越小的数据块，HDFS读取这个文件时候的并发性也就更高。但是，也意味着将带来更多的磁盘寻道的IO开销，所以这是个trade-off。

HDFS采用了冗余备份的策略，为每一个数据块都保存了多个复制（replicas）。默认replicas的大小为3份。下面展示了HDFS上数据的存储图：

![](../img/hdfs4.png)

1. 图中可以再次看到namenode上存储了文件到数据块的映射信息，比如文件part-0，有两个数据块1,3。每个数据块一共有两个复制，分别存储在不同的节点上。事实上，数据块的总数之间影响了MapReduce过程中，mapper的个数。

2. 大文件被划分成了很多的block，而不足block大小的文件（比如一个只有1k的文件），同样占用了一个block。

3. HDFS对文件的划分存储并没有考虑到文件的结构信息。这时候，HDFS引入了**InputSplit**，InputSplit由用户在读取HDFS数据的时候指定，它保证了一个文件的逻辑划分。比如，下图是一个由4条记录组成的文件，每条记录100M。

   ![](../img/hdfs5.png)

   假设，HDFS块的大小为128M，那么上面这个文件在HDFS的存储方式可以表示为：

   ![](../img/hdfs6.png)

   这时候，启动一个mapper来解析任何一个Block都不能都到正确的结果，比较record的结构以及被破坏。为了处理这里问题，我们需要指定一个InputSplit来读取数据，给出每一条record在逻辑上的划分。PS：普通文本文件当然可以不用设置InputSplit。

   ![](../img/hdfs7.png)

   具体来说，需要在设置Job的时候，调用**job.setInputFormatClass**(WholeFileInputFormat.class)来指定自己实现的InputSplit格式。这里的WholeFileInputFormat.class是自己实现的类，用来指定split，它需要继承FileInputFormat类（当然选择TextInputFormat，SequenceFileInputFormat等等类来集成也是可以的，具体场景，具体分析），覆写createRecordReader和isSplitable方法。同时还要指定自己的RecordReader。下面是一个小栗子，用于解析一个二进制文件。

   ```java
   public class WholeFileInputFormat extends FileInputFormat<Text, BytesWritable> {
       @Override
       public RecordReader<Text, BytesWritable> createRecordReader(
               InputSplit split, TaskAttemptContext context) throws IOException,
               InterruptedException {
         //这个类告诉程序要怎么去读文件，找出正确的input划分
           WholeFileRecordReader recordReader = new WholeFileRecordReader();  
           recordReader.initialize(split,context);
           return recordReader;
       }

       @Override
       protected boolean isSplitable(JobContext context, Path filename) {
           return false;
       }
   }
   ```

   ```java
   public class WholeFileRecordReader extends RecordReader<Text, BytesWritable> {
       private FileSplit fileSplit;
       private FSDataInputStream fin;
       private Text key = new Text();
       private BytesWritable value = new BytesWritable();
       private boolean processed = false;
       private Configuration conf;
       private String fileName;
       private int count=0;

       @Override
       public void initialize(InputSplit inputSplit, TaskAttemptContext context)
               throws IOException, InterruptedException {
           //这里整一些初始化的工作
       }


       @Override
       public boolean nextKeyValue() {
       	// 这个方法里需要实现具体这么解析一条一条的记录，然后将读取结果设置到key和value里
       	// 期间，有任何解析问题就返回false，否则返回true
   		value = new BytesWritable(info);
            key.set(count+"");
            return true;
       }
       
       @Override
       public float getProgress() throws IOException, InterruptedException {
           // TODO Auto-generated method stub
           return processed? fileSplit.getLength():0;
       }

       @Override
       public void close() throws IOException {
           // TODO Auto-generated method stub
           //fis.close();
       }

       @Override
       public Text getCurrentKey() throws IOException, InterruptedException {
           // TODO Auto-generated method stub
           return this.key;
       }

       @Override
       public BytesWritable getCurrentValue() throws IOException,
               InterruptedException {
           // TODO Auto-generated method stub
           return this.value;
       }
   }
   ```

在了解了数据是怎么存储了之后，我们再来了解下客户端是如何读写数据的。

1. **读数据**

   由于namenode上存储了datanode和数据块的路径地址，所以，客户端实际读取数据之前，需要访问namenode获取相关信息。而namenode会计算最佳读取的块，然后返回其位置信息给客户端。最后客户端需要根据返回的信息，自己去找datanode读取数据。流程如下图所示。

   ![](../img/hdfs8.png)

   1. 客户端(client)用FileSystem的**open()**函数打开文件
   2. DistributedFileSystem用RPC调用nameNode，得到文件的数据块信息。对于每一个数据块，**namenode节点返回保存数据块的数据节点的地址**。DistributedFileSystem返回FSDataInputStream给客户端，用来读取数据。
   3. 客户端调用stream的**read()**函数开始读取数据。
   4. **DFSInputStream连接保存此文件第一个数据块的最近的数据节点**。Data从数据节点读到客户端(client).
   5. 当此数据块读取完毕时，DFSInputStream关闭和此数据节点的连接，然后连接此文件下一个数据块的最近的数据节点。
   6. 当客户端读取完毕数据的时候，调用FSDataInputStream的close函数。在读取数据的过程中，如果客户端在与数据节点通信出现错误，则尝试连接包含此数据块的下一个数据节点。失败的数据节点将被记录，以后不再连接。

   幸运的是，客户端具体的程序调用，已经屏蔽了读取数据的细节：

   ```java
   Configuration configuration = new Configuration();
   String dataPath = "hdfs://localhost:9000/dml";
   FileSystem fileSystem = FileSystem.get(URI.create(dataPath), configuration);
   FSDataInputStream fsDataInputStream = fileSystem.open(new Path(dataPath));
   BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(fsDataInputStream));
   String line = bufferedReader.readLine();
   while(line != null){
       System.out.println(line);
       line = bufferedReader.readLine();
   }
   bufferedReader.close();
   fsDataInputStream.close();
   fileSystem.close();
   ```

2. **写数据**

   客户端写数据的流程如下所示：

   ![](../img/hdfs9.png)

   1. 客户端调用create()来创建文件
   2. DistributedFileSystem用RPC调用元数据节点，在文件系统的命名空间中创建一个新的文件。元数据节点**首先确定文件原来不存在，并且客户端有创建文件的权限，然后创建新文件**。DistributedFileSystem返回DFSOutputStream，客户端用于写数据。
   3. 客户端开始写入数据，**DFSOutputStream将数据分成块，写入data queue**。
   4. Data queue由Data Streamer读取，并通知元数据节点分配数据节点，用来存储数据块(每块默认复制3块)。分配的数据节点放在一个pipeline里。
   5. Data Streamer将数据块写入pipeline中的第一个数据节点。第一个数据节点将数据块发送给第二个数据节点。第二个数据节点将数据发送给第三个数据节点。DFSOutputStream为发出去的数据块保存了ack queue，等待pipeline中的数据节点告知数据已经写入成功。如果数据节点在写入的过程中失败：关闭pipeline，将ack queue中的数据块放入data queue的开始。当前的数据块在已经写入的数据节点中被元数据节点赋予新的标示，则错误节点重启后能够察觉其数据块是过时的，然后会被删除。失败的数据节点从pipeline中移除，另外的数据块，则写入pipeline中的另外两个数据节点。元数据节点则被通知此数据块是复制块数不足，将来会再创建第三份备份。
   6. 当客户端结束写入数据，则调用stream的close函数。
   7. 此操作将所有的数据块写入pipeline中的数据节点，并等待ack queue返回成功。最后通知元数据节点写入完毕。

   同样，程序实现：

   ```java
   Configuration configuration = new Configuration();
   String dst = "hdfs://localhost:9000/dml";
   String data = "data mining lab data mining lab,data mining lab";
   FileSystem fileSystem = FileSystem.get(URI.create(dst), configuration);
   Path path = new Path(dst);
   FSDataOutputStream fsDataOutputStream = fileSystem.create(path);
   fsDataOutputStream.write(data.getBytes());
   fsDataOutputStream.flush();
   fsDataOutputStream.close();
   ```



# 3. YARN



# 4. In action





