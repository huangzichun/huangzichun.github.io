---
layout:     post
title:      变分推断学习入坑指南
subtitle:   又是一个磨人的笔记
date:       2021-12-20
author:     HC
header-img: img/post-sample-image.jpg
catalog: true
tags:
    - 变分推断
    - Variational Inference
    - Mean Field
    - Latent Dirichlet Allocation
---



# 0x00 Variational Inference

据可靠消息称，由于在贝叶斯统计中的后验概率一般都很难，因此为了解决这个问题，常用的方式就是**Variational Inference**和Sampling。但是根据Michael I. Jordan说的，"VI can be much faster than MCMC and yield upper and lower bounds on probabilities"

对于Sampling来说，旨在通过对分布进行抽样，得到一些样本，那么根据这些样本，我们就可以得到该分布特性的估计，Sampling的方法包括

1.  两次采样数据之间相互独立

> **CDF Sampling，Reject Sampling，Adaptive Rejection Sampling，Importance Sampling**

2.  两次采样数据之间不相互独立

> **MCMC（MH和Gibbs），Slice Sampling...**

对于后者Variational Inference（以下用简写VI），表示完全不懂，鼓起勇气去做了些了解。如下图所示，Variational Inference旨在用一个相对简单的分布Q来对复杂后验分布P做近似。

![](/img/vi.png)

自然而言，这里需要一个衡量指标来断言Q与P之间的"近似程度"，在VI中这个衡量指标采用KL散度，这是专门用来衡量两个分布之间相似性的方法，当两个分布完全一样，则他们的KL散度等于0。需要注意的是，KL散度是不对称的，也就是说$KL(Q\|\|P)$是不等于$KL(P\|\|Q)$的。


$$
KL(P(X)|\left| Q\left( X \right) \right) = \int_{}^{}{P(X)log\frac{P(X)}{Q(X)}\text{dX}}
$$


一般而言，VI可以分为两个大类，一个是**Loopy Belief Propagation**，另一个是**Mean Field Approximation**。本文集中在Mean Field。接下来，从真实后验$P(X)$出发，引入$ELOB$。对于任意的$P(X)$，由条件概率公式得


$$
P\left( X \right) = P\left( X,Z \right)/P(Z|X)
$$




对上式两边取对数后，我们再引入$Q(X)$得


$$
\ln\left( P\left( X \right) \right) = \ln\left( P\left( X,Z \right) \right) - \ln\left( P\left( Z \middle| X \right) \right)\\
\ln\left( P\left( X \right) \right) = \ln\left( \frac{P\left( X,Z \right)}{Q\left( Z \right)} \right) - \ln\left( \frac{P\left( Z|X \right)}{Q\left( Z \right)} \right)
$$


接下来，仿照推倒EM算法的方法，对上式取期望，但是EM算法是在分布$P(Z|X,\theta^{k})$下取得期望，这里我们在$Q(Z)$下对上式取期望，其中$Q(Z)$就是我们要用来估计$P$的一个分布，可以得到：


$$
\int_{}^{}{Q(Z)\ln\left( P\left( X \right) \right)\text{dZ}} = \int_{}^{}{Q(Z)\ln\left( \frac{P\left( X,Z \right)}{Q\left( Z \right)} \right)\text{dZ}} - \int_{}^{}{Q(Z)\ln\left( \frac{P\left( Z|X \right)}{Q\left( Z \right)} \right)\text{dZ}}
$$


对于左边的式子，因为是对$Z$求积分，那么就恒等于$\ln\left( P\left( X \right) \right)$，所以将上式展开得


$$
\ln\left( P\left( X \right) \right) = \int_{}^{}{Q\left( Z \right)\ln\left( P\left( X,Z \right) \right)\text{dZ}} - \int_{}^{}{Q\left( Z \right)\ln\left( Q\left( Z \right) \right)\text{dZ}} - \int_{}^{}{Q\left( Z \right)\ln\left( \frac{P\left( Z|X \right)}{Q\left( Z \right)} \right)\text{dZ}}\\
= \underset{\text{ELOB}\mathcal{\text{\ L}}\left( Q \right)}{\overset{\int_{}^{}{Q\left( Z \right)\ln\left( P\left( X,Z \right) \right)\text{dZ}} - \int_{}^{}{Q\left( Z \right)\ln\left( Q\left( Z \right) \right)\text{dZ}}}{︸}} + \underset{KL(Q(Z)||P(Z|X))}{\overset{\int_{}^{}{Q\left( Z \right)\ln\left( \frac{Q\left( Z \right)}{P\left( Z|X \right)} \right)\text{dZ}}}{︸}} \\
= \mathcal{L}\left( Q \right) + KL(Q(Z)||P(Z|X))
$$




所以，我们将一个复杂的后验分布的对数展开成了KL散度和$\mathcal{L}\left( Q \right)$的和，其中$\mathcal{L}\left( Q \right)$叫做**ELOB（Evidence Lower Bound）**，同时$\mathcal{L}\left( Q \right)$也是一个泛函（functional），并且，由于KL是非负的，所以$\mathcal{L}\left( Q \right)$的upper bound是$\ln\left( P\left( X \right) \right)$，要想$\mathcal{L}\left( Q \right)$达到最大值，则必须KL散度等于0。这样操作的意思在于，通过寻找一个$Q$，使得$\mathcal{L}\left( Q \right)$达到最大值，KL散度为零，使得这个$Q$可以近似后验分布$P$。当然，一次性就找出这个最佳的$Q$是不现实的，相反是通过不断的调整这个分布的参数，使得最后收敛到target后验分布上。

从PRML上弄出来的一个图给出一个形象的分析，其中$\theta$泛指参数

![](/img/vi1.png)

**注意**

1.  EM算法也可以用上图来表示，但是这方面VI与EM略有不同，对EM来说，每一次E步建立期望表达式，M步对期望求最大值，每一次$L(q,\theta)和ln(P(X|\theta))$都会有所提升，这可以视为每一次提升下界；但是对于VI来说，$Q$的变化，并不会影响到等式左端（$\mathcal{L}\left( Q \right)$的最大值），所以每一次只有$L(q,\theta)$有所提升。

2.  $Q(Z)$是我们自己定义的，所以假定它的各个分量相互独立，以方便后面的计算

3.  明确目标，是最大化 argmax$\mathcal{\text{\ L}}\left( Q \right)$


$$
\mathcal{L}\left( Q \right) = \int_{}^{}{Q\left( Z \right)\ln\left( P\left( X,Z \right) \right)\text{dZ}} - \int_{}^{}{Q\left( Z \right)\ln\left( Q\left( Z \right) \right)\text{dZ}}
$$


4.  ELOB在概率图模型中称为能量泛函，第一项是能量项，第二项是$Q$的熵。所以VI可以看做是优化能量泛函为目标的推理方法。

5.  VI通过引入新的变分参数来增加自由度，然后优化这些参数，从而通用性的解决问题。并且可以用来对数据的marginal likelihood求一个下界（$ln(P(X)) \geq \mathcal{L}\left( Q \right)$）



虽然已知了$\mathcal{L}\left( Q \right)$的表达式，但是这个式子仍然还不够简化。为了进一步的简化这个神奇的ELOB，首先，我们把上面第二点的假设带入ELOB中，其中$Q(Z) = Q(Z_{1},...,Z_{M})$


$$
\mathcal{L}\left( Q \right) = \int_{}^{}{\prod_{i = 1}^{M}{Q\left( Z_{i} \right)}\ln\left( P\left( X,Z \right) \right)\text{dZ}} - \int_{}^{}{\prod_{i = 1}^{M}{Q\left( Z_{i} \right)}\sum_{i = 1}^{M}{ln(Q(Z_{i}))}\text{dZ}}
$$




然后，对**右边式子的第一项进行简化**，那么我们有


$$
\int_{}^{}{\prod_{i = 1}^{M}{Q\left( Z_{i} \right)}\ln\left( P\left( X,Z \right) \right)\text{dZ}} = \int_{Z_{1}}^{\ }{\int_{Z_{2}}^{\ }{\ldots\int_{Z_{M}}^{\ }{\prod_{i = 1}^{M}{Q\left( Z_{i} \right)}\ln\left( P\left( X,Z \right) \right)}dZ_{1}dZ_{2}\ldots dZ_{M}}}
$$




和HMM中遇到的解法一样，我们对上式的积分写成如下的形式


$$
\int_{Z_{1}}^{\ }{\int_{Z_{2}}^{\ }{\ldots\int_{Z_{M}}^{\ }{\prod_{i = 1}^{M}{Q\left( Z_{i} \right)}\ln\left( P\left( X,Z \right) \right)}dZ_{1}dZ_{2}\ldots dZ_{M}}} = \int_{Z_{j}}^{\ }{Q(Z_{j})\left( \int_{Z_{1}}^{\ }{\ldots\int_{Z_{i \neq j}}^{\ }{\int_{Z_{M}}^{\ }{\ln\left( P\left( X,Z \right) \right)}}\prod_{i \neq j}^{M}{Q(Z_{i})dZ_{i}}} \right)}dZ_{j}
$$




算到这里，神奇的事情就发生了，我们将等式的右端的式子，看做是$\ln\left( P\left( X,Z \right) \right)$在$\prod_{i \neq j}^{M}{Q(Z_{i})}$下的期望，以此得到一个elegant的表示....


$$
\int_{Z_{1}}^{\ }{\int_{Z_{2}}^{\ }{\ldots\int_{Z_{M}}^{\ }{\prod_{i = 1}^{M}{Q\left( Z_{i} \right)}\ln\left( P\left( X,Z \right) \right)}dZ_{1}dZ_{2}\ldots dZ_{M}}} = \int_{Z_{j}}^{\ }{Q\left( Z_{j} \right)\left( \int_{Z_{1}}^{\ }{\ldots\int_{Z_{i \neq j}}^{\ }{\int_{Z_{M}}^{\ }{\ln\left( P\left( X,Z \right) \right)}}\prod_{i \neq j}^{M}{Q\left( Z_{i} \right)dZ_{i}}} \right)}dZ_{j}\\
= \int_{Z_{j}}^{\ }{Q\left( Z_{j} \right)\left( E_{\prod_{i \neq j}^{M}{Q\left( Z_{i} \right)}}\left\lbrack \ln\left( P\left( X,Z \right) \right) \right\rbrack \right)}dZ_{j}
$$




**接下来对**$\mathcal{L}\left( \mathbf{Q} \right)$**的第二项进行简化（如下）**


$$
\int_{}^{}{\prod_{i = 1}^{M}{Q\left( Z_{i} \right)}\sum_{i = 1}^{M}{ln(Q(Z_{i}))}\text{dZ}}
$$


由于$Q(Z)$中的各个成分是独立的，我们对积分式子展开之后会得到更简便的形式，但是推到的过程看起来真的尤为繁琐，反正和EM中用到的方法是一样的，展开后，一项一项的简化，其他的多余变量会被积分积掉。。。。


$$
\int_{}^{}{\prod_{i = 1}^{M}{Q\left( Z_{i} \right)}\sum_{i = 1}^{M}{\ln\left( Q\left( Z_{i} \right) \right)}dZ = \int_{Z_{1}}^{\ }{\int_{Z_{2}}^{\ }{\ldots\int_{Z_{M}}^{\ }{\prod_{i = 1}^{M}{Q\left( Z_{i} \right)}\sum_{i = 1}^{M}{\ln\left( Q\left( Z_{i} \right) \right)}}dZ_{1}dZ_{2}\ldots dZ_{M}}}}\\
= \int_{Z_{1}}^{\ }{\int_{Z_{2}}^{\ }{\ldots\int_{Z_{M}}^{\ }{\prod_{i = 1}^{M}{Q\left( Z_{i} \right)}\left( \ln\left( Q\left( Z_{1} \right) \right) + \ln\left( Q\left( Z_{2} \right) \right) + \ldots + \ln\left( Q\left( Z_{M} \right) \right) \right)}dZ_{1}dZ_{2}\ldots dZ_{M}}}\\
= \int_{Z_{1}}^{\ }{\prod_{i = 1}^{M}{Q\left( Z_{i} \right)}\ln\left( Q\left( Z_{1} \right) \right)}dZ_{1}dZ_{2}\ldots dZ_{M} + \ldots + \int_{Z_{M}}^{\ }{\prod_{i = 1}^{M}{Q\left( Z_{i} \right)}\ln\left( Q\left( Z_{M} \right) \right)}dZ_{1}dZ_{2}\ldots dZ_{M}\\
= \sum_{i = 1}^{M}\left( \int_{Z_{i}}^{\ }{Q\left( Z_{i} \right)\ln\left( Q\left( Z_{i} \right) \right)dZ_{i}} \right)
$$




现在两项都已经做了适当的简化，我们把**两项合并**起来得到：


$$
\mathcal{L}\left( Q \right) = \int_{}^{}{\prod_{i = 1}^{M}{Q\left( Z_{i} \right)}\ln\left( P\left( X,Z \right) \right)\text{dZ}} - \int_{}^{}{\prod_{i = 1}^{M}{Q\left( Z_{i} \right)}\sum_{i = 1}^{M}{\ln\left( Q\left( Z_{i} \right) \right)}\text{dZ}}\\
= \int_{Z_{j}}^{\ }{Q\left( Z_{j} \right)\left( E_{\prod_{i \neq j}^{M}{Q\left( Z_{i} \right)}}\left\lbrack \ln\left( P\left( X,Z \right) \right) \right\rbrack \right)}dZ_{j} - \sum_{j = 1}^{M}\left( \int_{Z_{j}}^{\ }{Q\left( Z_{j} \right)\ln\left( Q\left( Z_{j} \right) \right)dZ_{j}} \right)
$$


又因为前面说过，$Q(Z)$的各个成分是独立的，那么在第二项中，对于特定的$Q(Z_{K})$，其他成分相当于常数，不影响计算。并且对于第一项中的那个奇葩的期望E与对数是可以交换计算顺序的，并且这个期望E是在$\prod_{i \neq j}^{M}{Q\left( Z_{i} \right)}$下进行计算的，也就是说，这个期望E实际上是关于$X$与$Z_{j}$的函数，那么记：


$$
E_{\prod_{i \neq j}^{M}{Q\left( Z_{i} \right)}}\left\lbrack \ln\left( P\left( X,Z \right) \right) \right\rbrack = ln(\widetilde{P}(X,Z_{j}))
$$




那么带入之后，又可以做进一步简化，，所以想出这个方法的人真是机智到不行啊


$$
= \int_{Z_{j}}^{\ }{Q\left( Z_{j} \right)\left( \ln\left( \widetilde{P}\left( X,Z_{j} \right) \right) \right)}dZ_{j} - \int_{Z_{j}}^{\ }{Q\left( Z_{j} \right)\ln\left( Q\left( Z_{j} \right) \right)dZ_{j}} + SomeConstants\\
\mathcal{L}\left( Q_{j} \right) = \int_{Z_{j}}^{\ }{Q\left( Z_{j} \right)\ln\left( \frac{\widetilde{P}\left( X,Z_{j} \right)}{Q\left( Z_{j} \right)} \right)dZ_{j}} + SomeConstants \\
= - KL\left( Q\left( Z_{j} \right)||\widetilde{P}\left( X,Z_{j} \right) \right) + SomeConstants
$$




证到这里，又莫名其妙的回到了KL散度的形式，但是注意前面有个负号。这个式子给出了一个迭代求解的方式，通过最小化KL散度来最大化$\mathcal{L}\left( Q_{j} \right)$，所以对于Variational Inference来说，他的核心公式如下所示，依次对$Q(Z)$各个成分进行迭代，以更新得到$Q^{\*}(Z_{i})$


$$
\mathbf{E}_{\prod_{\mathbf{i \neq j}}^{\mathbf{M}}{\mathbf{Q}\left( \mathbf{Z}_{\mathbf{i}} \right)}}\left\lbrack \mathbf{\ln}\left( \mathbf{P}\left( \mathbf{X,Z} \right) \right) \right\rbrack\mathbf{=}\mathbf{ln}\mathbf{(}\mathbf{Q}^{\mathbf{*}}\mathbf{(}\mathbf{Z}_{\mathbf{i}}\mathbf{))}
$$


这个核心公式直观上的说明是

1.  $固定Z_{i}$，在$\prod_{i \neq j}^{M}{Q\left( Z_{i} \right)}$（Q(Z)中除$Z_{i}的$其他成分的分布）上，用$\ln\left( P\left( X,Z \right) \right)$的期望值来更新$Z_{i}$
2.  在数据$X$已知的情况下，有$P\left( X,Z \right) \propto P(Z|X)$，那么这个核心公式也可以看出是在$\prod_{i \neq j}^{M}{Q\left( Z_{i} \right)}下$的log-likelihood的均值
3.  在迭代计算中，上一次的计算结果，可以用到下一次计算中，如下所示加粗的部分。


$$
\ln\left( \mathbf{Q}^{\mathbf{*}}\left( \mathbf{Z}_{\mathbf{i}} \right) \right) = \int_{}^{}{\ldots\int_{}^{}{\ln\left( P\left( X,Z \right) \right)Q\left( Z_{1} \right)\ldots Q\left( Z_{i - 1} \right)Q\left( Z_{i + 1} \right)\ldots Q\left( Z_{M} \right)dZ_{1}\ldots dZ_{i - 1}dZ_{i + 1}\ldots dZ_{M}}}\\
\ln\left( Q^{*}\left( Z_{i + 1} \right) \right) = \int_{}^{}{\ldots\int_{}^{}{\ln\left( P\left( X,Z \right) \right)Q\left( Z_{1} \right)\ldots\mathbf{Q}\left( \mathbf{Z}_{\mathbf{i}} \right)Q\left( Z_{i + 2} \right)\ldots Q\left( Z_{M} \right)dZ_{1}\ldots dZ_{i}dZ_{i + 2}\ldots dZ_{M}}}
$$


下图给出变分推断的一个问题：绿色的线是原始的后验分布P，a图中，红色的线是VI求出来的Q分布（$KL(P\|Q)$），而b图中红色的线是通过交换KL散度（$KL(Q\|P)$）得到的VI结果。可以看出，a图中VI的结果在P方差最小的方向上受到了限制。

![](/img/vi2.png)



这时候VI给出的结果并不是对原始P的一个很好的估计，其原因是在KL散度上


$$
KL(P(X)|\left| Q\left( X \right) \right) = \int_{}^{}{P(X)log\frac{P(X)}{Q(X)}\text{dX}}
$$


当$P(X)$值很小的时候，$Q(X)$也必须要比较小，才能使得KL散度值比较小，那么将KL散度看做是一个泛函，通过选择$Q$来最小化最终值，所以对于给定的$P$来说，$Q$受到了限制，在$P$概率低的地方，$Q$也得概率低（对应左图），对于右图来说是对$PQ$做了一个交换，那么要求$P$有值的地方，$Q$也得有值。



# 0x01 Variational Inference examples

### 1.1 Gaussian-Gamma Distribution



假设$\mathbb{D = \{}x_{1},x_{2},\ldots,x_{n}\}$，已知


$$
P\left( \mathbb{D} \middle| \mu,\tau \right) = \prod_{i = 1}^{n}{\left( \frac{\tau}{2\pi} \right)^{\frac{1}{2}}\exp\left( \frac{- \tau}{2}\left( x_{i} - \mu \right)^{2} \right) = \left( \frac{\tau}{2\pi} \right)^{\frac{n}{2}}exp(\frac{- \tau}{2}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2})}\\
P\left( \mu \middle| \tau \right) = N(\mu_{0},\left( \lambda_{0}\tau \right)^{- 1}) \propto exp(\frac{- \lambda_{0}\tau}{2}\sum_{i = 1}^{n}\left( \mu - \mu_{0} \right)^{2})\\
P\left( \tau \right) = Gamma(\tau|a_{0},b_{0}) \propto \tau^{a_{0} - 1}exp( - b_{0}\tau)
$$




那么，后验分布为


$$
P\left( \mu,\tau \middle| \mathbb{D} \right) \propto P\left( \mu,\tau,\mathbb{D} \right) = P\left( \mathbb{D} \middle| \mu,\tau \right)\text{\ P}\left( \mu \middle| \tau \right)\text{\ P}\left( \tau \right)
$$


现在我们要做的就是用VI的方法来找出$Q$来近似后验$P$，这个后验$P$中一共有两个参数$\mu,\tau$，那么利用VI的核心公式（这里我们用log-likelihood的版本直接进行带入），分别对这两个参数进行更新：


$$
E_{\prod_{i \neq j}^{M}{Q\left( Z_{i} \right)}}\left\lbrack \ln\left( P\left( Z|X \right) \right) \right\rbrack = ln(Q^{\*}(Z_{i}))
$$


1.  更新$\mu$，利用核心的公式之后，要做的就是带入已知那些分布的形式，然后进行化解：


$$
\ln\left( Q^{\*}\left( \mu \right) \right) \propto E_{Q\left( \tau \right)}\left\lbrack \ln\left( P\left( \mathbb{D} \middle| \mu,\tau \right)\text{\ P}\left( \mu \middle| \tau \right)\text{\ P}\left( \tau \right) \right) \right\rbrack \\
= E_{Q\left( \tau \right)}\left\lbrack \ln{\left( P\left( \mathbb{D} \middle| \mu,\tau \right) \right) + \ln\left( P\left( \mu \middle| \tau \right) \right) + \ln\left( P\left( \tau \right) \right)} \right\rbrack\\
= \int_{\tau}^{\ }{\left( \ln{\left( P\left( \mathbb{D} \middle| \mu,\tau \right) \right) + \ln\left( P\left( \mu \middle| \tau \right) \right) + \ln\left( P\left( \tau \right) \right)} \right)Q\left( \tau \right)d\tau}
$$


对于不含有$\mu$的第三项来说，并不影响最后的求解，然后我们把已知的分布带入到式子中来，所以上式近似等价于下式子


$$
\propto \int_{\tau}^{\ }{\ln\left( P\left( \mathbb{D} \middle| \mu,\tau \right) \right)Q\left( \tau \right)d\tau} + \int_{\tau}^{\ }{\ln\left( P\left( \mu \middle| \tau \right) \right)Q\left( \tau \right)d\tau} \\
\propto \int_{\tau}
\left( \frac{- \tau}{2}\sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2} + \frac{- \lambda_{0}\tau}{2} \sum_{i = 1}^{n}\left( \mu - \mu_{0} \right)^{2} \right) Q(\tau)d\tau \\ 

= \int_{\tau}\left( \frac{- \tau}{2}\sum_{i = 1}^{n}{\left( x_{i} - \mu \right)^{2} + \lambda_{0}\left( \mu - \mu_{0} \right)^{2}} \right)Q(\tau)d\tau \\

= \left( \sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2} + \lambda_{0}\left( \mu - \mu_{0} \right)^{2}\right)\int_{\tau}{\left( \frac{- \tau}{2} \right)Q\left( \tau \right)d\tau}\\
=\frac{{- E}_{Q\left( \tau \right)}\left\lbrack \tau \right\rbrack}{2}\left( \sum_{i = 1}^{n}{\left( x_{i} - \mu \right)^{2} + \lambda_{0}\left( \mu - \mu_{0} \right)^{2}} \right)\\
= \frac{{- E}_{Q\left( \tau \right)}\left\lbrack \tau \right\rbrack\left( n + \lambda_{0} \right)}{2}\left( \mu - \frac{(n\overline{X} + \lambda_{0}\mu_{0})}{\left( n + \lambda_{0} \right)} \right)^{2}
$$


所以，最后的$Q^{\*}\left( \mu \right)$可以表示为


$$
Q^{\*}\left( \mu \right) = \exp\left( \frac{{- E}_{Q\left( \tau \right)}\left\lbrack \tau \right\rbrack\left( n + \lambda_{0} \right)}{2}\left( \mu - \frac{\left( n\overline{X} + \lambda_{0}\mu_{0} \right)}{\left( n + \lambda_{0} \right)} \right)^{2} \right) = N(\frac{\left( n\overline{X} + \lambda_{0}\mu_{0} \right)}{\left( n + \lambda_{0} \right)},\frac{1}{E_{Q\left( \tau \right)}\left\lbrack \tau \right\rbrack\left( n + \lambda_{0} \right)})
$$




2.  同理可以更新$\tau$



$$
\ln\left( Q^{\*}\left( \tau \right) \right) \propto E_{Q\left( \mu \right)}\left\lbrack \ln\left( P\left( \mathbb{D} \middle| \mu,\tau \right)\text{\ P}\left( \mu \middle| \tau \right)\text{\ P}\left( \tau \right) \right) \right\rbrack
$$


后面就略啦



### 1.2 LDA（Latent Dirichlet Allocation）

第二个VI的例子是用在LDA模型上进行求解，在继续做下去之前，还需要介绍**指数簇函数Exponential Family**。所谓的指数簇函数就是可以表示为如下形式的函数：


$$
P\left( X \middle| \eta \right) = H(X)exp(T\left( X \right)^{T}\eta - A(\eta))
$$




$A\left( \eta \right)$叫log-normalizer，是唯一一个不含有X的项，$\eta$叫natural parameter，如果对原式子做积分等于1的话，可以反解出A就等于其他项在x上的积分，所以A也可以看做是一个partition function。

$T(X)$叫Sufficient Statistics（充分统计量），它的意思是给定了T，我们不能再从X的概率分布中获得更多关于$\eta$的更多信息。那么**（因子分解定理）**$T(X)$为$\eta$的充分统计量的充要条件是联合概率函数$P\left( X \middle| \eta \right)$可以因子分解为如下形式：


$$
P\left( X \middle| \eta \right) = g\left( T\left( X \right),\eta \right)h(X)
$$


可以看出，指数簇函数正好满足这个形式，指数簇函数也用于无向图模型（马尔科夫随机场）中，所以可能这里的因子分解定理或许与图模型中的因子分解有一定的联系？？？

许许多多的分布都可以写成指数簇的形式，比如Gaussian分布, gamma分布, 多项式分布（可以看做事n个球落在k个桶中），二项式分布（n个球，2个桶），伯努利分布（1个球，2个桶）和categorical distribution（1个球，k个桶），泊松分布以及LDA用到的Dirichlet分布等等（详见Wikipedia）

以一维的高斯分布为例子，将它表示成指数簇的形式


$$
N\left( X,\mu,\ \sigma^{2} \right) = \left( 2\pi\sigma^{2} \right)^{- 1/2}\exp\left( \frac{{- \left( X - \mu \right)}^{2}}{2\sigma^{2}} \right) \\
= exp\left( - \frac{X^{2} - 2X\mu + \mu^{2}}{2\sigma^{2}} - \frac{ln(2\pi\sigma^{2})}{2} \right) \\
= exp\left( \frac{- 1}{2\sigma^{2}}X^{2} + \frac{\mu}{\sigma^{2}}X - \frac{\mu^{2}}{2\pi\sigma^{2}} - \frac{ln(2\pi\sigma^{2})}{2} \right) \\
= exp\left( \underset{T\left( X \right)^{T}\eta}{\overset{\begin{bmatrix}
X \\
X^{2} \\
\end{bmatrix}^{T}\begin{bmatrix}
\frac{\mu}{\sigma^{2}} \\
\frac{- 1}{2\sigma^{2}} \\
\end{bmatrix}}{︸}} - \underset{A(\eta)}{\overset{\frac{\mu^{2}}{2\pi\sigma^{2}} - \frac{ln(2\pi\sigma^{2})}{2}}{︸}} \right)
$$




这里，一维高斯的参数是$\eta = \begin{bmatrix}
\eta_{1} \\
\eta_{2} \\
\end{bmatrix} = \begin{bmatrix}
\frac{\mu}{\sigma^{2}} \\
\frac{- 1}{2\sigma^{2}} \\
\end{bmatrix}$，那么在做一个替换可以得到$\begin{bmatrix}
\mu \\
\sigma^{2} \\
\end{bmatrix} = \begin{bmatrix}
\frac{- \eta_{1}}{{2\eta}_{2}} \\
\frac{- 1}{{2\eta}_{2}} \\
\end{bmatrix}$，在带入得到$A\left( \eta \right) = \frac{- \eta_{1}^{2}}{4\eta_{2}} - \frac{1}{2}ln( - 2\eta_{2})$

高斯的原始表达形式和高斯指数簇表达形式之间有什么差别？换句话说就是，用指数簇的表达形式有什么好处? 下面呢就给出指数簇具有的很多很好的性质，通过表现成这样的形式，就可以享用它带来的这些方便：

**性质1：矩生成（The Moment Generating Property）**


$$
\frac{\partial^{d}A(\eta)}{\partial\eta^{d}} = E_{P(X|\eta)}\lbrack T\left( X \right)^{d}\rbrack
$$


-   Log-normalizer 对Natural parameter的第d阶导数等于充分统计量$T(X)$在分布$P(X|\eta)$下的d阶中心距。那么d等于1就是期望，等于2就是方差。

-   Log-normalizer 对Natural parameter的2阶导数（就是方差）不小于0，那么Log-normalizer是凸函数



**性质2：MLE**


$$
\eta^{\*} = \text{argmax\ log}\left( P\left( X \middle| \eta \right) \right) = argmax\ log\left( P\left( \prod_{i = 1}^{n}X_{i} \middle| \eta \right) \right) \\
= argmax\log\left( \prod_{i = 1}^{n}{H\left( X_{i} \right)\exp\left( T\left( X_{i} \right)^{T}\eta - A\left( \eta \right) \right)} \right) \\
\propto \text{argmax}\sum_{i = 1}^{n}{T\left( X_{i} \right)^{T}\eta - A\left( \eta \right) \\
= argmax\left\lbrack \left( \sum_{i = 1}^{n}{T\left( X_{i} \right)^{T}} \right)\eta - nA\left( \eta \right) \right\rbrack}
$$


然后对上式求导，等于0，有


$$
\frac{\partial\left\lbrack \left( \sum_{i = 1}^{n}{T\left( X_{i} \right)^{T}} \right)\eta - nA\left( \eta \right) \right\rbrack}{\partial\eta} = \sum_{i = 1}^{n}{T\left( X_{i} \right)^{\ } - {nA}^{'}\left( \eta \right)} = 0\\
A^{'}\left( \eta \right) = \frac{1}{n}\sum_{i = 1}^{n}{T\left( X_{i} \right)^{\ }}
$$


这里求出的值，与期望的表达式之间有点相似，但是，这里MLE是说在给定了数据X之后，natural parameter的导数$A^{'}\left( \eta \right)$满足等于充分统计量取值的平均。这个表现形式很简介，我们仍然以一维高斯来说明，用一维高斯的原始形式来做MLE与直接用指数簇的形式来做对比（当然了，这两种的结果必然是相同的，因为我们只是改变了式子的表达形式而已，并没有对分布本身做修改额）：

1.  原始形式的MLE


$$
\mu_{\text{MLE}} = argmax\left\lbrack \sum_{i = 1}^{n}{logP(x_{i}|\mu,\sigma^{2})} \right\rbrack = argmax\left\lbrack \sum_{i = 1}^{n}{log\left( \frac{1}{\sqrt{2\pi\sigma^{2}}}exp(\frac{- \left( x_{i} - \mu \right)^{2}}{2\sigma^{2}}) \right)} \right\rbrack
$$


对上式关于$\mu$求偏导，等于0，即


$$
\frac{\partial\left\lbrack \sum_{i = 1}^{n}{log\left( \frac{1}{\sqrt{2\pi\sigma^{2}}}exp(\frac{- \left( x_{i} - \mu \right)^{2}}{2\sigma^{2}}) \right)} \right\rbrack}{\partial\mu} = \frac{\partial\left\lbrack \sum_{i = 1}^{n}\frac{- \left( x_{i} - \mu \right)^{2}}{2\sigma^{2}} \right\rbrack}{\partial\mu} \\
= \frac{- 1}{2\sigma^{2}}\frac{\partial\left\lbrack \sum_{i = 1}^{n}\left( x_{i} - \mu \right)^{2} \right\rbrack}{\partial\mu} = \frac{1}{\sigma^{2}}\sum_{i = 1}^{n}\left( x_{i} - \mu \right) = \frac{\sum_{i = 1}^{n}x_{i} - n\mu}{\sigma^{2}} = 0
$$


得到即是样本均值


$$
\mu_{\text{MLE}} = \frac{1}{n}\sum_{i = 1}^{n}x_{i}
$$


同理，用同样的方法对$\sigma^{2}$求偏导，可以解得等于样本方差


$$
\sigma_{\text{MLE}}^{2} = \frac{1}{n}\sum_{i = 1}^{n}{{(x}_{i} - \mu)}^{2}
$$


2.  指数簇形式的MLE，已知下式成立



$A^{\'}\left( \eta \right) = \frac{1}{n}\sum_{i = 1}^{n}{T\left( X_{i} \right)^{\ }}\ \ \ \ \eta = \begin{bmatrix}
\eta_{1} \\
\eta_{2} \\
\end{bmatrix} = \begin{bmatrix}
\frac{\mu}{\sigma^{2}} \\
\frac{- 1}{2\sigma^{2}} \\
\end{bmatrix}$ $A\left( \eta \right) = \frac{- \eta_{1}^{2}}{4\eta_{2}} - \frac{1}{2}ln( - 2\eta_{2})$

$$A^{\'}\left( \eta \right) = \begin{bmatrix}
\frac{{\partial A(\eta)}_{\ }}{\partial\eta_{1}} \\
\frac{{\partial A(\eta)}_{\ }}{\partial\eta_{2}} \\
\end{bmatrix} = \begin{bmatrix}
\frac{- \eta_{1}}{{2\eta}_{2}} \\
\frac{\eta_{1}^{2}}{4\eta_{2}^{2}} - \frac{1}{2\eta_{2}} \\
\end{bmatrix} = \begin{bmatrix}
\frac{- \eta_{1}}{{2\eta}_{2}} \\
\left( \frac{- \eta_{1}}{{2\eta}_{2}} \right)^{2} - \frac{1}{2\eta_{2}} \\
\end{bmatrix} = \begin{bmatrix}
\frac{\sum_{i = 1}^{n}x_{i}}{n} \\
\frac{\sum_{i = 1}^{n}x_{i}^{2}}{n} \\
\end{bmatrix} = \frac{1}{n}\sum_{i = 1}^{n}{T\left( X_{i} \right)^{\ }}$$



所以可以得到$\eta_{2}$，进一步可以解出$\eta_{1}$


$$
\left( \frac{\sum_{i = 1}^{n}x_{i}}{n} \right)^{2} - \frac{1}{2\eta_{2}} = \frac{\sum_{i = 1}^{n}x_{i}^{2}}{n}
$$


然后可以反解出$\mu 和\sigma^{2}$，这里就不在推倒下去了。。。有兴趣的自己推倒咯



**性质3：共轭**

先说说什么共轭。它是针对似然和先验来说的，如果先验和后验属于同种分布（参数不同的同种分布），那么先验和似然就叫共轭分布。比如，因为Dirichlet分布和多项式分布是共轭的，所以当先验分布P(B)服从Dirichlet分布，似然函数P(X\|B)服从多项式分布时，后验分布P(B\|X)也会是Dirichlet分布（这正是LDA所涉及的知识点）

对于指数簇分布来说，如果先验分布式指数簇形式，那么一定存在一个似然函数使得后验分布也可以写成指数簇分布的形式，并且这个似然也是指数簇的形式。

为了证明这一结论的正确性，首先假设存在一个似然函数$P\left( X \middle| B \right)$和先验分布$P\left( B \middle| \alpha \right)$满足以下形式，我们的目标就是证明，对应的后验分布$P\left( B \middle| X \right)$与先验共轭


$$
P\left( X \middle| B \right) = h\left( X \right)\exp\left( T\left( X \right)^{T}B - A\left( B \right) \right)\\
P\left( B \middle| \alpha \right) = h(B)exp(T\left( B \right)^{T}\alpha - A(\alpha))\\
P\left( B \middle| X \right) \propto P\left( X \middle| B \right)P\left( B \right) \\
= h\left( X \right)\exp\left( T\left( X \right)^{T}B - A\left( B \right) \right)h\left( B \right)\exp\left( T\left( B \right)^{T}\alpha - A\left( \alpha \right) \right) \\
\propto h\left( B \right)\exp\left( T\left( X \right)^{T}B - A\left( B \right) + T\left( B \right)^{T}\alpha \right)
$$




现在我们把$T\left( B \right)^{\ }$和$\alpha$分成两个部分，即写成以下表示


$$
T\left( B \right) = \begin{bmatrix} B \\ -g(B) \end{bmatrix}\\
\alpha = \begin{bmatrix}
\alpha_{1} \\
\alpha_{2} \\
\end{bmatrix}
$$


那么，我们将这个表示带入到原来的公式中，得到


$$
h\left( B \right)\exp\left( T\left( X \right)^{T}B - A\left( B \right) + \alpha_{1}B - g(B)\alpha_{1} \right) \\
= h\left( B \right)\exp\left( \left( T\left( X \right)^{T}{+ \alpha}_{1} \right)B - \left( A\left( B \right) + g(B)\alpha_{2} \right) \right)\\
\overset{令g\left( B \right) = A\left( B \right)}{\Leftrightarrow}h\left( B \right)\exp{\left( \left( T\left( X \right)^{T}{+ \alpha}_{1} \right)B - A\left( B \right)\left( 1 + \alpha_{2} \right) \right) \\
= h\left( B \right)\exp\left( \widetilde{\alpha_{1}}B - A\left( B \right)\widetilde{\alpha_{2}} \right)} \\
= h(B)exp(\left\{ \widetilde{\alpha_{1}},\widetilde{\alpha_{2}} \right\}^{T}T\left( B \right))
$$


通过$g(B)$的特殊构造（$令g\left( B \right) = A\left( B \right)$），即令先验的充分统计量的第二项等于似然函数的log normalizer的话，这样算出来的后验就和先验共轭，和先验的区别就在于a的不同，所以是共轭的（一定要注意的是这个结论的前提条件一定是似然的log normalizer要等于先验的sufficient statistics的第二项）。所以理论上来说，指数簇分布都可以找到满足这个条件的共轭指数簇分布



**性质4：Variational Inference**

如果后验分布p是指数簇形式，那么，用同样也是指数簇形式的分布q，通过不断调整q中的参数来做变分推断近似p的话，将简便不少。回想做VI，我们是通过最大化ELOB来使得分布Q与P之间的KL散度最小，这样Q就是P的一个很好的近似。现在假设Q(Z')中Z'的各个成分由Z和B组成，现在ELOB可以写成


$$
\mathcal{L}\left( Q(Z') \right)\mathcal{= L}\left( Q(Z,B) \right)\\
= \int_{}^{}{Q\left( Z,B \right)\ln\left( P\left( X,Z,B \right) \right)\text{dZdB}} - \int_{}^{}{Q\left( Z,B \right)\ln\left( Q\left( Z,B \right) \right)\text{dZdB}} \\
= E_{Q\left( Z,B \right)}\left\lbrack \ln\left( P\left( X,Z,B \right) \right) \right\rbrack - E_{Q\left( Z,B \right)}\left\lbrack \ln\left( Q\left( Z,B \right) \right) \right\rbrack
$$


我们的目标就是找出一个分布$Q(Z,B)$来使得ELOB最大化，换句话说就是要对Q的参数Z和B做求解（因为Q函数的形式是自己定义的，并且为了简便还假设了$Q(Z,B) = Q(Z)Q(B)$）。但是与一开始我们介绍VI的推倒时的例子不一样，这里我们的Q函数中不只有一个变量，所以直接对上式进行求导的方式是求不出来的，所以这里采用的是Coordinate Ascent，也就是说先固定一个变量，再对另一个进行求解。

在正式求解之前，现在我们引入指数簇，对于联合分布函数$P\left( X,Z,B \right)$有两个后验分布$P\left( B|X,Z \right)$和$P\left( Z|B,X \right)$, 假设这里的后验分布都是指数簇，即可写为如下的形式：


$$
P\left( B|X,Z \right) = h(B)exp(T\left( B \right)^{T}\eta\left( Z,X \right) - A(\eta\left( Z,X \right)))\\
P\left( Z|X,B \right) = h(Z)exp(T\left( Z \right)^{T}\eta\left( B,X \right) - A(\eta\left( B,X \right)))
$$




那么对应的，我们想要的Q函数来近似


$$
Q\left( B|\lambda \right) = h(B)exp(T\left( B \right)^{T}\lambda - A(\lambda))\\
Q\left( Z|\phi \right) = h(Z)exp(T\left( Z \right)^{T}\phi - A(\phi))

$$


1. 求解$\lambda$，固定$\phi$ （求解B，固定Z）



$$
\mathcal{L}\left( Q(Z,B) \right) = E_{Q\left( Z,B \right)}\left\lbrack \ln\left( P\left( X,Z,B \right) \right) \right\rbrack - E_{Q\left( Z,B \right)}\left\lbrack \ln\left( Q\left( Z,B \right) \right) \right\rbrack \\
= E_{Q\left( Z,B \right)}\left\lbrack \ln{\left( P\left( B|X,Z \right) \right) + ln\left( P\left( X,Z \right) \right)} \right\rbrack - E_{Q\left( Z,B \right)}\left\lbrack \ln\left( Q\left( Z|\phi \right) \right) \right\rbrack - E_{Q\left( Z,B \right)}\left\lbrack \ln\left( Q\left( B|\lambda \right) \right) \right\rbrack \\
\propto E_{Q\left( Z,B \right)}\left\lbrack \ln\left( P\left( B|X,Z \right) \right) \right\rbrack - E_{Q\left( Z,B \right)}\left\lbrack \ln\left( Q\left( B|\lambda \right) \right) \right\rbrack
$$




带入，得


$$
E_{Q\left( Z,B \right)}\left\lbrack \ln\left( h(B)exp(T\left( B \right)^{T}\eta\left( Z,X \right) - A(\eta\left( Z,X \right))) \right) \right\rbrack - E_{Q\left( Z,B \right)}\left\lbrack \ln\left( h\left( B \right)\exp\left( T\left( B \right)^{T}\lambda - A\left( \lambda \right) \right) \right) \right\rbrack \\
= E_{Q\left( Z,B \right)}\left\lbrack T\left( B \right)^{T}\eta\left( Z,X \right) \right\rbrack - E_{Q\left( Z,B \right)}\left\lbrack T\left( B \right)^{T}\lambda \right\rbrack - E_{Q\left( Z,B \right)}\lbrack A\left( \eta\left( Z,X \right) \right\rbrack + E_{Q\left( Z,B \right)}\left\lbrack A\left( \lambda \right) \right\rbrack \\
\propto E_{Q\left( Z \right)}\left\lbrack \eta\left( Z,X \right) \right\rbrack E_{Q\left( B \right)}\left\lbrack T\left( B \right)^{T} \right\rbrack - E_{Q\left( B \right)}\left\lbrack T\left( B \right)^{T}\lambda \right\rbrack + A\left( \lambda \right)\text{\ \ \ \ \ }
$$


由指数簇的期望性质有：



$$
\frac{\partial A(\eta)}{\partial\eta} = E_{P(X|\eta)}\lbrack T(X)\rbrack
$$


所以，再次带入简化


$$
{\mathcal{L}\left( Q(Z,B) \right) \propto E}_{Q\left( Z \right)}\left\lbrack \eta\left( Z,X \right) \right\rbrack E_{Q\left( B \right)}\left\lbrack T\left( B \right)^{T} \right\rbrack - E_{Q\left( B \right)}\left\lbrack T\left( B \right)^{T}\lambda \right\rbrack + A\left( \lambda \right) \\
= E_{Q\left( Z \right)}\left\lbrack \eta\left( Z,X \right) \right\rbrack\left( A^{'}\left( \lambda \right) \right)^{T} - \lambda\left( A^{'}\left( \lambda \right) \right)^{T} + A\left( \lambda \right)
$$


现在就可以对上式求导等于0，得出最后的解


$$
\left( A\'\'( \lambda) \right)^{T} E_{Q(Z)}[\eta\left(Z,X \right)]- \lambda\left( A\'\'\left( \lambda \right) \right)^{T} = 0\\
{\lambda = E}_{Q\left( Z\|\phi \right)}\left\lbrack \eta\left( Z,X \right) \right\rbrack
$$


这个式子的含义在于，假设我要用q(B\|$λ$)来近似$p(B\|Z,X)$，并且p是指数簇函数分布的话，我通过不断的更新$\lambda$的值来近似p中的参数，而$\lambda$的更新方式是上图所示，是对p中的参数在Q(Z)的分布下求一个期望，这里的Q(Z)是指除去B部分的分布。

2.  同理，固定$\lambda$，求解$\phi$ （固定B，求解Z）


$$
{\phi = E}_{Q\left( B\|\lambda \right)}\left\lbrack \eta\left( B,X \right) \right\rbrack
$$


**LDA 求解**

扯了这么多，回到一开始要做的，对LDA用VI进行求解，LDA是一个很经典的topic model，下图是LDA对应的图模型：

![](/img/vi3.png)

LDA生成文档的过程

:对于每一个文档d{

​       $\Theta_{d}\sim Dir\left( \alpha_{1},\ldots,\alpha_{k} \right)$，控制生成主题集合（Dir指服从Dirichlet分布）;

​        对于文档d中的每一个词n{

​                  $Z_{d,n}\sim Multi(\Theta_{d})$，<span class="underline">从集合中为这个词确定一个主题</span>;（Dirichlet分布是"分布的分布"，它的每次取值会得到一个离散的分布，如图所示，准确的说这里应该是服从categorical 分布，但是也算是一种多项式分布）

​                $W_{d,n}\sim Multi(\beta_{Z_{d,w}})$，<span class="underline">从这个主题中，选出一个词</span>。（一个主题就相当于是词的分布，并且$\beta 也是服从\text{Dirichlet}分布$）;

​        }

}

![](/img/vi4.png)

对LDA来说，我们的待求解的参数是$\left\{ Z_{d,n}，\beta_{k}，\Theta_{d} \right\}$，其中


$$
\Theta_{d}\sim Dir\left( \gamma_{d} \right)\text{\ \ \ }\\
\beta_{k}\sim Dir\left( \lambda_{k} \right)\text{\ \ }\\
Z_{d,n}\sim Multi\left( \Theta_{d1},\Theta_{d2},\ldots,\Theta_{\text{dk}} \right)\sim Multi(\varphi_{d,n}^{k})
$$


假设每一篇文档中所含的词的数目是相等的，那么对于D篇文章来说，LDA参数个数是很多很多的，求解LDA可以用MCMC和VI两种方式，这里我们用VI进行求解（这里同样采用Coordinate Ascent）。

-   固定其他，求解$Z_{d,n}$

> 假如一共有k个主题，那么第d个文档的第n个单词的主题$Z_{d,n}$服从多项式分布，$Z_{d,n}\sim Multi\left( \Theta_{d1},\Theta_{d2},\ldots,\Theta_{\text{dk}} \right)\sim Multi(\varphi_{d,n}^{k})$，那么$Z_{d,n}$的后验为$P(Z_{d,n} = k|\Theta_{d},w_{d,\ n},\beta_{k})$。已知LDA的概率图模型，根据有向图模型中的独立性(给定$w_{d,\ n}$，$Z_{d,n}$和$\beta_{k}$是不独立的，不给定$w_{d,\ n}$，则独立；对$w_{d,\ n}$来说，给定他的一个父节点$Z_{d,n}$，则$w_{d,\ n}$与$\Theta_{d}独立$)，可以对后验写成：
>
> 
> $$
> P\left( Z_{d,n} = k \middle| \Theta_{d},w_{d,\ n},\beta_{k} \right) \\
>  \propto P\left( Z_{d,n} = k,\Theta_{d},w_{d,\ n},\beta_{k} \right)\\
>  = P\left( \Theta_{d} \right)P\left( Z_{d,n} \\
>  = k \middle| \Theta_{d} \right)P\left( \beta_{k} \middle| Z_{d,n} \\
>  = k,\Theta_{d} \right)P\left( w_{d,\ n} \middle| \beta_{k},Z_{d,n} = k,\Theta_{d} \right)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \\  \propto P(Z_{d,n} = k|\Theta_{d})P\left( w_{d,\ n} \middle| \beta_{k},Z_{d,n} = k \right)
> $$
> 



我们把上式写成了指数簇的形式。


$$
P\left( Z_{d,n} = k \middle| \Theta_{d},w_{d,\ n},\beta_{k} \right)\\
\propto P\left( Z_{d,n} = k \middle| \Theta_{d} \right)P\left( w_{d,\ n} \middle\| \beta_{k},Z_{d,n} = k \right) \\
= exp(\underset{\eta(\Theta_{d},\beta_{k})}{\overset{\text{logP}\left( Z_{d,n} = k \middle\| \Theta_{d} \right) + logP\left( w_{d,\ n} \middle| \beta_{k},Z_{d,n} = k \right)\ }{︸}}*\underset{T(X)}{\overset{1}{︸}})
$$


那么就可以带入公式${\lambda = E}_{Q\left( Z\|\phi \right)}\left\lbrack \eta\left( Z,X \right) \right\rbrack$咯：


$$
\varphi_{d,n}^{k} = E_{Q(\Theta_{d},\beta_{k})}\left\lbrack \eta\left( \Theta_{d},\beta_{k} \right) \right\rbrack \\
= E_{Q(\Theta_{d},\beta_{k})}\left\lbrack \text{logP}\left( Z_{d,n} \\
= k \middle| \Theta_{d} \right) + logP\left( w_{d,\ n} \middle| \beta_{k},Z_{d,n} = k \right)\  \right\rbrack \\
= E_{Q(\Theta_{d})}\left\lbrack \text{logP}\left( Z_{d,n} = k \middle| \Theta_{d} \right)\rbrack + E_{Q(\beta_{k})}\lbrack logP\left( w_{d,\ n} \middle| \beta_{k},Z_{d,n} = k \right)\  \right\rbrack
$$


又因为Dirichlet有如下的性质：

> 如果$\left( X_{1},X_{2},\ldots,X_{n} \right)\sim Dir(\alpha_{1},\alpha_{2},\ldots,\alpha_{n})$，那么
>
> $E\left\lbrack X_{i} \right\rbrack = \frac{\alpha_{i}}{\sum_{i = 1}^{n}\alpha_{i}}$ $E\lbrack{log(X}_{i})\rbrack = \psi(\alpha_{i}) - \psi(\sum_{i = 1}^{n}\alpha_{i})$

其中$\psi(\alpha_{i})$是di-gamma函数，$\psi\left( \alpha_{i} \right) = \frac{d(log(\tau(\alpha_{i})))}{d\alpha_{i}} = \frac{\tau'(\alpha_{i})}{\tau(\alpha_{i})}$，$\tau\left( \alpha_{i} \right)是\text{gamma}函数$

所以，带入上式可以得到最终$\varphi_{d,n}^{k}$的迭代式子：


$$
\varphi_{d,n}^{k} = E_{Q(\Theta_{d},\beta_{k})}\left\lbrack \eta\left( \Theta_{d},\beta_{k} \right) \right\rbrack \\
= \psi\left( \gamma_{d,k} \right) - \psi\left( \sum_{k = 1}^{n}\gamma_{d,k} \right) + \psi(\lambda_{k,w_{d,\ n}}) - \psi(\sum_{w_{d,\ n}}^{\ }\lambda_{k,w_{d,\ n}})
$$


需要注意的是：VI 都是独立的处理每一个参数，这样的参数算出来的时候,可能加起来不等于1,所以最后需要有一个归一化操作。

同理，其他参数就不再计算啦
