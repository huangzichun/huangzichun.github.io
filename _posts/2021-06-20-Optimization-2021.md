---
layout:     post
title:      机器学习中的那些优化算法
subtitle:   优化算法の笔记
date:       2021-06-20
author:     HC
header-img: img/d1b8fb912d33d3641d3e87b61be08d20.jpeg
catalog: true
tags:
    - Optimization in machine learning
    - First-order Methods
---

> 历史笔记迁移系列
>
> Word转markdown真的好累

# 0x00. 说在前面的话

## 0x00_1. Convex Optimization Programming

凸优化问题定义为下面这个形式


$$
\min_{x \in D} f(x) \\
s.t. g_i(x) \le 0, i=1,2,...,m\\
      Ax=b
$$


其中：

- $f(x)$是我们要最小化的目标函数，一般使用$x^{*} $表示**最优解（Optimal Value）**
- $D$表示**Optimization Domain**，一般这不会明显的直接给出来，但可以表示成$D=domain(f) \cap domain(g_{i})$。在$D$中，并且满足约束条件的点叫做**可行点（feasible point）**
- $g_{i}$表示**不等式约束（inequality constraints）**，他是一个凸函数。如果存在一个$x_{0}$，使得$g_{i}(x_{0})=0$，那么称$g_{i}$是Active的。
- $X_{opt}$是解（solution）的集合，并且是一个**凸集（convex set）**。如果$X_{opt}$中只有一个元素，则称为**Unique solution**
- $\epsilon-suboptimal$是指一个满足$f(x) \le f^{*}+\epsilon$的可行点$x$
- **局部最优和全局最优（Locally / Global Optimal）**的区别在于，一个可行点是否在一个局部的领域里能取到solution。比如，存在一个$R>0$，使得$f(x)\le f(y)$对所有满足$\parallel x-y \parallel_{2} \le R $的$y$都成立。对于全局最优来说，他的solution是在整个optimization domain上。对于凸优化来说，局部最优就是全局最优。
- **如果$f(x)$是严格凸的**（即对于任意$x \neq y$和$t\in(0,1)$来说，满足$f(tx+(1-t)y)<tf(x) + (1-t) f(y)$ ），**那么他的solution是唯一的**。假设solution不唯一，存在$x\neq y$，使得$f(x)=f(y)=f^{*}$，那么可以推出$f(tx + (1-t)y) < tf^{*} + (1-t)f^{*}=f^{*}$，即$f^{*}$不是最优解。所以solution一定是唯一的。



## 0x00_2. Partial Optimization

给定$f$在变量$(x, y)$上是convex的，并且$C$也是凸集，那么，$g(x)=\min_{y \in C}⁡ f(x,y)$在$x$上也是convex的。因此我们常常对一个凸问题进行局部优化以保证他的**凸性(retain convexity)**，这也是坐标下降等算法的理论动机。即，如果下面这个原始凸问题可以对原始变量$x$进行分解（decompose）成$x=(x_{1},x_{2})$


$$
\min_{x_1,x2} f(x_1,x_2)\\
s.t. g_1(x_1) \le 0, g_2(x_2) \le 0
$$


那么，下面这个问题也是凸的


$$
\min_{x_1} \widetilde{f}(x_1)\\
s.t. g_1(x_1) \le 0, \widetilde{f}(x_1)=min\{f(x_1,x_2): g_2(x_2) \le 0\}
$$




## 0x00_3. Formation

凸优化是优化问题中的一大类，下图给出了各种子问题的层次结构，其中包括**线性规划（LP, Linear Programming）**，**二次规划（QP, Quadratic Programming）**，**二阶锥规划（SOCP, Second-order Cone Programming）**和**半定规划（SDP, Semi-definite Programming）**

![](/img/youhua1.png)



下面是各类问题的定义

| 类型   | 形式                      | 说明                          |
| ---- | ----------------------- | --------------------------- |
| LR   | ![](/img/youhua2.png) | ![](/img/youhua3.png)     |
| QP   | ![](/img/youhua4.png) | Q是对称半正定矩阵。也只有Q是半正定的时候，才是凸问题 |
| SDP  | ![](/img/youhua5.png) | ![](/img/youhua6.png)     |
| SOCP | ![](/img/youhua7.png) |                             |





# 0x01 First-order Methods

## 0x01_1 Gradient Descent

梯度下降，无约束优化中的常用方法，假设**函数$f(x)$是可微的光滑凸函数**，满足$domain(f)=R^{n}$，下式的优化目标形式成为无约束优化


$$
\min f(x)
$$


为了求解出这个 $ x^{*} $ $，我们可以直接求解$\bigtriangledown f(x^{*} )=0$，但是不容易直接求解出来的。然后我们可以采用迭代的方式，找出一组$x$的序列$x^{0}, x^{1},$满足当$k \rightarrow \infty$时，$f(x^{k}) \rightarrow f(x^{*})$。梯度下降便是这样的迭代算法，给定初始$x^{0}$，该算法的$x$序列设置为


$$
x^k = x^{k-1} - t_k \nabla f(x^{k-1})
$$


其中，$k$是迭代次数，$t_{k}>0$是每次迭代的步长，这样的更新方式可以保证每次迭代之后的函数值都有所减小，即通过泰勒展开，以下式子成立


$$
f(x^k) = f(x^{k-1} - t_k \nabla f(x^{k-1})) \\
\approx f(x^{k-1}) - t_k \nabla f(x^{k-1})^T f(x^{k-1}) \\
\le f(x^{k-1})
$$


需要注意的是，**梯度下降算法可能会收敛到局部最优解，这取决于目标函数是否是convex**的

![](/img/youhua8.png)



下面来看看梯度下降的更新式子是怎么来的。一般来说，有两种视角。



**视角1**：假设函数$f$是二阶可微的，那么用泰勒公式进行展开，其中，$1\le \theta \le 1$


$$
f(y)≈f(x)+∇f(x)^T (y-x)+\frac{1}{2} (y-x)^T ∇^2 f(θ(x-y)+y)(y-x)
$$


现在对$∇^{2} f(θ(x-y)+y)$用$\frac{1}{t}I$进行二级近似，那么得到


$$
f(y)≈f(x)+∇f(x)^T (y-x)+\frac{1}{2t} \parallel y-x \parallel_2^2=g(y)
$$


为了使得函数值在每次迭代中降低，所以需要$y$的值，使得$∇g(y)=0$，等价于$∇f(x) +\frac{1}{t} (y-x)=0  \leftrightarrow y=x-t∇f(x)$



**视角2**：考虑泰勒展开$f(y) \approx f(x) + \bigtriangledown f(x)^{T}(y-x)$，记$(y-x)=ad_{k}$，$a$是给定的一个数。若$∇f(x)^{T} d_{k}<0$，这方向$d_{k}$则是我们要的使得函数值下降的方向。另外，结合Cauchy-Schwartz不等式，得到


$$
\| \bigtriangledown f(x)^T d_k \| \le \| \bigtriangledown f(x)^T \| \| d_k \|
$$


所以当且仅当$d_{k}=-∇f(x)^{T}$时，函数值下降得最快，所以负梯度方向是最速下降方向。

梯度下降中，我们是沿着负梯度的方向进行迭代更新，但是每次更新的步长选择也很重要。步长控制着前后两次取点的大小，如果步长太大，算法可能不会收敛，而产生很大的震荡，如果步长太小，则算法收敛的时间将变得很慢，如下图所示。所以如何选择合适的步长，在实际使用中至关重要，下面介绍两种方式：**Backtracking line search**和**Exact line search**

![](/img/youhua9.png)



**Exact line search**：在实际编程使用中并不太常用，因为他对步长还求了一次最小，使得函数值进一步降低。但是这个步骤还不如一般的BLS的方法高效，但是他比较适用于教学与考试。**Backtracking line search（BLS）**：旨在更新的过程中缩小步长，控制步长变得太大

![](/img/youhua10.png)

当步长t足够小的时候，他应该会满足下式


$$
f(x-t∇f(x))≈(f(x)-t \parallel ∇f(x)\parallel_2^2 )<f(x)-at‖∇f(x)‖_2^2
$$


所以当步长太大时，BLS算法会缩短步骤，否则就选取下一个迭代点。下图是BLS对比一般梯度下降的结果图，上图原函数，下图为每次迭代后的函数值，GD采用固定步长t=0.9（这里t=1的话，不会收敛），BLS的步长初始化为t=1，两个参数$a$,$β$都设为0.5

![](/img/youhua11.png)



## 0x01_2. Subgradient Method

梯度下降虽然简单实用，但是遇到不可微的函数，那就捉襟见肘了。事实上在这种情况下，还有次梯度Subgradient的存在。我们说一个函数$f(x)$是凸函数，则他的一阶充要条件是函数$f$可行域是凸集，并且在可行域中的任意点都满足$f(y) \ge f(x) + \bigtriangledown f^{T} (x) (y-x)$。即对于一个凸函数来说，其一阶泰勒近似实质上是原函数的一个全局下估计，subgradient的定义与这个很相似，一个凸函数$f$在点$x$处的**subgradient**定义为，满足以下条件的任意$g \in R^{n}$


$$
f(y) \ge f(x) + \bigtriangledown g^T (y-x) , \forall y
$$


记$∂f(x)$表示在点$x$处，函数f的subgradient能取到的所有值的集合，值得一提的是$∂f(x)$也是闭的凸集。另外，如果函数$f$在$x$点处可微，那么这点的subgradient就是这点的gradient，否则，subgradient仍然存在，即无论什么情况，subgradient都希望达到全局下估计。另外对于不是convex的函数，subgradient也是可能存在的。下面给出几个subgradient的例子

**TODO**



## 0x01_3. Stochastic Subgradient Method and Batch Method

### SGD & mini-batch



### Momentum

### Nesterov Accelerated Gradient

### Adagrad

### Adadelta

### Adam

### AdaMax

### Nadam



## 0x01_4. Proximal Gradient Method

proximal mapping



soft-thresholding operator



Lasso的著名算法Iterative soft-thresolding algorithm (ISTA)



例子

1. projected gradient descent
2. proximal minimization algorithm

## 0x01_5. Accelerated Proximal Gradient Method

Nesterov 

## 0x01_6. Conclusion of Convergence Analysis

Lipschitz continuous



Strongly convex



总结

- 梯度下降的收敛性

- 次梯度下降的收敛性

- SGD的收敛性

- Proximal Gradient Descent的收敛性

- Accelerated Proximal Gradient Descent的收敛性

  ​

## 0x01_7. Parallelizing & Distributed SGD

### HogWild!

### Downpour SGD

### Delay-tolerant

### Elastic Averaging

## 0x01_8. Strategies for Optimizing SGD

1. Shuffling 
2. Batch Norm
3. Early stopping
4. Gradient Noise

# 0x02 Duality Method

## 0x02_1. Duality in Linear Programs and General Form

Duality Problem



Lagrange dual function



例子：

1. QP
2. SVM
3. dual Norm



## 0x02_2 KKT conditions

- stationarity condition
- complementary condition
- primal feasibility
- dual feasibility

例子：svm



## 0x02_3 Duality Uses and Correspondences

Conjugate function

Conjugate function for Duality

# 0x03 Second-order Method

## 0x03_1 Newton Method

## 0x03_2 Barrier Method

## 0x03_3 Primal-dual Interior Point Method

## 0x03_4 Proximal Newton Method



# 0x04 Others and Application

## 0x04_1 Generalized Lasso Problem

## 0x04_2 Dual Methods and ADMM

dual subgradient



dual deconposition



Augmented Lagrangian Method



ADMM



例子：

- Norm regularization
- lasso

## 0x04_3 Distributed  ADMM 

## 0x04_4 Coordinate Descent

例子

1. linear regression

2. lasso regression

3. box-constrained regression

4. svm

   ​

## 0x04_5 Conditional Gradient Method

Frank-Wolfe Method

例子：

1. losso
2. Lp norm

## 0x04_6 Cutting-plane Method

## 0x04_7 Non-convex Problem

