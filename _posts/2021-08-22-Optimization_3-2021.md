---
layout:     post
title:      机器学习中的那些优化算法
subtitle:   优化算法の笔记（下） - ADMM & Coordinate Descent
date:       2021-08-22
author:     HC
header-img: img/32040b9902e7378d6996d46135af3cfa.jpeg
catalog: true
tags:
    - Optimization in machine learning
    - ADMM
    - Coordinate Descent
---

> 历史笔记迁移系列
>
> Word转markdown真的好累

第一集：[优化算法の笔记（上）- First Order Methods](http://huangc.top/2021/06/20/Optimization_1-2021/)

第二集：[优化算法の笔记（中） - Duality Method & Seconds Order Methods](http://huangc.top/2021/07/31/Optimization_2-2021/)

第三集：[优化算法の笔记（下） - ADMM & Coordinate Descent](http://huangc.top/2021/08/22/Optimization_3-2021/)

# 0x04 Dual Methods and Application

我们回忆一下，给定一个函数$f$，他的共轭函数定义为：


$$
f^*(y) = \max_{x} y^Tx - f(x)
$$


共轭函数是关于$y$的一些凸函数的逐点上确界，下面是他的一些性质：

1. 对于任意的$x,y$，满足$f(x) + f^{*}(y) \le x^{T}y$
2. 对于任意函数$f$，他的共轭函数的共轭函数，值不大于原函数：$f^{\*\*}(x) \le f(x)$
3. 如果函数$f$是凸的或者凹的，那上面两个性质都可以取等号，并且，满足$x \in \partial f\*(y)  \leftrightarrow x \in \arg\min _{z} f(z) - y^{T}z$。



共轭函数常常用于推导问题的对偶形式，但是实际上，当我们很想用对偶来求解问题的时候，这个共轭函数却不给力了，表现为很难给出一个闭式解来推得对偶形式，这时候就是**dual-based subgradient** or **gradient methods**起作用的时候了。考虑下面的优化目标，其中，$f$是closed的凸函数


$$
\min_x f(x) \\
s.t. Ax=b
$$


构造Lagrangian multiplier，可以求出其对偶形式为下式，其中$f^\*$是共轭函数，记$f\* (-A^{T} u)=g(u), ∂g(u)=-A∂f\* (-A^{T} u)$。


$$
\max_u -f^*(-A^Tu) - b^Tu = -g(u) - b^Tu
$$


**Dual subgradient算法就在于处理，我们无法直接求出对偶形式的最大值的时候**，我们通过subgradient的方式进行求解，此时对偶问题的subgradient为$∂g(u)-b$。那么接下来顺着subgradient的迭代，我们就可以得到对偶问题的最优值。为了进行下一步的求解，这里是用到了共轭函数具有以下的性质，即


$$
x \in \partial f^*(y)  \leftrightarrow x \in arg\min_z f(z) - y^Tz\\
x \in \partial f^*(-A^Tu)  \leftrightarrow x \in arg\min_z f(z) + u^TAz
$$


所以，**dual subgradient的算法步骤**为

1. 首先确定一个初始的$u_{0}$
2. 反复迭代直到满足停止准则


$$
x^k \in arg\min_x f(x) + (u^{k-1})^TAx\\
u^k = u^{k-1} + t_k (Ax^k - b)
$$


​	其中，第一个式子在于找出一个满足subgradient的点，第二步进行迭代更新，$t_{k}$是步长，当$f$函数是严格凸函数的时候，他的共轭函数是可微的，那么次梯度就等于梯度值。当然迭代更新的时候也可以使用proximal subgradient的加速方法。另一方面，dual subgradient的收敛性和primal gradient在本质上是相等的。
​	注意，在第一个式子中，我们需要求式子的最小值，这取决于$f$函数的形式是否简单。如果$f$函数是很多个函数的和呢？ 即$f(x)=\sum f_{i}(x)$，此时我们可以考虑小批量的梯度下降，但是，如果$f$函数变成下面这个形式，每个子函数与特定的某部分的x有关。


$$
f(x) = \sum^B_{i=1} f_i(x_i)
$$


​	如果没有等式约束$Ax=b$，那么就可以并行的分块求解，在有约束的情况下，现在我们的优化问题变成下式，即已知变量$x=(x_{1},...,x_{B}) \in R^{n}$，$x_{i} \in R^{n_{i}}$， $\sum_{i=1}^{B} n_{i} =n$，被分割成$B$块，$A=[A_{1}, ..., A_{B}]$，$A_{i} \in R^{m\* n_{i}}$。


$$
\min_x \sum_{i=1}^B f_i(x_i)  \\
s.t. Ax=\sum_{i=1}^{B} A_ix_i = b
$$


带入到之前的dual subgradient式子中


$$
x \in \partial f^*(-A^Tu) \leftrightarrow x \in \arg\min_z \sum_{i=1}^B f_i(z_i) + u^TAz \leftrightarrow  x_i \in \arg\min_z \sum_{i=1}^B f_i(z_i) + u^TA_iz_i
$$


可以看到最后的形式，相当于是解B个子问题，这就是**dual decomposition algorithm**，其步骤为


$$
x^k_i \in arg\min_{x_i} f_i(x_i) + (u^{k-1})^TA_ix_i\\
u^k = u^{k-1} + t_k (\sum_{i=1}^B A_ix_i^k - b)
$$


这个过程很像概率图模型中的**belief propagation**的思想：
（Broadcast step）首先把第$k-1$次的$u$值同时赋值给$B$个子问题或叫sub-processor，之后每个sub-processor分别并行的更新$x_{i}$
（Gather step）最后各个部分的$x_{i}$集合起来再次更新$u^{k}$

![](/img/youhua32.png)

另外，对于不等式约束也同样适用，此时问题变成下式。为了对他进行求解，不能再使用subgradient method，因为此时对偶变量$u$有了大于零的限制，所以可以采用projected subgradient method(也就是当$h(x)$是指示函数时候的proximal subgradient method)


$$
\min_x \sum_{i=1}^B f_i(x_i)  \\
s.t. \sum_{i=1}^{B} A_ix_i \le b
$$


projected subgradient method相当于是在凸集上的投影，所以更新方式是

![](/img/youhua33.png)

​	这个问题以系统资源占用为例，系统中有$B$个进程，每个进程有自己的决策变量$x_{i}$来决定需要哪些资源，需要多少资源，系统资源的多少限制记为$A$，资源$j$的价格记为$u_{j}$，此时更新过程为下式。如果资源$j$被过度使用，$s_{j}<0$，则提高价格$u_{j}$，反之若资源$j$没咋被使用，就降低$u_{j}$，过程中一直保证资源$j$是非负的。


$$
u_j^+ = (u_j -ts_j)_+\\
s= b-\sum_{i=1}^B A_ix_i \\
where \quad (a)_+ = max(0,a)
$$


但是dual methods是通过对对偶问题进行求解，以收敛到原问题的解，但是传说这个需要很强的假设才行，而且即使在一定条件下对偶变量收敛了，原问题的迭代可能还是会不收敛，或者收敛点不符合约束条件，这时候就是**Augmented Lagrangian method**上场了，该算法首先把原问题转化为下式


$$
\min_x f(x) + \frac{\rho}{2} \|Ax-b\|^2_2\\
s.t. Ax=b
$$


这附加项并不会改变问题的解，因为在任意可行解的情况下，他是等于0的。之后的步骤就是和dual subgradient方法差不多了:


$$
x^k \in \arg\min_x f(x) + (u^{k-1})^T Ax + \frac{\rho}{2} \|Ax -b \| ^2_2\\
u^k = u^{k-1} + \rho (Ax^{k-1} - b)
$$


这里需要注意的是，在第二项中的迭代步长是恒为$ρ$，因为对于第一项的最小化有


$$
0 \in \partial f(x^k) + A^T \left( u^{k-1} + \rho (Ax^k -b) \right) =  \partial f(x^k) + A^Tu^k
$$

​	这个式子，正好是原问题KKT条件中的stationary condition，这使得在$u^{k}$的迭代中，$x^{k}$也同时慢慢的达到最优，所以较一般的dual subgradient方法来说，具有更好的收敛性，但是由于附加项$\frac{ρ}{2} \|Ax-b\|_{2}^{2}$，并不具有可分性，所以这个方法不具有dual decomposition。 有没有同时保证收敛和对偶分解的算法呢？答案就是**ADMM（Alternating Direction Method of Multipliers）**。



## 0x04_1 ADMM

首先来看看下面这个问题


$$
\min_x f_1(x_1) + f_2(x_2)\\
s.t. A_1x_1+A_2x_2=b
$$


接下来转换为Augmented form


$$
\min_x⁡f_1 (x_1 )+f_2 (x_2 )+\frac{ρ}{2} \|A_1 x_1+A_2 x_2-b\|_2^2\\
s.t.   A_1 x_1+A_2 x_2=b
$$


他的Lagrangian为


$$
L(x_1,x_2,u)=\min_x⁡ f_1 (x_i )+f_2 (x_2 )+\frac{ρ}{2}  \|A_1 x_1+A_2 x_2-b\|_2^2+u^T (A_1 x_1+A_2 x_2-b)
$$


ADMM的更新方法为：



$$
x_1^{k}=\arg\min_{x_1} L(x_1,x_2^{k-1},u^{k-1} )\\
x_2^{k}=\arg\min_{x_2} L(x_1^k,x_2,u^{k-1} )\\
u^k=u^{k-1}+ρ(A_1 x_1^k+A_2 x_2^k-b)
$$


一般来说，还可以令$w=\frac{u}{ρ}$，此时有


$$
x_1^k=\arg\min_{x_1}  f_1 (x_1 )+\frac{ρ}{2} \|A_1 x_1+A_2 x_2^{k-1}-b+w^{k-1} \|_2^2\\
x_2^k=\arg\min_{x_2}  f_2 (x_2 )+\frac{ρ}{2} \|A_1 x_1^k+A_2 x_2-b+w^{k-1} \|_2^2\\
w^{k}=w^{k-1}+A_1 x_1^k+A_2 x_2^k-b=w^0+∑_{i=1}^k \left(A_1 x_1^i+A_2 x_2^i-b \right)
$$


由于附加项$\frac{ρ}{2} \|A_1 x_1+A_2 x_2-b\|_2^2$的存在，ADMM确实不能完全做decomposition，但是他可以近似的分解，也算是一种折中。并且参数$ρ$的选择也很重要，如果$ρ$太小，可能会得不到可行解，如果$ρ$太大，算法将忽略原始问题最小化$f(x)$。一般的办法就是多试试（祝你好运）

我们以Generalized lasso regression为例，看看ADMM是怎么做的。给定下面的优化问题：


$$
\min_{\beta} f(\beta) + \lambda \|D\beta\|_1   \rightarrow \min_{\beta} f(\beta) + \lambda \|\alpha\|_1 \quad s.t. D\beta = \alpha
$$


他对应的ADMM算法更新式子为：


$$
β^k=(X^T X+ρD^T D)^+ (X^T y+ρD^T (a^{k-1}-w^{k-1} ))\\
a^k=S_{λ/ρ} (Dβ^k+w^{k-1} )\\
w^k=w^{k-1}+Dβ^k-a^k
$$


另外，基于分布式环境下的ADMM算法也是业界常用的算法，算法的更多介绍可以访问

[这里]: https://blog.csdn.net/weixin_41923961/article/details/83722234	"这里"





## 0x04_2 Coordinate Descent

我们再来讲讲另外一个更常用的算法：**Coordinate Descent**，又叫做**Coordinatewise Minimization**，是一种简单，高效并且很常用的优化方法。用来处理直接求解很复杂，但是如果只限制在一个维度的情况下则变得很简单甚至可以直接求极值的情况。

给定下面的优化问题：给定函数$g$是可微的凸函数，$h$是凸函数


$$
\min f(x) = g(x) + \sum_{i=1}^n h_i(x_i)
$$


Coordinate Descent运行我们沿着坐标轴一维一维的优化。首先初始化$x^0$，在第k次迭代中执行每个维度的更新


$$
x_1^k \in \arg\min_{x_1} f(x_1, x_2^{k-1}, ..., x_n^{k-1})\\
x_2^k \in \arg\min_{x_1} f(x_1^k, x_2, ..., x_n^{k-1})\\
x_n^k \in \arg\min_{x_1} f(x_1^k, x_2^k, ..., x_n)
$$


这种更新策略已经在ADMM算法中用到，但是coordinate descent不一定能达到最优解。

现在先考虑$f(x)$是可微的凸函数。假设最小值在$x^\*$处取得，即$f(x^\* )=\min_{z}⁡ f(z)$，令$e_{i}=(0,0,…,1,…,0) \in R^{n}$，表示第$i$个坐标轴方向，那么对于任意$i,δ$有（凸函数的一阶充要条件）：

| ![](/img/youhua34.png) | ![](/img/youhua35.png) |
| ------------------------ | ------------------------ |
|                          |                          |

对于一般的不可微的凸函数来说，却不一定能收敛到最优解

![](/img/youhua36.png)

如上图在两条虚线相交的点处，每次再按照坐标方向寻最优已经收敛不到蓝色点的最小值处。然而对于，一开始目标形式的凸函数$\min⁡  f(x)=g(x)+∑_{i=1}^{n} h_{i} (x_{i}$来说是可以收敛到最优的，此时$h$函数那部分被称为separable。

![](/img/youhua37.png)



此时对于任意的$y=x^{\*}+δe_{i}$有下式成立

![](/img/youhua38.png)

下面有一些例子，可以帮助大家更好的理解。



### Linear Regression

给定下面的优化问题，其中$y\in R^{n},X=[X_{1},X_{2},..,X_{p}] \in R^{n,p}$


$$
\min_{\beta} \frac{1}{2} \| y-X\beta\|_2^2
$$


由于目标问题可微，直接对$β_{i}$求导得


$$
0=∇_i f(β)=X_i^T (Xβ-y)=X_i^T (X_i β_i+X_{-i} β_{-i}-y)   →  β_i=\frac{X_i^T (y -X_{-i} β_{-i} )}{X_i^T X_i}
$$


下图是蓝色是coordinate descent与gradient descent，Accelerated gradient descent的比较，明显它很快，但是coordinate descent并不是一阶的算法

![](/img/youhua39.png)



### SVM (SMO算法)

给定经典的SVM算法


$$
\min_{\beta,\beta_0,\delta} \frac{1}{2} \|\beta\|_2^2 + C \sum_{i=1}^n \delta_{i} \\
s.t. \quad -\delta_i \le 0,i=1,2,...,n\\
\quad 1-\delta_i-y_i(x_i^T\beta + \beta_0) \le 0, i=1,2,...,n
$$


和它的对偶形式


$$
\max_{w} -\frac{1}{2}W^T\hat{X}\hat{X}^TW + W^T1 \\
s.t. \quad 0\le w_i \le C, i=1,2,...,n\\
\quad W^Ty=0
$$


著名的**SMO算法是典型的blockwise coordinate descent**，具体来说是two blocks，并且贪心的选择不满足KKT条件的block进行更新而不是简单的坐标循环更新。对于SVM问题的KKT条件为

Stationary：$\sum_{i=1}^{n}w_{i}y_{i}=0, C-v_{i}-w_{i}=0, \beta=\hat{X}^{T}W=\sum_{i=1}^{n}w_{i}x_{i}y_{i}$

Complementary：$v_{i}\delta_{i}=0, w_{i}(1-\delta_{i}-y_{i}(x_{i}^{T}\beta+\beta_{0}))$

对于对偶问题来说，SMO算法每次选择两个不满足complementary条件的$w_{i}$和$w_{j}$进行coordinate descent更新。如果每次只对一个$w_{i}$进行更新，其他固定为常数，此时由于等式约束条件$∑_{i=1}^{n} w_{i} y_{i} =0$的存在，使得$w_{i}$的更新值只能为


$$
w_iy_i = -\sum_{k \neq i}^n w_ky_k
$$


SMO启发式的选择两个block $w_{1}$和$w_{2}$同时更新，此时有


$$
w_1=\left( -\sum_{k=3}w_ky_k  - w_2y_2 \right) \frac{1}{y_1} = (\pi - w_2y_2) \frac{1}{y_1} \\
\rightarrow w_2y_2 + w_1y_1 = \pi
$$


此时构造出来了一个取值域（图里的$a$就是我们的$w$），这个可行域由上式的等式与w本身的取值范围组成，最后组成了一条正方形中的线段。

![](/img/youhua40.png)



此时把$w_1$的形式带回到对偶形式中，得到关于$w_2$，在一定取值范围内的二次问题，而这个取值范围由上图的线段投影到$w_2$方向上的范围决定。同理得到$w_1$的更新值，从而完成一次coordinate descent的迭代



1. linear regression

2. lasso regression

3. box-constrained regression

4. svm

   ​

## 0x04_3 Conditional Gradient Method

Frank-Wolfe Method

例子：

1. losso
2. Lp norm

## 0x04_4 Cutting-plane Method

## 0x04_5 Non-convex Problem

