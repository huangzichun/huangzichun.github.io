---
layout:     post
title:      机器学习中的那些优化算法
subtitle:   优化算法の笔记（下） - ADMM & Coordinate Descent
date:       2021-08-22
author:     HC
header-img: img/32040b9902e7378d6996d46135af3cfa.jpeg
catalog: true
tags:
    - Optimization in machine learning
    - ADMM
    - Coordinate Descent
---

> 历史笔记迁移系列
>
> Word转markdown真的好累

第一集：[优化算法の笔记（上）- First Order Methods](http://huangc.top/2021/06/20/Optimization_1-2021/)

第二集：[优化算法の笔记（中） - Duality Method & Seconds Order Methods](http://huangc.top/2021/07/31/Optimization_2-2021/)

第三集：[优化算法の笔记（下） - ADMM & Coordinate Descent](http://huangc.top/2021/08/22/Optimization_3-2021/)

# 0x04 Others and Application

我们回忆一下，给定一个函数$f$，他的共轭函数定义为：


$$
f^*(y) = \max_{x} y^Tx - f(x)
$$


共轭函数是关于$y$的一些凸函数的逐点上确界，下面是他的一些性质：

1. 对于任意的$x,y$，满足$f(x) + f^{*}(y) \le x^{T}y$
2. 对于任意函数$f$，他的共轭函数的共轭函数，值不大于原函数：$f^{\*\*}(x) \le f(x)$
3. 如果函数$f$是凸的或者凹的，那上面两个性质都可以取等号，并且，满足$x \in \partial f\*(y)  \leftrightarrow x \in \arg\min _{z} f(z) - y^{T}z$。



共轭函数常常用于推导问题的对偶形式，但是实际上，当我们很想用对偶来求解问题的时候，这个共轭函数却不给力了，表现为很难给出一个闭式解来推得对偶形式，这时候就是**dual-based subgradient** or **gradient methods**起作用的时候了。考虑下面的优化目标，其中，$f$是closed的凸函数


$$
\min_x f(x) \\
s.t. Ax=b
$$


构造Lagrangian multiplier，可以求出其对偶形式为下式，其中$f^\*$是共轭函数，记$f\* (-A^{T} u)=g(u), ∂g(u)=-A∂f\* (-A^{T} u)$。


$$
\max_u -f^*(-A^Tu) - b^Tu = -g(u) - b^Tu
$$


**Dual subgradient算法就在于处理，我们无法直接求出对偶形式的最大值的时候**，我们通过subgradient的方式进行求解，此时对偶问题的subgradient为$∂g(u)-b$。那么接下来顺着subgradient的迭代，我们就可以得到对偶问题的最优值。为了进行下一步的求解，这里是用到了共轭函数具有以下的性质，即


$$
x \in \partial f^*(y)  \leftrightarrow x \in arg\min_z f(z) - y^Tz\\
x \in \partial f^*(-A^Tu)  \leftrightarrow x \in arg\min_z f(z) + u^TAz
$$


所以，**dual subgradient的算法步骤**为

1. 首先确定一个初始的$u_{0}$
2. 反复迭代直到满足停止准则


$$
x^k \in arg\min_x f(x) + (u^{k-1})^TAx\\
u^k = u^{k-1} + t_k (Ax^k - b)
$$


​	其中，第一个式子在于找出一个满足subgradient的点，第二步进行迭代更新，$t_{k}$是步长，当$f$函数是严格凸函数的时候，他的共轭函数是可微的，那么次梯度就等于梯度值。当然迭代更新的时候也可以使用proximal subgradient的加速方法。另一方面，dual subgradient的收敛性和primal gradient在本质上是相等的。
​	注意，在第一个式子中，我们需要求式子的最小值，这取决于$f$函数的形式是否简单。如果$f$函数是很多个函数的和呢？ 即$f(x)=\sum f_{i}(x)$，此时我们可以考虑小批量的梯度下降，但是，如果$f$函数变成下面这个形式，每个子函数与特定的某部分的x有关。


$$
f(x) = \sum^B_{i=1} f_i(x_i)
$$


​	如果没有等式约束$Ax=b$，那么就可以并行的分块求解，在有约束的情况下，现在我们的优化问题变成下式，即已知变量$x=(x_{1},...,x_{B}) \in R^{n}$，$x_{i} \in R^{n_{i}}$， $\sum_{i=1}^{B} n_{i} =n$，被分割成$B$块，$A=[A_{1}, ..., A_{B}]$，$A_{i} \in R^{m\* n_{i}}$。


$$
\min_x \sum_{i=1}^B f_i(x_i)  \\
s.t. Ax=\sum_{i=1}^{B} A_ix_i = b
$$


带入到之前的dual subgradient式子中


$$
x \in \partial f^*(-A^Tu) \leftrightarrow x \in \arg\min_z \sum_{i=1}^B f_i(z_i) + u^TAz \leftrightarrow  x_i \in \arg\min_z \sum_{i=1}^B f_i(z_i) + u^TA_iz_i
$$


可以看到最后的形式，相当于是解B个子问题，这就是**dual decomposition algorithm**，其步骤为


$$
x^k_i \in arg\min_{x_i} f_i(x_i) + (u^{k-1})^TA_ix_i\\
u^k = u^{k-1} + t_k (\sum_{i=1}^B A_ix_i^k - b)
$$


这个过程很像概率图模型中的**belief propagation**的思想：
（Broadcast step）首先把第$k-1$次的$u$值同时赋值给$B$个子问题或叫sub-processor，之后每个sub-processor分别并行的更新$x_{i}$
（Gather step）最后各个部分的$x_{i}$集合起来再次更新$u^{k}$

![](/img/youhua32.png)

另外，对于不等式约束也同样适用，此时问题变成下式。为了对他进行求解，不能再使用subgradient method，因为此时对偶变量$u$有了大于零的限制，所以可以采用projected subgradient method(也就是当$h(x)$是指示函数时候的proximal subgradient method)


$$
\min_x \sum_{i=1}^B f_i(x_i)  \\
s.t. \sum_{i=1}^{B} A_ix_i \le b
$$


projected subgradient method相当于是在凸集上的投影，所以更新方式是

![](/img/youhua33.png)

​	这个问题以系统资源占用为例，系统中有$B$个进程，每个进程有自己的决策变量$x_{i}$来决定需要哪些资源，需要多少资源，系统资源的多少限制记为$A$，资源$j$的价格记为$u_{j}$，此时更新过程为下式。如果资源$j$被过度使用，$s_{j}<0$，则提高价格$u_{j}$，反之若资源$j$没咋被使用，就降低$u_{j}$，过程中一直保证资源$j$是非负的。


$$
u_j^+ = (u_j -ts_j)_+\\
s= b-\sum_{i=1}^B A_ix_i \\
where \quad (a)_+ = max(0,a)
$$


但是dual methods是通过对对偶问题进行求解，以收敛到原问题的解，但是传说这个需要很强的假设才行，而且即使在一定条件下对偶变量收敛了，原问题的迭代可能还是会不收敛，或者收敛点不符合约束条件，这时候就是**Augmented Lagrangian method**上场了，该算法首先把原问题转化为下式


$$
\min_x f(x) + \frac{\rho}{2} \|Ax-b\|^2_2\\
s.t. Ax=b
$$


这附加项并不会改变问题的解，因为在任意可行解的情况下，他是等于0的。之后的步骤就是和dual subgradient方法差不多了:


$$
x^k \in \arg\min_x f(x) + (u^{k-1})^T Ax + \frac{\rho}{2} \|Ax -b \| ^2_2\\
u^k = u^{k-1} + \rho (Ax^{k-1} - b)
$$


这里需要注意的是，在第二项中的迭代步长是恒为$ρ$，因为对于第一项的最小化有


$$
0 \in \partial f(x^k) + A^T \left( u^{k-1} + \rho (Ax^k -b) \right) =  \partial f(x^k) + A^Tu^k
$$


​	这个式子，正好是原问题KKT条件中的stationary condition，这使得在$u^{k}$的迭代中，$x^{k}$也同时慢慢的达到最优，所以较一般的dual subgradient方法来说，具有更好的收敛性，但是由于附加项$\frac{ρ}{2} \|Ax-b\|_{2}^{2}$，并不具有可分性，所以这个方法不具有dual decomposition。 有没有同时保证收敛和对偶分解的算法呢？答案就是**ADMM（Alternating Direction Method of Multipliers）**。



TODO

## 0x04_1 Dual Methods and ADMM

dual subgradient



dual deconposition



Augmented Lagrangian Method



ADMM



例子：

- Norm regularization
- lasso

## 0x04_2 Distributed  ADMM 

## 0x04_3 Coordinate Descent

例子

1. linear regression

2. lasso regression

3. box-constrained regression

4. svm

   ​

## 0x04_4 Conditional Gradient Method

Frank-Wolfe Method

例子：

1. losso
2. Lp norm

## 0x04_5 Cutting-plane Method

## 0x04_6 Non-convex Problem

