---
layout:     post
title:      Reinforcement Learning学习笔记
subtitle:   入坑指南
date:       2018-05-03
author:     HC
header-img: img/post-bg-android.jpg
catalog: true
tags:
    - Reinforcement Learning
    - MDP
    - Policy
---

# 0. Dynamic System

**动态模型**是用来描述一个给定空间（如某个物理系统的状态空间）中点随时间的变化情况。例如描述钟摆晃动、管道中水的流动，或者湖中每年春季鱼类的数量，凡此等等的数学模型都是动态系统 (From Wiki)。比如下图以点的运动为例子，左图展示了在不同时刻t上，构成的点X的移动轨迹。 右图则是加入了额外噪声的情况。

![](/img/ds.png)

动态系统旨在对时序数据的刻画，它由三个元素组成：**状态值**，**状态转移**和**观察值**。比如我们以股市为例（如下的图）。它有三个状态（Bull，Bear，Even），在每个状态下股市都表现出特定的涨跌，最后每个状态之间会以一定的概率进行状态转移。

![](/img/ds1.png)



动态系统假设仅仅前后两个时刻的状态是相互依赖的，符合**马尔科夫条件独立性假设。**因此，可以将状态Xt看做是$X_{t−1}$的函数，**$P(X_{t}\mid X_{t-1})$**。而在状态$X_{t}$上，会以某种方式表现出（或被观测到）$Y_{t}$，即**$P(Y_{t}\mid X_{t})$**。一般来说，ML 中的动态系统可以分为三种情况，**离散状态**，**线性高斯**和**非线性非高斯**。如下所示：

![](/img/ds2.png)

1. 离散状态的动态系统的例子就是**隐马尔科夫模型（HMM）**。它由一个**State Transition Matrix $P(x'\mid x)$**，**Measurement Matrix $P(y\mid x)$**和一个**Initial State π**组成。注意：HMM 的状态X必须是离散值，而不是连续值，否则 HMM 的状态转移矩阵就是无穷大的。而对Measurement Y没有做要求。
2. **线性高斯动态系统**和**非线性非高斯动态系统**的状态是连续的，线性高斯的State Transition Matrix和Measurement Matrix对应服从高斯分布，而非线性非高斯则可以是任意的。线性高斯动态系统的例子是**Kalman Filter**，而非线性非高斯的例子是**Partical Filter**

关于例子

1.  HMM上的常用的操作，一是 **uncertainty measurement**，已知模型参数和一个观测序列，求它产生的概率，二是 **parameter learning**，顾名思义就是去对λ = {A, B, π}进行求解。还有一种是做 **variable Inference**: 在 B 发生的条件下，A 发生的概率 $P(B\mid A)$

2.  卡尔曼滤波是线性动态系统的一种，它的图模型也和 HMM 一样，只是状态取值是连续值，并且它的噪声服从零均值高斯分布，transition probability 和 measurement probability 都服从高斯分布。卡尔曼滤波上的常用操作是 **filtering**, 给定 T = 1 到 t 时间的观测值，希望对时间 T=t 时的状态进行求解，目标为$P(X_{t}\mid Y_{1}, Y_{2}, … , Y_{t})$。这个filtering是不断的 predict （求解$P(X_{t}\mid Y_{1}, Y_{2}, … , Y_{t-1}) $）和不断 update（求解$P(X_{t}\mid Y_{1}, Y_{2}, … , Y_{t})$） 的过程。如下图：

     ![](/img/ds3.png)

3.  Particle filter 可以看错是一种**MC Sampling**的算法。具体来说他是建立在**Sequential Importance Sampling（SIS）**基础之上的。这个SIS算法是为了解决 Importance Sampling 的不能很好处理高维数据的问题，SIS 旨在对每个样本一维一维的进行采样。particle filter 是通过使用了SIS 进行求解的模型，其中 filtering 的分布由采样数据的位置和其权重来刻画

4.  Particle filter 与 Kalman filter 的区别在于，后者认为 measurement 和 transfer probability服从高斯分布，从而 filtering 的分布也服从高斯，他建立起的是 t 时刻和 t+1 时刻 filtering 高斯分布的参数联系；而 particle filter 可以适用于任何分布，所以求解麻烦，转而求助于 SIS 的方法，以当前时刻采样出来的 particle 的位置和权重来表示当前时刻的 filtering 分布。



# 1. Markov Process

上回，我们说到了动态系统。以HMM模型为例，它由状态，状态转移和观察值三部分组成，用来刻画随时间变化的事物。实际上，我们可以把动态系统看做更为广泛的模型：马尔科夫过程（Markov Process，MP），或者被叫做马氏链（Markov Chain）。

MP是一个状态序列满足马尔科夫条件独立性假设（即给定当前时刻的状态，下一时刻的状态与历史时刻的状态保持独立）的随机过程
$$
P(S_{t+1}\mid S_t) = P(S_{t+1}\mid S_1, S_2, ..., S_t)
$$
MP由两个部分组成：有限状态集合$S$和状态转移矩阵$P$，$P_{ss'}=P(S_{t+1}=s'\mid S_{t}=s)$。以下图为例，这是一个学生上课的MP图。学生需要听过三节课之后才能通过（pass）本门课，圆圈和方块表示状态，其中方块表示终止状态（只进不出），边上的数字表示状态转移概率。在第一节课的时候，同学可能会开小差，去刷facebook，并且一旦开始刷facebook，就会有极大的概率（0.9）一直刷下去。同理，也可能上课睡觉，或者课后去酒吧（pub），喝得烂醉之后重新听课。

![](/img/mrp.png)

对于HMM模型来说，他还存在观察量（Observations）。准确的来说，HMM其实是一个部分可观察的马氏链（Partially Observable Markov Chain ），因为当前的观察值并不足以完全确定当前所处的状态，就像只有视觉信号的机器人无法全面感知周围的环境一样。如果Observations = State，观察能全面刻画所处的状态信息，那么就叫Full Observability。

无论观察是否全面，MP通过$S$和$P$刻画了状态随时间的转移。如果学生上课的MP图以Class 1作为开始状态，那么，能够得到以下的状态序列

1. C1 C2 C3 Pass Sleep 
2. 
   C1 FB FB C1 C2 Sleep 

3. C1 C2 C3 Pub C2 C3 Pass Sleep 
4. ........

对于每一个状态序列，总是在终止状态下结束（当然，并不是所有的MP都含有终止状态，这时的状态序列将一直持续下去）。每一个序列都被称作是一个episode。这样的episode其实就是一条时序数据啦。

# 2. Markov Reward Process

上次说到了Markov Process，他由两部分组成，有限状态集合S和状态转移P。本次我们介绍Markov Reward Process（MRP），他是一个特殊的Markov Process，因为MRP的每个状态上都被附上一个值，这个值被称为Reward。同样，我们以学生上课的状态图为例，这次在每个状态里都附上了Reward（图中红色R）。学生通过考试（pass）可以得到10分的Reward，但是上课的过程是痛苦的，这里给了-2的Reward。

![](/img/mrp1.png)

相比MP，MRP由四个部分组成：

1. 有限状态集S
2. 状态转移P，$P_{ss'}=P(S_{t+1}=s'\mid S_{t}=s)$
3. Reward函数 R，$R_{s}=E[R_{t+1}\mid S_{t}=s]$。注意状态s上的Reward是一个期望值
4. 折扣因子（Discount Factor）$\gamma \in [0, 1]$。

在MRP中，Reward描述了某个状态下的奖励值。在上面的例子中，通过考试（Pass）以最高的奖励（10）来吸引学生向该状态转换。那么折扣因子用来干啥？实际上，MRP并不关心单独某个状态下的Reward，而是喜欢一个Reward的累加量，叫Return。Return G的定义如下：
$$
G_t=R_{t+1}+\gamma R_{t+2} + ... = \sum_{k=0}^\infty \gamma^k R_{t+k+1}
$$
$G_{t}$ 是在第t时刻的Return。从形式上看，Return计算了从t时刻的某个状态开始，某一个episode所获得的所有discounted Reward的总和。这里的折扣因子是一个0到1之间的数，对于第k+1时刻，累加的reward是$\gamma^k R$，因此时刻越久远，所计算的reward越小。$\gamma$越大，衰减也就越慢。当$\gamma=1$的时候，称为undiscounted MRP。

那为什么需要discount呢？

1. 避免无穷值（万一MRP没有终止状态，或者MRP中有环）。
2. 保留对未来的不确定性。（注意，$R_{s}$是一个期望值，$R_{t}$不是）
3. 其他解释：现在的钱比未来更值钱

因此，我们可以使用Return来表示从一个状态开始的一个episode所带来的奖励。我们以上次的episode为例，计算他们的Return（这里假设$\gamma=1/2$）：

1. C1 C2 C3 Pass Sleep ：G1=-2 -2* (1/2)-2* (1/4)+10* (1/8)
2. C1 FB FB C1 C2 Sleep ：G1=-2-1* (1/2) -1 * (1/4) -2* (1/8) -2 *(1/16)

那么，如果我们能得到很多个从同一个状态开始的episodes，便可以用它们来衡量该状态的好坏了（从好的状态开始采样，更容易得到更大的return）。对于状态好坏的衡量，MRP采用了Value Function，准确的说，是叫State Value Function：
$$
v(s)=E[G_t\mid S_t=s]
$$
状态价值函数衡量了在某一状态s下，所能获得的Return的期望。它可以看做是对Return的一种预测。那么，对于学生MRP例子，我们可以得到下图（$\gamma=1$），图中红色的数字就表示某状态的state value function的大小。比如对于Pass而言，只有一条路径到终止状态，那么他的v(s)就等于10

![](/img/mrp2.png)

那么，其他状态的v是怎么计算的呢？这就要引入一个公式了，叫Bellman Equation。具体来说，它认为一个状态的v函数可以分解成两部分

1. immediate reward $R_{t+1}$
2. 下一个状态的discounted $\gamma v(S_{t+1})$

用数学语言就是说：


$$
v(s)=E[G_t\mid S_t=s]\\
=E[R_{t+1}+\gamma R_{t+2} + \gamma^2R_{t+3}+...\mid S_t=s]\\
=E[R_{t+1} + \gamma G_{t+1}\mid S_t=s]=E[R_{t+1} + \gamma v(S_{t+1})\mid S_t=s]
$$


所以，Bellman Equation的形式如下。注意，在状态s的时候，我们并不知道下一个状态会是啥，所以公式中使用了$S_{t+1}$表示，而不是$s_{t+1}$
$$
v(s)=E[R_{t+1} + \gamma v(S_{t+1})\mid S_t=s]
$$
也就是说，我们需要遍历下状态空间，如下所示，s'表示下一个状态

![](/img/mrp3.png)

对应的公式变成：
$$
v(s)=R_s + \gamma \sum_{s' \in S}P_{ss'} v(s')
$$
我们来看看，学生MRP例子是不是符合这个式子（废话，当然是符合）

![](/img/mrp4.png)



对于这个形式来说，可以写成矩阵的形式求解：


$$
v=R+\gamma Pv = \begin{bmatrix}
v(1)
\\ 
...
\\ 
v(n)
\end{bmatrix} = \begin{bmatrix}
R_1
\\ 
...
\\ 
R_n
\end{bmatrix} + \gamma \begin{bmatrix}
P_{11} & ... & P_{1n}\\ 
... &...  & ...\\ 
P_{n1} & ... & P_{nn}
\end{bmatrix} \begin{bmatrix}
v(1)
\\ 
...
\\ 
v(n)
\end{bmatrix}
$$


对应的闭式解是：


$$
v=(I-\gamma P)^{-1}R
$$


当然，计算量很大，求逆是O(n^3)的复杂度，当状态多了，这是很恐怖的。当然也有很多的高级算法啦，比如动态规划，蒙特卡洛和Temporal-Difference learning (TD Learning)等等。之后我们慢慢推送，不急不急。。

# 3. Markov Decision Process

上回说到了MRP，这次我们做进一步的扩展，它的名字叫做Markov Decision Process (MDP)。它使得系统加入更多交互属性，可以用来刻画序列决策过程。与MRP相比，MDP加入了决策的动作。MDP由以下五个部分组成

1. 有限状态集合S
2. 有限动作（决策）集合A
3. 状态转移P，$P_{ss'}^a=P[S_{t+1}=s'\mid S_{t}=s, A_{t}=a]$
4. 奖励函数R，$R_{s}^a=E[R_{t+1}\mid S_{t}=s,A_{t}=a]$
5. 折扣因子$\lambda \in [0,1]$

MRP中，处于某个状态的事物可以做出一个决策动作。根据所处的状态以及所做的决策动作，该事物会获得一个reward，然后转移到新的状态。下图中就是大家熟悉而又喜爱的学生状态图，图中红色的字表示action。

![](/img/mdp.png)

实际上，在MDP中有两个角色：agent和environment。整个MDP就是agent和environment之间不断玩耍交互的过程。agent在第t时刻处于某一个state，agent做出一个action，然后收到一个reward；environment看到agent的action之后，将发送一个reward给agent，然后agent的state被转换。

实际上，agent往往并不能完全察觉自己处于什么样的状态，它只能通过对外界的观察（observation）来推断自己的状态（也就是说，这是partially observable）。所以整个过程就变成了下图所示：agent只接收到对state的观察。

![](/img/mdp1.png)

那么，agent为什么需要从observation中推断出state？这是因为MDP是state满足马尔科夫条件，而不是observation。一般来说，可以认为state是history of observations, action和reward的函数，即


$$
S_t=f(H_t)\\
\mbox{history：}H_t=O_1, R_1, A_1, ..., A_{t-1}, O_{t}, R_{t}
$$


因此，agent的目的就是要通过观察state，做出决策动作，以最大化能获得的累计reward。那么要怎么出招呢？agent需要一个出招秘籍，这个秘籍被称为agent的policy。policy是在给定state下，agent动作的分布
$$
\pi(a\mid s)=P(A_{t}=a\mid S_t=s)
$$
在MRP中，我们引入了State value function来衡量一个state的好坏。在MDP中，加入了动作之后，自然我们想要知道在给定state下，做出每个动作的好坏。这个功能是通过Action value function（或者叫Q函数）来实现的：


$$
q_\pi(s,a)=E_\pi[G_t\mid S_t=s,A_t=a]
$$


同样，state value function的Bellman Equation同样也适用于Q函数


$$
v_\pi(s)=E_\pi[R_{t+1}+\gamma v_\pi(S_{t+1})\mid S_t=s]\\
q_\pi(s,a)=E_\pi[R_{t+1}+\gamma q_\pi(S_{t+1},A_{t+1})\mid S_t=s, A_t=a]
$$


事实上，MDP中state s和action a就是这么交替的向后执行：在一个状态下，从动作空间A中选择做出某个动作a，然后不同的a将被转换到不同的新state下。用图片表示则是：

![](/img/mdp2.png)

同理，我们也可以从某个action出发，转移到某个状态，在新状态下做出下一个动作：

![](/img/mdp3.png)

我们可以便可以把Bellman Equation进一步写成：


$$
v_\pi(s) = E_\pi[R_{t+1} + \gamma v_{\pi}(S_{t+1}) \mid  S_t=s] = \sum_{a\in A} \pi(a\mid s) q_{\pi}(s,a)\\
q_\pi(s,a) = E_\pi[R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) \mid  S_t=s, A_t=a] = R_s^a + \gamma \sum_{s' \in S}P^a_{ss'} v_{\pi}(s')\\
v_\pi(s) = \sum_{a\in A} \pi(a\mid s) \left( R_s^a + \gamma \sum_{s' \in S}P^a_{ss'} v_{\pi}(s')\right)
$$


以下面的例子来说明，假设跳转概率没有特别说明的话，都是0.5，否则标注在边上。那么就可以验证下图中红色状态的state value function

![](/img/mdp4.png)

作为agent，他的目的是最大化最后的累计reward。为了达到这个目的，agent需要按照一个最好的policy来出招。实际上，对于任何一个MDP来说，一定存在一个（或者多个）最优policy $\pi^{\star}​$ 。而最优的policy对应了最优的state value function $v_\star (s)​$和最优的action value function $q_{\star}(s,a)​$。他们分别表示在所有可能的policy下，所能达到的最大值，这个最大值就预示着在给定MDP下的做好结果


$$
v_*(s)=\max_\pi v_\pi(s)\\
q_*(s,a)=\max_\pi q_\pi(s,a)
$$


当我们知道最优的value function的时候，agent的最优策略也就知道了：


$$
\pi^*(a\mid s) = \left\{\begin{matrix}
1 \mbox{ if }a=\arg\max_{a\in A}q_*(s,a)
\\ 
0 \mbox{ otherwise}
\end{matrix}\right.
$$


比如下面的例子中的最优策略是红线，可以看出，学习才是最好的出路（手动滑稽）。

![](/img/mdp5.png)

图中的结果是怎么计算出来？这又要用到一个叫Bellman Optimality Equations。总的来说，整个过程还是步步为营的。首先，$v(s)$是在状态s下所能得到return的期望，$q(s,a)$是在给定状态s下，作出动作a之后所能得到的return的大小，然后，在作出动作a之后，将跳转到另一个新状态s'。因此，两者满足


$$
v_*(s)=\max_a q_*(s,a)\\
q_*(s,a) = R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v_*(s')
$$


那么，两者结合起来就是

![](/img/mdp6.png)

![](/img/mdp7.png)

实际上，Bellman Optimality Equation是非线性的，他不能像Bellman Equation一样有闭式解（虽然复杂度很高），但是还是有很多方法可以用来求解。比如Value Iteration, Policy Iteration, Q-learning和Sarsa等等。

# 4. Reinforcement Learning

从Markov Process，Markov Reward Process到Markov Decision Process，写了这么多，就是引出Reinforcement Learning（RL），几乎所有的RL问题都可以转换成MDP问题。当然了，当前RL研究的问题远不止之前我们所介绍的内容。

一般来说，机器学习可以分为三类：监督学习，无监督学习和强化学习（怎么可能是半监督学习）。在监督或无监督学习中，我们总是把目标数学化成优化loss的形式（即便，目标任务并不是最小化模型损失），而强化学习是对对序列动作建模，通过reward机制建立与任务之间的联系，它给我们提供了一种能够更为直接地去优化任务本身的方法。也就是说，有时候，人们首先是根据监督或者非监督模型的输出结果，然后针对任务作出决策（比如先判断眼前的动物是猫还是老虎，再决定我们要不要逃跑）。强化学习是一步到位，提供了从what we receiving到what we doing的一个close loop。最后，世间任务千千万，并不是所有的任务都可以明确写出优化目标，对于有些任务监督或者非监督学习，他们是不适用的（就比如下围棋AlphaGo），这也是强化学习另一个优势。PS，这也并不是说监督和非监督学习没啥用，不要偏激的理解，强化学习当然也不是万能的（钱才是）。

只能说明的是，RL虽然有reward反馈，但是，它并不是监督学习：

1. RL的反馈是可以有时延的。实际情况下，并不是做一个动作之后一定会收到一个真正的reward，比如有的游戏要玩到结束之后，才会收到reward，胜利+1，失败-1。监督学习的标签则不会这样
2. RL处理的是时序episode，监督学习是iid data
3. agent的决策动作是会影响下一步的进展的。

和MDP一样，RL的目的在于最大化累计reward。这也是RL的Reward Hypothesis：所有的目标都可以被描述成最大化累计reward，如下式，其中r是reward函数


$$
\max_\pi E_\pi \left[ \sum_{t}r(s_t,a_t) \right]
$$


一般来说，RL有两类任务：

1. Prediction：给定policy，估计未来, 找出value function
2. Control：在所有可能的policy中，最好的value function是啥？哪个是最好的policy？

这两类任务可以在不同的背景下完成，这里的背景指的是environment或者model是否已知：agent是否知道状态空间，状态转移以及reward function等等。所以强化学习可以分为

1. Model-Free RL：不知道model（一般来说是指不知道状态转移以及reward function）
2. Model-Based RL：知道model

另一方面，强化学习还可以分为

1. value-based RL：在没有明确的policy下，去找value function
2. policy-based RL：在没有value function的条件下，去找policy
3. Actor-critic RL：用当前的policy来估计value ，并用它来提升policy


总的来说，可以用下图表示

![](/img/mdp8.png)

RL的过程可以看做是agent和environment之间的游戏，是state和action之间的转换游戏，分别对应了transition和policy。他们可以用下面的图例过程表示。这里的参数$\theta$是用来parameterize策略的（比如使用神经网络来刻画policy，那么$\theta$就是神经网络的参数）。

![](/img/rl.png)

实际上，这个过程是一个Markov Chain on state and action!

![](/img/rl1.png)

满足


$$
p((s_{t+1},a_{t+1})\mid (s_{t},a_t))=p(s_{t+1}\mid s_t,a_t)\pi_\theta(a_{t+1}\mid s_{t+1})
$$


那么给定一个episode，他的概率可以写成


$$
p_\theta(s_1,a_1,...s_T,a_T)=p(s_1) \prod_{t=1}^T \pi_\theta(a_t\mid s_t)p(s_{t+1}\mid s_t,a_t)
$$



# 5. Policy Gredients

对于RL来说，我们想要找出一个最好的policy来完成任务，比如打游戏的时候见到坑要知道跳，见到怪要知道打，见到金币得知道吃等等。前面说的RL的最终任务就是要最大化期望累计reward，这里的最大化是通过找一个最优的policy来实现的。下面公式中的$\theta$是policy $\pi$的参数（比如我们用神经网络来计算policy，那么$\theta$就是网络的参数）


$$
\theta^*=\arg\max_\theta E_{\tau \sim \pi_\theta(\tau)} \left[ \sum_{t=1} \gamma^{t-1} r(s_t,a_t) \right]=\arg\max_\theta \int \pi_\theta(\tau) r(\tau) d\tau\\
\pi_\theta(\tau)=p_\theta(s_1,a_1,...,s_T,a_T)=p(s_1) \prod_{t=1}^T \pi_\theta(a_t\mid s_t)p(s_{t+1}\mid s_t,a_t)\\
r(\tau)=\sum_{t=1} \gamma^{t-1}r(s_t,a_t)
$$


学过监督学习的我们知道，在监督学习中很多很多的算法都是可以通过梯度下降的方法来求解的，那么给定RL的目标方程，我们同样也可以使用梯度的方法来更新求解policy，使得最大化期望累计reward。也就是说，只需要用积分那一项（令他为$J(\theta)$）对$\theta$求导就可以了。注意，这里我们假定了policy是可导的。policy的形式往往是人定的（比如gaussian，神经网络等等），所以policy可不可导也是可控的。


$$
\bigtriangledown_\theta J(\theta)=\int \bigtriangledown_\theta\pi_\theta(\tau) r(\tau) d\tau
$$


问题就变成求出$\bigtriangledown_\theta\pi_\theta(\tau)$就ok了，大牛们当然不会直接去求解这一项，因为从最开始的式子里可以看到，$\pi_\theta(\tau)$涉及到多项相乘，直接求导会很心累。所以大牛提出了一个 convenient identity


$$
\bigtriangledown_\theta\pi_\theta(\tau)=\pi_\theta(\tau) \frac{\bigtriangledown_\theta\pi_\theta(\tau)}{\pi_\theta(\tau)}=\pi_\theta(\tau)\bigtriangledown_\theta log \pi_\theta(\tau)
$$


其中在贝叶斯里往往把$\bigtriangledown_\theta log \pi_\theta(\tau)$叫做score function。比如以一次动作为例，当我们用softmax来近似policy $\pi_{\theta}(\tau)$的时候（这叫做Softmax Policy），这时的score function可以写成下式，这个式子衡量了当前动作相比于平均动作的好坏。


$$
\pi_\theta(a\mid s)= \frac{e^{\phi(s) ^T \theta }}{\sum_a e^{\phi(s) ^T \theta}} \\
\bigtriangledown_\theta log \pi_\theta(a\mid s)=\phi(s) -  \sum_{a' \in A} \pi_\theta(a'\mid s) \phi(s)
$$


同理，如果我们使用的是Gaussian，也就对应着叫Gaussian Policy:


$$
\pi_\theta(a\mid s)=N(\phi(s) ^T \theta;\Sigma)\\
log\pi_\theta(a\mid s)=-\frac{1}{2} \mid \mid \phi(s) ^T \theta-a\mid \mid _\Sigma^2 +const\\
\bigtriangledown_\theta log \pi_\theta(a\mid s)=-\frac{1}{2}\Sigma^{-1}(\phi(s) ^T \theta-a) \phi(s)
$$


这个score function的含义还是类似的，他也刻画了action与平均action的差异。注意，以上两个例子中，我们都假定了policy中的输入参数state是policy 参数$\theta$和state feature $\phi(s)$之间的linear combination $\phi(s) ^T \theta$。如果需要加大表达能力，则完全可以采用其他函数，比如神经网络$f(s)$。

现在，我们把convenient identity带入到$\bigtriangledown_\theta J(\theta)$中就得到


$$
\bigtriangledown_\theta J(\theta)=\int \pi_\theta(\tau)\bigtriangledown_\theta log \pi_\theta(\tau) r(\tau) d\tau=E_{\tau \sim \pi_\theta(\tau)} [\bigtriangledown_\theta log \pi_\theta(\tau) r(\tau)]\\
=E_{\tau \sim \pi_\theta(\tau)} \left[ \left(\sum_{t=1}^T \bigtriangledown_\theta log \pi_\theta(a_t\mid s_t) \right) \left( \sum_{t=1}^T \gamma^{t-1} r(s_t,a_t)\right) \right]
$$


也就是说，这个identity可以在保证期望形式不变的条件下，简化计算。形式优美，让当下RL科研工作者沿用至今，是一个常用的方法。那么这个导数里的期望怎么算？采样！


$$
\bigtriangledown_\theta J(\theta) \approx \frac{1}{N} \sum^N_{i=1} \left(  \left(\sum_{t=1}^T \bigtriangledown_\theta log \pi_\theta(a_{i,t}\mid s_{i,t}) \right) \left( \sum_{t=1}^T \gamma^{t-1} r(s_{i,t},a_{i,t})\right) \right)
$$


到这里，policy gradient和极大似然估计还是比较神似的。只是说，RL中对episode加权咯，优化的结果就是，它会偏好reward高的episodes，使得它们出现概率更高，相反不好的action所导致的episode，出现概率低。


$$
\bigtriangledown_\theta J(\theta) \approx \frac{1}{N} \sum^N_{i=1} \left(  \left(\sum_{t=1}^T \bigtriangledown_\theta log \pi_\theta(a_{i,t}\mid s_{i,t}) \right) \left( \sum_{t=1}^T \gamma^{t-1} r(s_{i,t},a_{i,t})\right) \right)\\
=\frac{1}{N} \sum^N_{i=1} \bigtriangledown_\theta log \pi_\theta(\tau_i) r(\tau_i)\\
\bigtriangledown_\theta J_{ML}(\theta) \approx \frac{1}{N} \sum^N_{i=1} \left(\sum_{t=1}^T \bigtriangledown_\theta log \pi_\theta(a_{i,t}\mid s_{i,t}) \right)=\frac{1}{N} \sum^N_{i=1} \bigtriangledown_\theta log \pi_\theta(\tau_i)
$$



其实，REINRORCE算法就是这么做的，这里折扣因子$\gamma=1$

![](/img/rl2.png)

该算法会在每次迭代中，从当前的policy中采样，也就是用这个policy来玩一下游戏，得到一些episodes，然后用采样样本计算梯度，执行梯度上升算法（因为我们要max，而不是min）以提升当前policy。所以REINFORCE算法是一种On-Policy Monte-Carlo Policy-Based RL算法。这里的On-policy指用于学习policy $\pi$的样本是原于$\pi$本身的；相反则是off-policy，用于学习policy $\pi$的样本是原于其他策略。



### 5.1 Improved Policy Gradients

Policy Gradient和大家常用的SGD同样都有high variance，slow convergence以及难以选择learning rate的问题。对于variance的问题，可以想象成如果policy空间铺得太开，就不容易选中最应该选择的action（比如高斯的均值处）。下面就reduce variance做一个总结，其他的问题可以通过叫natural policy gradient的方法解决。

一般来说，降低variance，可以走两种方案

1. Causality, reward to go：今天做的动作，不会影响昨天发生的事
2. Baselines：选一个比平均动作更好的动作

**Causality, Reward to go**

简单的形式如下。唯一不同的地方就是对reward的sum，不在是从1开始，而是从当前action发生的时刻开始，对于当前action动作以前的时刻所产生的reward不在考虑。这时候的方差减小的原因是。。reward叠加的数值变小咯，相对variance也会减少。


$$
\bigtriangledown_\theta J(\theta) \approx \frac{1}{N} \sum^N_{i=1} \left(  \left(\sum_{t=1}^T \bigtriangledown_\theta log \pi_\theta(a_{i,t}\mid s_{i,t}) \right) \left( \sum_{t‘=t}^T \gamma^{t'-t} r(s_{i,t'},a_{i,t'})\right) \right)
$$


**Baselines**

Baseline想做的是，与其直接最大化每个episode的累计reward，不如最大化每个episode reward 与 平均reward的差值，使得episode能做得比平均值更好。对于比平均做得差的，那就降低他的出现概率。


$$
\bigtriangledown_\theta J(\theta) \approx \frac{1}{N} \sum^N_{i=1} \bigtriangledown_\theta log \pi_\theta(\tau_i)[ r(\tau_i) - b]\\
b=f(r(\tau))
$$


与之前的公式相比，新添加的b并不会带来影响，因为期望是等于0的


$$
E[ \bigtriangledown_\theta log \pi_\theta(\tau) b]=\int \pi_\theta(\tau) \bigtriangledown_\theta log\pi_\theta(\tau) b d\tau=\int  \bigtriangledown_\theta \pi_\theta(\tau)  bd\tau=b\bigtriangledown_\theta \int \pi_\theta(\tau) d\tau=0
$$


那么b应该取什么值呢？一开始引入Baseline的原因是为了降低variance，那么引入b之后的variance等于啥呢


$$
Var=E_{\tau \sim \pi_\theta(\tau)}[\left( \bigtriangledown_\theta log \pi_\theta(\tau)[ r(\tau) - b] \right)^2] - E_{\tau \sim \pi_\theta(\tau)}[\bigtriangledown_\theta log \pi_\theta(\tau)[ r(\tau) - b] ]^2
$$


为了最小化上面的式子，可以对b求导，可以得到下面的第一个式子，但是在实际使用中往往使用第二个，也就是第一个的不加权求和的版本。


$$
b=\frac{E\left[(\bigtriangledown_\theta log \pi_\theta(\tau))^2 r(\tau)\right]}{E\left[(\bigtriangledown_\theta log \pi_\theta(\tau))^2\right]}\\
b=\frac{1}{N}\sum_{i=1}^N r(\tau_i)
$$

### 5.2 Off-policy policy gradients

之前说到的policy gradient是on-policy的，我们使用源于同一个policy下的样本来更新policy本身。当然，我们也可以使用off-policy的方式。on-policy的方式，一旦policy改变了，就必须在下一次迭代的时候。重新采样新的episode，折痕inefficient！off-policy是从别的策略（叫做behaviour policy）中采样，来更新我们想要的策略（target policy），这种方式最大的好处就在于可以做policy exploration，而不是仅仅的policy exploitation。啥意思？RL最大的bug就在于policy和value funciton的搜索空间实在是太大了，在寻找最优解的时候，我们要竟可能的去探索未知（exploration），同时需要在已经探索过的地方，充分挖掘，找出最好（exploitation）。这两者之间的矛盾就好比晚饭吃去过的最好餐馆还是去尝试新馆子，新馆子当然会有好有坏。总的来说，off-policy提供了一个behavior policy，以便我们用别的知识来接入。

从别的地方采样，用于自己的任务，这种方式就是大家熟悉而喜爱的Importance Sampling干的事。简单的说，重要性采样提供了直接近似期望的框架，它假定直接从目标分布P(X)中采样无法完成，但是对于任意给定的X值，我们可以很容易地计算P(X)的值。如此，为了计算其期望$E[f(X)]=\int f(X)P(X)dX$，我们希望对P(X)值大（质量大）的区域中尽可能多的进行采样，因为这部分的样本对期望的计算发挥的作用更大。重要性采样，把容易被采样的新的分布Q(X)作为目标采样分布，并且赋予这些采样的样本一定的权重，整个样本上的不同大小的权重值的则构成了真实的P(X)的采样数据，如下所示。


$$
E_{x \sim p(x)}[f(x)]=\int p(x)f(x)dx=\int q(x)\frac{p(x)}{q(x)}f(x)dx=E_{x \sim q(x)}[\frac{p(x)}{q(x)}f(x)]
$$


那么应用到RL的目标中就是


$$
J(\theta) = E_{\tau \sim \pi'(\tau)} [\frac{\pi_\theta(\tau)}{\pi'(\tau)}r(\tau)]\\
\frac{\pi_\theta(\tau)}{\pi'(\tau)}=\frac{p(s_1) \prod_{t=1}^T \pi_\theta(a_t\mid s_t)p(s_{t+1}\mid s_t,a_t)}{p(s_1) \prod_{t=1}^T \pi'(a_t\mid s_t)p(s_{t+1}\mid s_t,a_t)} = \prod_{t=1}^T \frac{\pi_\theta(a_t\mid s_t)}{\pi'(a_t\mid s_t)}
$$


对应的梯度是下式，其中第三个式子，是在第二个式子的基础上考虑到Causality之后得到的：第三项表示现在的动作不会影响以前的reward，第二项是表示当前的动作仅仅与之前的策略有关。


$$
\bigtriangledown_\theta J(\theta) = E_{\tau \sim \pi'(\tau)} [\frac{\pi_\theta(\tau)}{\pi'(\tau)} \bigtriangledown_\theta log \pi_\theta(\tau) r(\tau)]\\
=E_{\tau \sim \pi'(\tau)} \left[  \left(\prod_{t=1}^T \frac{\pi_\theta(a_t\mid s_t)}{\pi'(a_t\mid s_t)}\right)\left(\sum_{t=1}^T \bigtriangledown_\theta log \pi_\theta(a_{i,t}\mid s_{i,t}) \right) \left( \sum_{t‘=1}^T \gamma^{t'-1} r(s_{i,t'},a_{i,t'})\right)\right] \\
=E_{\tau \sim \pi'(\tau)} \left[ \sum_{t=1}^T \bigtriangledown_\theta log \pi_\theta(a_{i,t}\mid s_{i,t})  \left(\prod_{t'=1}^t \frac{\pi_\theta(a_t'\mid s_t')}{\pi'(a_t'\mid s_t')}\right) \left( \sum_{t‘=t}^T \gamma^{t'-t} r(s_{i,t'},a_{i,t'})\right)\right]
$$



### 5.3 Other Policy Gradients Methods

前面都是要求policy是可导的，但是如果policy实在不可导怎么办？有一个简单的办法就是去一维一维的近似梯度。这个方式很低效而且也会有很大error，但是也算是一种策略嘛


$$
\frac{\partial J(\theta)}{\partial \theta_k} \approx \frac{J(\theta+\epsilon u_k)-J(\theta)}{\epsilon}
$$


对于REINFORCE算法，他的梯度计算方式下式。这里的$r(\tau)$是整个episode的累计reward，这也就意味着，必须要等到episode执行完了之后才能更新


$$
\bigtriangledown_\theta J(\theta)=E_{\tau \sim \pi_\theta(\tau)} [\bigtriangledown_\theta log \pi_\theta(\tau) r(\tau)]
$$


这之外还有很多其他的算法针对$r(\tau)$这项做变化


$$
\bigtriangledown_\theta J(\theta)=E_{\tau \sim \pi_\theta(\tau)} [\bigtriangledown_\theta log \pi_\theta(\tau) r(\tau)] \rightarrow  \mbox{   REINFORCE algorithm} \\
\bigtriangledown_\theta J(\theta)=E_{\tau \sim \pi_\theta(\tau)} [\bigtriangledown_\theta log \pi_\theta(\tau) Q_w(\tau)] \rightarrow\mbox{   Q Actor-Critic algorithm}\\
\bigtriangledown_\theta J(\theta)=E_{\tau \sim \pi_\theta(\tau)} [\bigtriangledown_\theta log \pi_\theta(\tau) A_w(\tau)]\rightarrow\mbox{   Advantage Actor-Critic algorithm}\\ 
\bigtriangledown_\theta J(\theta)=E_{\tau \sim \pi_\theta(\tau)} [\bigtriangledown_\theta log \pi_\theta(\tau) \delta ]\rightarrow\mbox{   TD Actor-Critic algorithm}\\ 
\bigtriangledown_\theta J(\theta)=E_{\tau \sim \pi_\theta(\tau)} [\bigtriangledown_\theta log \pi_\theta(\tau) \delta e]\rightarrow\mbox{   TD(lambda) Actor-Critic algorithm}\\
...
$$

# 6. Actor-Critic Algorithms

## 6.1 Reduce variance by actor-critic algorithm

之前REINFORCE算法通过$\bigtriangledown_\theta J(\theta) \approx \frac{1}{N} \sum^N_{i=1} \bigtriangledown_\theta log \pi_\theta(\tau_{i}) r(\tau_{i}) $来计算梯度，但是$r(\tau_{i})$只是依赖一个样本episode，这也会带来hige variance（这和SGD一样，mini-batch的variance总比SGD小）。实际上从某个状态出发，可能会有很多中的走法（如下图所示）。一个比较好的替换策略是采用真实的期望reward to go来代替。

![](/img/ac.png)

也就是说，我们可以用下式计算梯度以进一步降低方差

$$
\bigtriangledown_\theta J(\theta) \approx \frac{1}{N} \sum^N_{i=1}  \sum_{t=1}^T \bigtriangledown_\theta log \pi_\theta(a_{i,t}\mid s_{i,t}) Q(s_{i,t},a_{i,t})
$$

还可以进一步加入Baseline。之前的b采用的平均return。现在则是采用$V(s_{i,t})$。他也是一种对Q在不同action下的平均。

$$
V(s_{t})=E_{a_t \sim \pi_\theta(a_t \mid s_t)}[Q(s_{t},a_{t})]
$$

这时候称$A(s_{t},a_{t})=Q(s_{t},a_{t})-V(s_{t})$为advance function，它衡量的同样也是一个action比"平均action"好多少。相比于之前的$r(\tau_{i})-b$，advance function的方差要小很多。那么问题来了，这个advance function要怎么求？我们不知道Q，也不知道V，更不知道A了。

事实上，对于A的求解，在实际算法里只求解V就好了：

$$
Q(s_t,a_t)=r(s_t,a_t)+V(S_{t+1})=r(s_t,a_t)+E_{s_{t+1} \sim P(s_{t+1}\mid s_{t}, a_t)}[V(s_{t+1})]
$$

如果model是已知的（也就说$P(s_{t+1} \mid s_{t}, a_{t})$），那么上面的式子就可以直接带入A中。如果model是未知的，那么往往采用近似的方法（事实上，就算model已知，也可以采用近似，毕竟计算量小）。这里有两种近似$Q(s_{t},a_{t})$的方式，一种是采用MC采样，另一种是通过bootstrap。

对于MC来说，他会用样本的平均return来代替Q

$$
Q(s_t,a_t) = \sum_{t'=t} \gamma^{t'-t} r(s_{t'},a_{t'})\\
A_{MC}(s_t,a_t) \approx \sum_{t'=t} \gamma^{t'-t} r(s_{t'},a_{t'}) - V(s_t)
$$

对于bootstrap来说，我们直接用下一个状态来带入就行了，不必去遍历整个状态空间。虽然这个方式损失了一些信息，但是相比于$r(\tau)$的方式，还是要好太多了。

$$
Q(s_t,a_t) \approx r(s_t,a_t)+V(s_{t+1})\\
A_{Bootstrap}(s_t,a_t) \approx r(s_t,a_t)+V(s_{t+1}) - V(s_t)
$$

那么，我们只用得到V就可以求出A咯。要得到V，就要用到Actor-Critic Algorithm了。之前在policy gradient中，我们介绍的算法都只是在求policy（也就是actor），而这个算法还要求value（也就是critic干的事）。

1. Critic：用于更新Value function，判断当前policy的好坏
2. Actor：接收critic的信息，用于更新policy（update policy in direction suggested by critic）



## 6.2 Improve policy gradient by critic

那么，怎么用critic，在给定policy的时候求解V呢？这就要用到policy evaluation了。

### 6.2.1 Policy evaluation with known MDPs

policy evaluation是建立上Bellman Equation之上的。用上一时刻的value function来做更新

$$
v_{k+1}(s)=\sum_{a \in A} \pi(a\mid s) \left(R_s^a + \gamma \sum_{s' \in S}P^a_{ss'} v_k(s') \right)
$$

简单的说，这是一个迭代的过程，直至收敛：$v_{1} \rightarrow v_{2} ... \rightarrow v_{\pi}$。在第k+1次迭代的时候，对于所有的状态s，都需要用第k次迭代得到的v来更新。这个过程是可以保证收敛到真实值上去的。下面是一个经典的例子：给定一个4*4的格子，格子上有两个门（如图中阴影部分），小明需要尽快的从一个门到另一个门，每走一步，小明就会收到-1的reward。当给定一个随机策略（也就说在空白格子上可以等概率的选择任意方向前进），现在要评估每一个格子上的expected return。

在刚开始，初始化v都等于0，然后下一步，我们按照上面的公式进行迭代，可以得出v都变成了-1（两个门的地方，我们不做更新），以此类推

![](/img/ac1.png)![](/img/ac2.png)

当进行很多次之后，v矩阵就收敛不变了，这时候就得到在随机策略下，各个state的v函数了。左图表示一个迭代之后的结果，也就是迭代之后的state value function。并且每一次迭代之后产生的v都能对应得到一个明确的policy（右图）。注意，右图只是单纯画出来的而已，在evaluation的过程中，并没有更新policy本身。

### 6.2.2 Policy evaluation with unknown MDPs

前面提到的方式是建立在Bellman方程上的，也就说必须要知道状态转移P。在实际情况下，大多都是不知道的。那么，在不知道MDPs的情况下，如何做Policy Evaluation呢？

#### 6.2.2.1 Monte-Carlo Learning 

MC走的就是采样的路，用样本来近似求解，比如用 样本均值来近似期望等等。所以，MC的方式是用过样本的平均return来近似State-value function的。但是，MC方法需要对整个episode进行采样，必须要得到整条episode之后，才能做后续的操作（这里隐藏暗示了每一个episode都会terminate），所以MC方法一般是off-line update的。

MC方法一般有两种

1. **First-visit Monte-Carlo Policy Evaluation**
2. **Every-visit Monte-Carlo Policy Evaluation**

两者之间的差别不大，先说说第一种。假设要对state s计算他的v函数，那么：

1. 采用好多个episodes
2. 遍历每一个episode
   1. 如果s在该episode中的第t时刻被**第一次访问到了**
      1. $N(s) += 1$
      2. $S(s) += G_{t}$
3. 计算$V(s)=\frac{S(s)}{N(s)}$

第二种方式的不同之处在于如果s在该episode中的任何时刻被访问到了，则都进行N和S的更新。另外，更常用的是增量式更新的策略：

$$
u_k=\frac{1}{k} \sum_{j=1}^k x_j=\frac{1}{k} \left( x_k + \sum_{j=1}^{k-1}x_j \right) = \frac{1}{k} (x_k + (k-1)u_{k-1})=u_{k-1}+\frac{1}{k}(x_k-u_{k-1})
$$

那么，就可以有两种以下更新方式，第一种是常规的MC

$$
N(S_t) += 1 \\
V(S_t) \leftarrow V(S_t) + \frac{1}{N(S_{t})} (G_t -V(S_t))
$$

第二种是更广泛意义，它用于非稳定环境下的更新问题，对应时间久远的episode就不那么关注了，

$$
N(S_t) += 1 \\
V(S_t) \leftarrow V(S_t) +\alpha (G_t -V(S_t))
$$


#### 6.2.2.2 Temporal-Difference Learning (TD Learning) 

TD Learning不必从完整的episode中学习，反而是采用bootstrapping（引导）的方式进行online的更新。具体来说，TD采用以下的更新方式。其中R是immediate reward

$$
V(S_t) \leftarrow V(S_t) +\alpha (R_{t+1} + \gamma V(S_{t+1}) -V(S_t))
$$

很明显，TD与MC的第一个不同之处便是：TD采用$R_{t+1} + \gamma V(S_{t+1})$代替了$G_{t}$。这个$R_{t+1} + \gamma V(S_{t+1})$被称为TD target，而$R_{t+1} + \gamma V(S_{t+1}) -V(S_{t})$被称为TD error。这个算法被称为TD(0)算法。TD learning旨在步步为营的优化求解，用下一步的状态来更新当前步，也就不用一次性走完全部的episode。下面是一个例子：预计到家还有多少分钟。从开车到回家的路上，算是一个episode，到家了就terminate。左图是用MC的方式，在episode中的每一步与最后的return的差值$G_{t} -V(S_{t})$用箭头表示，而右图是TD的方法，他的差异是由下一个状态的值决定，而不是最后的$G_{t}$。

![](/img/ac3.png)

在当前state下，怎么得到下一个state呢？其实，我们直接在当前state下，按照policy走一步，就可以得到下一步的state了。TD(0)算法的伪代码如下：

![](/img/ac4.png)

总的来说，TD和MC的第一个不同之处就是：

1. MC采用了整个episode的return来做更新，TD采用TD target，可以在线更新
2. MC方式不能用于没有终止状态的MDP，TD可以
3. MC采用$G_{t}$来更新，而$G_{t}$是$v_\pi(S_{t})$的无偏估计。原本来说，TD target也是无偏的，但是！实际操作中（如算法中，我们是采用下一步的v的估计量来更新的，这时候的下一步v是不准确的！）是有偏估计的。但是TD方法仅仅依赖一个action，一次transition和一个reward，MC是好多好多个。所以TD的方差远小于MC的
4. 总的来说，MC收敛性好，初值不敏感。TD反之，但是TD效率高于MC
5. TD target是建立在Markov假设上的，所以要是环境不太满足假设，还是MC的效果会好一些

#### 6.2.2.3 N-step TD & TD($\lambda$)

一个直观的想法是，TD(0)是向前多走了一步，那么能不能多走n步呢？答案当然是可以的，那就是n-step TD算法。

$$
G_t^n = R_{t+1} + \gamma R_{t+2} + ... \gamma ^{n-1} R_{t+n} + \gamma^n V(S_{t+n}) \\
V(S_t) \leftarrow V(S_t) +\alpha (G_t^n -V(S_t))
$$

正因为n-step TD算法多走的那几步，进一步的降低了TD(0)算法的bias，但是，走多了，方差就会变得大起来。所以n-step TD约等于是一个bias和variance之间的trade-off：多走几步，降低bias，然后截断，之后的步子用V来代替，防止方差变大。当n-step TD之间走到最后，那就变回MC算法

![](/img/ac5.png)

但是要走几步才好呢？答案是不知道。但是一个解决方式是考虑Multi-kernel learning：既然不知道那个kernel好，那就把他们都加起来，取个平均。TD learning也是这个思路。比如对2-step和4-step取均值

$$
\frac{1}{2}  G^2 + \frac{1}{2} G^4
$$

但是，对于不同的n，n-step TD的价值是不一样的。当n比较小的时候，它引入的方差是更小的。所以，在结合多种走法的时候，他们的权重是递降的。并且，如果前n步都打算结合起来，那么就很费时间了，因为每一个n都得求一次，加起来。为了高效的处理这个问题，引入了TD($\lambda$)算法。

首先，TD($\lambda$)定义了一个叫$\lambda-$return的东西来定义带权的return

$$
G_t^\lambda=(1-\lambda)\sum_{n=1}^\infty \lambda^{n-1}G_t^{(n)} \\
V(S_t) \leftarrow V(S_t) + \alpha (G_t^\lambda-V(S_t))
$$

实际上，现在这个样子的算法复杂度还是很高的，等于还是要跑完整个episode，然后记录中间值进行融合更新。事实上，这个算法叫做forward view TD($\lambda$)，它仅仅是提供一个理论上的理解。它就像是在当前时刻，向前看，对以后时刻的return加权融合一样，然后用这个未来视角来更新当前的v。

![](/img/ac9.png)



真正高效的算法是backward view TD($\lambda$)。在Backward TD($\lambda$)里，它有一个与每个状态相关联的附加内存变量，Eligibility Traces。一个状态s在时间t时候的Eligibility Trace为$E_{t}(s)$。Eligibility Traces和Hawkes Process的思维很像，用于刻画经常发生和最近发生的事件。不过Eligibility Traces的形式更加简便。如果要计算state s的Eligibility Traces，可以用下式计算。其中$1(S_{t}=s)$表示，如果第t时刻的状态是s的话取值为1，否则是0

$$
E_0(s)=0\\
E_{t}(s)=\gamma \lambda E_{t-1}(s) + 1(S_t=s)
$$

总的来说，Eligibility Trace每个时间窗口下都在以$\gamma\lambda$缩减（因此，$\lambda$被称为trace-decay parameter），但是，如果在t时刻状态停留在s上，那么就给$E_{t}(s)$加一。它的图像如下：

![](/img/ac6.png)

结合Eligibility Traces，可以得到backward TD($\lambda$)算法的更新方式

$$
\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\\
V(s) = V(s) + \alpha \delta_tE_t(s)
$$

backward TD是在当前状态下，首先计算出TD Error $\delta_{t}$，然后将TD Error回传，通知以前别的状态处的v做更新，这个更新结合了传过来的TD error和自己状态的Eligibility Traces。



![](/img/ac10.png)



#### 6.2.2.4 Relations MC, TD(0) and TD($\lambda$)

**TD(0)=TD($\lambda$), $\lambda=0$**

从算法名字就知道，TD(0)算法就是TD($\lambda$)算法在$\lambda=0$时候的特例。此时就是一次更新一个状态，因为如果$S_{t}=s$，则状态不是s的Eligibility Traces是等于0的：$E_{t}(s)=1(S_{t}=s)$。此时，对于状态s的更新就是下式，非s的不变。

$$
\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\\
V(S_t) = V(S_t) + \alpha \delta_t
$$

**TD($\lambda$)在offline 更新的时候，backward view = forward view**

**当$\lambda=1$并且TD采用offline更新的时候，TD(1)=every visit MC**

![](/img/ac12.png)



#### 6.2.2.5 Other Methods

上面说的那个方法实际上都不太适用于large MDPs，如果state或（和）action的状态空间很大，比如是个联系空间，那么上面的方法都难以处理。实际上，我们可以用一下function approximation来做，比如V(s)就是一个神经网络等等，我们转而去求解神经网络的参数就可以得到V函数。这个东西又是一个大类，将会在后续章节进行介绍。

### 6.2.3 Actor-critic policy gradient

在知道了怎么用critic做policy evaluation之后，就可以用actor-critic来做policy gradient啦。这里有两种方式做更新，一个是batch-based，一种是online的。

![](/img/ac8.png)

步骤中的第二个，我们可以用很多中的方式进行更新，不同的方式，就是上一章最后提到的不同的算法啦

$$
\bigtriangledown_\theta J(\theta)=E_{\tau \sim \pi_\theta(\tau)} [\bigtriangledown_\theta log \pi_\theta(\tau) r(\tau)] \rightarrow  \mbox{   REINFORCE algorithm} \\
\bigtriangledown_\theta J(\theta)=E_{\tau \sim \pi_\theta(\tau)} [\bigtriangledown_\theta log \pi_\theta(\tau) Q_w(\tau)] \rightarrow\mbox{   Q Actor-Critic algorithm}\\
\bigtriangledown_\theta J(\theta)=E_{\tau \sim \pi_\theta(\tau)} [\bigtriangledown_\theta log \pi_\theta(\tau) A_w(\tau)]\rightarrow\mbox{   Advantage Actor-Critic algorithm}\\ 
\bigtriangledown_\theta J(\theta)=E_{\tau \sim \pi_\theta(\tau)} [\bigtriangledown_\theta log \pi_\theta(\tau) \delta ]\rightarrow\mbox{   TD Actor-Critic algorithm}\\ 
\bigtriangledown_\theta J(\theta)=E_{\tau \sim \pi_\theta(\tau)} [\bigtriangledown_\theta log \pi_\theta(\tau) \delta e]\rightarrow\mbox{   TD(lambda) Actor-Critic algorithm}\\
...
$$

一开始我们是通过advance function中引出actor-critic的，这种方式critic是用来评估state value function的。

$$
A(s_t,a_t) \approx r(s_t,a_t)+V(s_{t+1}) - V(s_t)
$$

其实上A还可用Q来近似，同时，critic还可以用来评估action value function。一个例子就是Q-Prop算法。 



**TO BE CONTINUE**




























