---
layout:     post
title:      Reinforcement Learning学习笔记
subtitle:   入坑指南
date:       2018-05-03
author:     HC
header-img: img/post-bg-android.jpg
catalog: true
tags:
    - Reinforcement Learning
    - MDP
    - Policy
---

# 0. Dynamic System

**动态模型**是用来描述一个给定空间（如某个物理系统的状态空间）中点随时间的变化情况。例如描述钟摆晃动、管道中水的流动，或者湖中每年春季鱼类的数量，凡此等等的数学模型都是动态系统 (From Wiki)。比如下图以点的运动为例子，左图展示了在不同时刻t上，构成的点X的移动轨迹。 右图则是加入了额外噪声的情况。

![](/img/ds.png)

动态系统旨在对时序数据的刻画，它由三个元素组成：**状态值**，**状态转移**和**观察值**。比如我们以股市为例（如下的图）。它有三个状态（Bull，Bear，Even），在每个状态下股市都表现出特定的涨跌，最后每个状态之间会以一定的概率进行状态转移。

![](/img/ds1.png)



动态系统假设仅仅前后两个时刻的状态是相互依赖的，符合**马尔科夫条件独立性假设。**因此，可以将状态Xt看做是$X_{t−1}$的函数，**$P(X_{t}\mid X_{t-1})$**。而在状态$X_{t}$上，会以某种方式表现出（或被观测到）$Y_{t}$，即**$P(Y_{t}\mid X_{t})$**。一般来说，ML 中的动态系统可以分为三种情况，**离散状态**，**线性高斯**和**非线性非高斯**。如下所示：

![](/img/ds2.png)

1. 离散状态的动态系统的例子就是**隐马尔科夫模型（HMM）**。它由一个**State Transition Matrix $P(x'\mid x)$**，**Measurement Matrix $P(y\mid x)$**和一个**Initial State π**组成。注意：HMM 的状态X必须是离散值，而不是连续值，否则 HMM 的状态转移矩阵就是无穷大的。而对Measurement Y没有做要求。
2. **线性高斯动态系统**和**非线性非高斯动态系统**的状态是连续的，线性高斯的State Transition Matrix和Measurement Matrix对应服从高斯分布，而非线性非高斯则可以是任意的。线性高斯动态系统的例子是**Kalman Filter**，而非线性非高斯的例子是**Partical Filter**

关于例子

1.  HMM上的常用的操作，一是 **uncertainty measurement**，已知模型参数和一个观测序列，求它产生的概率，二是 **parameter learning**，顾名思义就是去对λ = {A, B, π}进行求解。还有一种是做 **variable Inference**: 在 B 发生的条件下，A 发生的概率 $P(B\mid A)$

2.  卡尔曼滤波是线性动态系统的一种，它的图模型也和 HMM 一样，只是状态取值是连续值，并且它的噪声服从零均值高斯分布，transition probability 和 measurement probability 都服从高斯分布。卡尔曼滤波上的常用操作是 **filtering**, 给定 T = 1 到 t 时间的观测值，希望对时间 T=t 时的状态进行求解，目标为$P(X_{t}\mid Y_{1}, Y_{2}, … , Y_{t})$。这个filtering是不断的 predict （求解$P(X_{t}\mid Y_{1}, Y_{2}, … , Y_{t-1}) $）和不断 update（求解$P(X_{t}\mid Y_{1}, Y_{2}, … , Y_{t})$） 的过程。如下图：

    ![](/img/ds3.png)

3.  Particle filter 可以看错是一种**MC Sampling**的算法。具体来说他是建立在**Sequential Importance Sampling（SIS）**基础之上的。这个SIS算法是为了解决 Importance Sampling 的不能很好处理高维数据的问题，SIS 旨在对每个样本一维一维的进行采样。particle filter 是通过使用了SIS 进行求解的模型，其中 filtering 的分布由采样数据的位置和其权重来刻画

4.  Particle filter 与 Kalman filter 的区别在于，后者认为 measurement 和 transfer probability服从高斯分布，从而 filtering 的分布也服从高斯，他建立起的是 t 时刻和 t+1 时刻 filtering 高斯分布的参数联系；而 particle filter 可以适用于任何分布，所以求解麻烦，转而求助于 SIS 的方法，以当前时刻采样出来的 particle 的位置和权重来表示当前时刻的 filtering 分布。



# 1. Markov Process

上回，我们说到了动态系统。以HMM模型为例，它由状态，状态转移和观察值三部分组成，用来刻画随时间变化的事物。实际上，我们可以把动态系统看做更为广泛的模型：马尔科夫过程（Markov Process，MP），或者被叫做马氏链（Markov Chain）。

MP是一个状态序列满足马尔科夫条件独立性假设（即给定当前时刻的状态，下一时刻的状态与历史时刻的状态保持独立）的随机过程
$$
P(S_{t+1}\mid S_t) = P(S_{t+1}\mid S_1, S_2, ..., S_t)
$$
MP由两个部分组成：有限状态集合$S$和状态转移矩阵$P$，$P_{ss'}=P(S_{t+1}=s'\mid S_{t}=s)$。以下图为例，这是一个学生上课的MP图。学生需要听过三节课之后才能通过（pass）本门课，圆圈和方块表示状态，其中方块表示终止状态（只进不出），边上的数字表示状态转移概率。在第一节课的时候，同学可能会开小差，去刷facebook，并且一旦开始刷facebook，就会有极大的概率（0.9）一直刷下去。同理，也可能上课睡觉，或者课后去酒吧（pub），喝得烂醉之后重新听课。

![](/img/mrp.png)

对于HMM模型来说，他还存在观察量（Observations）。准确的来说，HMM其实是一个部分可观察的马氏链（Partially Observable Markov Chain ），因为当前的观察值并不足以完全确定当前所处的状态，就像只有视觉信号的机器人无法全面感知周围的环境一样。如果Observations = State，观察能全面刻画所处的状态信息，那么就叫Full Observability。

无论观察是否全面，MP通过$S$和$P$刻画了状态随时间的转移。如果学生上课的MP图以Class 1作为开始状态，那么，能够得到以下的状态序列

1. C1 C2 C3 Pass Sleep 
2. 
   C1 FB FB C1 C2 Sleep 

3. C1 C2 C3 Pub C2 C3 Pass Sleep 
4. ........

对于每一个状态序列，总是在终止状态下结束（当然，并不是所有的MP都含有终止状态，这时的状态序列将一直持续下去）。每一个序列都被称作是一个episode。这样的episode其实就是一条时序数据啦。

# 2. Markov Reward Process

上次说到了Markov Process，他由两部分组成，有限状态集合S和状态转移P。本次我们介绍Markov Reward Process（MRP），他是一个特殊的Markov Process，因为MRP的每个状态上都被附上一个值，这个值被称为Reward。同样，我们以学生上课的状态图为例，这次在每个状态里都附上了Reward（图中红色R）。学生通过考试（pass）可以得到10分的Reward，但是上课的过程是痛苦的，这里给了-2的Reward。

![](/img/mrp1.png)

相比MP，MRP由四个部分组成：

1. 有限状态集S
2. 状态转移P，$P_{ss'}=P(S_{t+1}=s'\mid S_{t}=s)$
3. Reward函数 R，$R_{s}=E[R_{t+1}\mid S_{t}=s]$。注意状态s上的Reward是一个期望值
4. 折扣因子（Discount Factor）$\gamma \in [0, 1]$。

在MRP中，Reward描述了某个状态下的奖励值。在上面的例子中，通过考试（Pass）以最高的奖励（10）来吸引学生向该状态转换。那么折扣因子用来干啥？实际上，MRP并不关心单独某个状态下的Reward，而是喜欢一个Reward的累加量，叫Return。Return G的定义如下：
$$
G_t=R_{t+1}+\gamma R_{t+2} + ... = \sum_{k=0}^\infty \gamma^k R_{t+k+1}
$$
$G_{t}$ 是在第t时刻的Return。从形式上看，Return计算了从t时刻的某个状态开始，某一个episode所获得的所有discounted Reward的总和。这里的折扣因子是一个0到1之间的数，对于第k+1时刻，累加的reward是$\gamma^k R$，因此时刻越久远，所计算的reward越小。$\gamma$越大，衰减也就越慢。当$\gamma=1$的时候，称为undiscounted MRP。

那为什么需要discount呢？

1. 避免无穷值（万一MRP没有终止状态，或者MRP中有环）。
2. 保留对未来的不确定性。（注意，$R_{s}$是一个期望值，$R_{t}$不是）
3. 其他解释：现在的钱比未来更值钱

因此，我们可以使用Return来表示从一个状态开始的一个episode所带来的奖励。我们以上次的episode为例，计算他们的Return（这里假设$\gamma=1/2$）：

1. C1 C2 C3 Pass Sleep ：G1=-2 -2* (1/2)-2* (1/4)+10* (1/8)
2. C1 FB FB C1 C2 Sleep ：G1=-2-1* (1/2) -1 * (1/4) -2* (1/8) -2 *(1/16)

那么，如果我们能得到很多个从同一个状态开始的episodes，便可以用它们来衡量该状态的好坏了（从好的状态开始采样，更容易得到更大的return）。对于状态好坏的衡量，MRP采用了Value Function，准确的说，是叫State Value Function：
$$
v(s)=E[G_t\mid S_t=s]
$$
状态价值函数衡量了在某一状态s下，所能获得的Return的期望。它可以看做是对Return的一种预测。那么，对于学生MRP例子，我们可以得到下图（$\gamma=1$），图中红色的数字就表示某状态的state value function的大小。比如对于Pass而言，只有一条路径到终止状态，那么他的v(s)就等于10

![](/img/mrp2.png)

那么，其他状态的v是怎么计算的呢？这就要引入一个公式了，叫Bellman Equation。具体来说，它认为一个状态的v函数可以分解成两部分

1. immediate reward $R_{t+1}$
2. 下一个状态的discounted $\gamma v(S_{t+1})$

用数学语言就是说：


$$
v(s)=E[G_t\mid S_t=s]\\
=E[R_{t+1}+\gamma R_{t+2} + \gamma^2R_{t+3}+...\mid S_t=s]\\
=E[R_{t+1} + \gamma G_{t+1}\mid S_t=s]=E[R_{t+1} + \gamma v(S_{t+1})\mid S_t=s]
$$


所以，Bellman Equation的形式如下。注意，在状态s的时候，我们并不知道下一个状态会是啥，所以公式中使用了$S_{t+1}$表示，而不是$s_{t+1}$
$$
v(s)=E[R_{t+1} + \gamma v(S_{t+1})\mid S_t=s]
$$
也就是说，我们需要遍历下状态空间，如下所示，s'表示下一个状态

![](/img/mrp3.png)

对应的公式变成：
$$
v(s)=R_s + \gamma \sum_{s' \in S}P_{ss'} v(s')
$$
我们来看看，学生MRP例子是不是符合这个式子（废话，当然是符合）

![](/img/mrp4.png)



对于这个形式来说，可以写成矩阵的形式求解：


$$
v=R+\gamma Pv = \begin{bmatrix}
v(1)
\\ 
...
\\ 
v(n)
\end{bmatrix} = \begin{bmatrix}
R_1
\\ 
...
\\ 
R_n
\end{bmatrix} + \gamma \begin{bmatrix}
P_{11} & ... & P_{1n}\\ 
... &...  & ...\\ 
P_{n1} & ... & P_{nn}
\end{bmatrix} \begin{bmatrix}
v(1)
\\ 
...
\\ 
v(n)
\end{bmatrix}
$$


对应的闭式解是：


$$
v=(I-\gamma P)^{-1}R
$$


当然，计算量很大，求逆是O(n^3)的复杂度，当状态多了，这是很恐怖的。当然也有很多的高级算法啦，比如动态规划，蒙特卡洛和Temporal-Difference learning (TD Learning)等等。之后我们慢慢推送，不急不急。。

# 3. Markov Decision Process

上回说到了MRP，这次我们做进一步的扩展，它的名字叫做Markov Decision Process (MDP)。它使得系统加入更多交互属性，可以用来刻画序列决策过程。与MRP相比，MDP加入了决策的动作。MDP由以下五个部分组成

1. 有限状态集合S
2. 有限动作（决策）集合A
3. 状态转移P，$P_{ss'}^a=P[S_{t+1}=s'\mid S_{t}=s, A_{t}=a]$
4. 奖励函数R，$R_{s}^a=E[R_{t+1}\mid S_{t}=s,A_{t}=a]$
5. 折扣因子$\lambda \in [0,1]$

MRP中，处于某个状态的事物可以做出一个决策动作。根据所处的状态以及所做的决策动作，该事物会获得一个reward，然后转移到新的状态。下图中就是大家熟悉而又喜爱的学生状态图，图中红色的字表示action。

![](/img/mdp.png)

实际上，在MDP中有两个角色：agent和environment。整个MDP就是agent和environment之间不断玩耍交互的过程。agent在第t时刻处于某一个state，agent做出一个action，然后收到一个reward；environment看到agent的action之后，将发送一个reward给agent，然后agent的state被转换。

实际上，agent往往并不能完全察觉自己处于什么样的状态，它只能通过对外界的观察（observation）来推断自己的状态（也就是说，这是partially observable）。所以整个过程就变成了下图所示：agent只接收到对state的观察。

![](/img/mdp1.png)

那么，agent为什么需要从observation中推断出state？这是因为MDP是state满足马尔科夫条件，而不是observation。一般来说，可以认为state是history of observations, action和reward的函数，即


$$
S_t=f(H_t)\\
\mbox{history：}H_t=O_1, R_1, A_1, ..., A_{t-1}, O_{t}, R_{t}
$$


因此，agent的目的就是要通过观察state，做出决策动作，以最大化能获得的累计reward。那么要怎么出招呢？agent需要一个出招秘籍，这个秘籍被称为agent的policy。policy是在给定state下，agent动作的分布
$$
\pi(a\mid s)=P(A_{t}=a\mid S_t=s)
$$
在MRP中，我们引入了State value function来衡量一个state的好坏。在MDP中，加入了动作之后，自然我们想要知道在给定state下，做出每个动作的好坏。这个功能是通过Action value function（或者叫Q函数）来实现的：


$$
q_\pi(s,a)=E_\pi[G_t\mid S_t=s,A_t=a]
$$


同样，state value function的Bellman Equation同样也适用于Q函数


$$
v_\pi(s)=E_\pi[R_{t+1}+\gamma v_\pi(S_{t+1})\mid S_t=s]\\
q_\pi(s,a)=E_\pi[R_{t+1}+\gamma q_\pi(S_{t+1},A_{t+1})\mid S_t=s, A_t=a]
$$


事实上，MDP中state s和action a就是这么交替的向后执行：在一个状态下，从动作空间A中选择做出某个动作a，然后不同的a将被转换到不同的新state下。用图片表示则是：

![](/img/mdp2.png)

同理，我们也可以从某个action出发，转移到某个状态，在新状态下做出下一个动作：

![](/img/mdp3.png)

我们可以便可以把Bellman Equation进一步写成：


$$
v_\pi(s) = E_\pi[R_{t+1} + \gamma v_{\pi}(S_{t+1}) \mid  S_t=s] = \sum_{a\in A} \pi(a\mid s) q_{\pi}(s,a)\\
q_\pi(s,a) = E_\pi[R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) \mid  S_t=s, A_t=a] = R_s^a + \gamma \sum_{s' \in S}P^a_{ss'} v_{\pi}(s')\\
v_\pi(s) = \sum_{a\in A} \pi(a\mid s) \left( R_s^a + \gamma \sum_{s' \in S}P^a_{ss'} v_{\pi}(s')\right)
$$


以下面的例子来说明，假设跳转概率没有特别说明的话，都是0.5，否则标注在边上。那么就可以验证下图中红色状态的state value function

![](/img/mdp4.png)

作为agent，他的目的是最大化最后的累计reward。为了达到这个目的，agent需要按照一个最好的policy来出招。实际上，对于任何一个MDP来说，一定存在一个（或者多个）最优policy $\pi^{\star}$ 。而最优的policy对应了最优的state value function $v_\star (s)$和最优的action value function $q_{\star}(s,a)$。他们分别表示在所有可能的policy下，所能达到的最大值，这个最大值就预示着在给定MDP下的做好结果


$$
v_*(s)=\max_\pi v_\pi(s)\\
q_*(s,a)=\max_\pi q_\pi(s,a)
$$


当我们知道最优的value function的时候，agent的最优策略也就知道了：


$$
\pi^*(a\mid s) = \left\{\begin{matrix}
1 \mbox{ if }a=\arg\max_{a\in A}q_*(s,a)
\\ 
0 \mbox{ otherwise}
\end{matrix}\right.
$$


比如下面的例子中的最优策略是红线，可以看出，学习才是最好的出路（手动滑稽）。

![](/img/mdp5.png)

图中的结果是怎么计算出来？这又要用到一个叫Bellman Optimality Equations。总的来说，整个过程还是步步为营的。首先，$v(s)$是在状态s下所能得到return的期望，$q(s,a)$是在给定状态s下，作出动作a之后所能得到的return的大小，然后，在作出动作a之后，将跳转到另一个新状态s'。因此，两者满足


$$
v_*(s)=\max_a q_*(s,a)\\
q_*(s,a) = R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v_*(s')
$$


那么，两者结合起来就是

![](/img/mdp6.png)

![](/img/mdp7.png)

实际上，Bellman Optimality Equation是非线性的，他不能像Bellman Equation一样有闭式解（虽然复杂度很高），但是还是有很多方法可以用来求解。比如Value Iteration, Policy Iteration, Q-learning和Sarsa等等。

# 4. Reinforcement Learning

从Markov Process，Markov Reward Process到Markov Decision Process，写了这么多，就是引出Reinforcement Learning（RL），几乎所有的RL问题都可以转换成MDP问题。当然了，当前RL研究的问题远不止之前我们所介绍的内容。

一般来说，机器学习可以分为三类：监督学习，无监督学习和强化学习（怎么可能是半监督学习）。在监督或无监督学习中，我们总是把目标数学化成优化loss的形式（即便，目标任务并不是最小化模型损失），而强化学习是对对序列动作建模，通过reward机制建立与任务之间的联系，它给我们提供了一种能够更为直接地去优化任务本身的方法。也就是说，有时候，人们首先是根据监督或者非监督模型的输出结果，然后针对任务作出决策（比如先判断眼前的动物是猫还是老虎，再决定我们要不要逃跑）。强化学习是一步到位，提供了从what we receiving到what we doing的一个close loop。最后，世间任务千千万，并不是所有的任务都可以明确写出优化目标，对于有些任务监督或者非监督学习，他们是不适用的（就比如下围棋AlphaGo），这也是强化学习另一个优势。PS，这也并不是说监督和非监督学习没啥用，不要偏激的理解，强化学习当然也不是万能的（钱才是）。

只能说明的是，RL虽然有reward反馈，但是，它并不是监督学习：

1. RL的反馈是可以有时延的。实际情况下，并不是做一个动作之后一定会收到一个真正的reward，比如有的游戏要玩到结束之后，才会收到reward，胜利+1，失败-1。监督学习的标签则不会这样
2. RL处理的是时序episode，监督学习是iid data
3. agent的决策动作是会影响下一步的进展的。

和MDP一样，RL的目的在于最大化累计reward。这也是RL的Reward Hypothesis：所有的目标都可以被描述成最大化累计reward，如下式，其中r是reward函数


$$
\max_\pi E_\pi \left[ \sum_{t}r(s_t,a_t) \right]
$$


一般来说，RL有两类任务：

1. Prediction：给定policy，估计未来, 找出value function
2. Control：在所有可能的policy中，最好的value function是啥？哪个是最好的policy？

这两类任务可以在不同的背景下完成，这里的背景指的是environment或者model是否已知：agent是否知道状态空间，状态转移以及reward function等等。所以强化学习可以分为

1. Model-Free RL：不知道model（一般来说是指不知道状态转移以及reward function）
2. Model-Based RL：知道model

另一方面，强化学习还可以分为

1. value-based RL：在没有明确的policy下，去找value function
2. policy-based RL：在没有value function的条件下，去找policy
3. Actor-critic RL：用当前的policy来估计value ，并用它来提升policy


总的来说，可以用下图表示

![](/img/mdp8.png)

RL的过程可以看做是agent和environment之间的游戏，是state和action之间的转换游戏，分别对应了transition和policy。他们可以用下面的图例过程表示。这里的参数$\theta$是用来parameterize策略的（比如使用神经网络来刻画policy，那么$\theta$就是神经网络的参数）。

![](/img/rl.png)

实际上，这个过程是一个Markov Chain on state and action!

![](/img/rl1.png)

满足


$$
p((s_{t+1},a_{t+1})\mid (s_{t},a_t))=p(s_{t+1}\mid s_t,a_t)\pi_\theta(a_{t+1}\mid s_{t+1})
$$


那么给定一个episode，他的概率可以写成


$$
p_\theta(s_1,a_1,...s_T,a_T)=p(s_1) \prod_{t=1}^T \pi_\theta(a_t\mid s_t)p(s_{t+1}\mid s_t,a_t)
$$



# 5. Policy Gredients

对于RL来说，我们想要找出一个最好的policy来完成任务，比如打游戏的时候见到坑要知道跳，见到怪要知道打，见到金币得知道吃等等。前面说的RL的最终任务就是要最大化期望累计reward，这里的最大化是通过找一个最优的policy来实现的。下面公式中的$\theta$是policy $\pi$的参数（比如我们用神经网络来计算policy，那么$\theta$就是网络的参数）


$$
\theta^*=\arg\max_\theta E_{\tau \sim \pi_\theta(\tau)} \left[ \sum_{t=1} \gamma^{t-1} r(s_t,a_t) \right]=\arg\max_\theta \int \pi_\theta(\tau) r(\tau) d\tau\\
\pi_\theta(\tau)=p_\theta(s_1,a_1,...,s_T,a_T)=p(s_1) \prod_{t=1}^T \pi_\theta(a_t\mid s_t)p(s_{t+1}\mid s_t,a_t)\\
r(\tau)=\sum_{t=1} \gamma^{t-1}r(s_t,a_t)
$$


学过监督学习的我们知道，在监督学习中很多很多的算法都是可以通过梯度下降的方法来求解的，那么给定RL的目标方程，我们同样也可以使用梯度的方法来更新求解policy，使得最大化期望累计reward。也就是说，只需要用积分那一项（令他为$J(\theta)$）对$\theta$求导就可以了。注意，这里我们假定了policy是可导的。policy的形式往往是人定的（比如gaussian，神经网络等等），所以policy可不可导也是可控的。


$$
\bigtriangledown_\theta J(\theta)=\int \bigtriangledown_\theta\pi_\theta(\tau) r(\tau) d\tau
$$


问题就变成求出$\bigtriangledown_\theta\pi_\theta(\tau)$就ok了，大牛们当然不会直接去求解这一项，因为从最开始的式子里可以看到，$\pi_\theta(\tau)$涉及到多项相乘，直接求导会很心累。所以大牛提出了一个 convenient identity


$$
\bigtriangledown_\theta\pi_\theta(\tau)=\pi_\theta(\tau) \frac{\bigtriangledown_\theta\pi_\theta(\tau)}{\pi_\theta(\tau)}=\pi_\theta(\tau)\bigtriangledown_\theta log \pi_\theta(\tau)
$$


其中在贝叶斯里往往把$\bigtriangledown_\theta log \pi_\theta(\tau)$叫做score function。比如以一次动作为例，当我们用softmax来近似policy $\pi_{\theta}(\tau)$的时候（这叫做Softmax Policy），这时的score function可以写成下式，这个式子衡量了当前动作相比于平均动作的好坏。


$$
\pi_\theta(a\mid s)= \frac{e^{\phi(s) ^T \theta }}{\sum_a e^{\phi(s) ^T \theta}} \\
\bigtriangledown_\theta log \pi_\theta(a\mid s)=\phi(s) -  \sum_{a' \in A} \pi_\theta(a'\mid s) \phi(s)
$$


同理，如果我们使用的是Gaussian，也就对应着叫Gaussian Policy:


$$
\pi_\theta(a\mid s)=N(\phi(s) ^T \theta;\Sigma)\\
log\pi_\theta(a\mid s)=-\frac{1}{2} \mid \mid \phi(s) ^T \theta-a\mid \mid _\Sigma^2 +const\\
\bigtriangledown_\theta log \pi_\theta(a\mid s)=-\frac{1}{2}\Sigma^{-1}(\phi(s) ^T \theta-a) \phi(s)
$$


这个score function的含义还是类似的，他也刻画了action与平均action的差异。注意，以上两个例子中，我们都假定了policy中的输入参数state是policy 参数$\theta$和state feature $\phi(s)$之间的linear combination $\phi(s) ^T \theta$。如果需要加大表达能力，则完全可以采用其他函数，比如神经网络$f(s)$。

现在，我们把convenient identity带入到$\bigtriangledown_\theta J(\theta)$中就得到


$$
\bigtriangledown_\theta J(\theta)=\int \pi_\theta(\tau)\bigtriangledown_\theta log \pi_\theta(\tau) r(\tau) d\tau=E_{\tau \sim \pi_\theta(\tau)} [\bigtriangledown_\theta log \pi_\theta(\tau) r(\tau)]\\
=E_{\tau \sim \pi_\theta(\tau)} \left[ \left(\sum_{t=1}^T \bigtriangledown_\theta log \pi_\theta(a_t\mid s_t) \right) \left( \sum_{t=1}^T \gamma^{t-1} r(s_t,a_t)\right) \right]
$$


也就是说，这个identity可以在保证期望形式不变的条件下，简化计算。形式优美，让当下RL科研工作者沿用至今，是一个常用的方法。那么这个导数里的期望怎么算？采样！


$$
\bigtriangledown_\theta J(\theta) \approx \frac{1}{N} \sum^N_{i=1} \left(  \left(\sum_{t=1}^T \bigtriangledown_\theta log \pi_\theta(a_{i,t}\mid s_{i,t}) \right) \left( \sum_{t=1}^T \gamma^{t-1} r(s_{i,t},a_{i,t})\right) \right)
$$


到这里，policy gradient和极大似然估计还是比较神似的。只是说，RL中对episode加权咯，优化的结果就是，它会偏好reward高的episodes，使得它们出现概率更高，相反不好的action所导致的episode，出现概率低。


$$
\bigtriangledown_\theta J(\theta) \approx \frac{1}{N} \sum^N_{i=1} \left(  \left(\sum_{t=1}^T \bigtriangledown_\theta log \pi_\theta(a_{i,t}\mid s_{i,t}) \right) \left( \sum_{t=1}^T \gamma^{t-1} r(s_{i,t},a_{i,t})\right) \right)\\
=\frac{1}{N} \sum^N_{i=1} \bigtriangledown_\theta log \pi_\theta(\tau_i) r(\tau_i)\\
\bigtriangledown_\theta J_{ML}(\theta) \approx \frac{1}{N} \sum^N_{i=1} \left(\sum_{t=1}^T \bigtriangledown_\theta log \pi_\theta(a_{i,t}\mid s_{i,t}) \right)=\frac{1}{N} \sum^N_{i=1} \bigtriangledown_\theta log \pi_\theta(\tau_i)
$$



其实，REINRORCE算法就是这么做的，这里折扣因子$\gamma=1$

![](/img/rl2.png)

该算法会在每次迭代中，从当前的policy中采样，也就是用这个policy来玩一下游戏，得到一些episodes，然后用采样样本计算梯度，执行梯度上升算法（因为我们要max，而不是min）以提升当前policy。所以REINFORCE算法是一种On-Policy Monte-Carlo Policy-Based RL算法。这里的On-policy指用于学习policy $\pi$的样本是原于$\pi$本身的；相反则是off-policy，用于学习policy $\pi$的样本是原于其他策略。



### 5.1 Improved Policy Gradients

Policy Gradient和大家常用的SGD同样都有high variance，slow convergence以及难以选择learning rate的问题。对于variance的问题，可以想象成如果policy空间铺得太开，就不容易选中最应该选择的action（比如高斯的均值处）。下面就reduce variance做一个总结，其他的问题可以通过叫natural policy gradient的方法解决。

一般来说，降低variance，可以走两种方案

1. Causality, reward to go：今天做的动作，不会影响昨天发生的事
2. Baselines：选一个比平均动作更好的动作

**Causality, Reward to go**

简单的形式如下。唯一不同的地方就是对reward的sum，不在是从1开始，而是从当前action发生的时刻开始，对于当前action动作以前的时刻所产生的reward不在考虑。这时候的方差减小的原因是。。reward叠加的数值变小咯，相对variance也会减少。


$$
\bigtriangledown_\theta J(\theta) \approx \frac{1}{N} \sum^N_{i=1} \left(  \left(\sum_{t=1}^T \bigtriangledown_\theta log \pi_\theta(a_{i,t}\mid s_{i,t}) \right) \left( \sum_{t‘=t}^T \gamma^{t'-1} r(s_{i,t'},a_{i,t'})\right) \right)
$$


**Baselines**

Baseline想做的是，与其直接最大化每个episode的累计reward，不如最大化每个episode reward 与 平均reward的差值，使得episode能做得比平均值更好。对于比平均做得差的，那就降低他的出现概率。


$$
\bigtriangledown_\theta J(\theta) \approx \frac{1}{N} \sum^N_{i=1} \bigtriangledown_\theta log \pi_\theta(\tau_i)[ r(\tau_i) - b]\\
b=f(r(\tau))
$$


与之前的公式相比，新添加的b并不会带来影响，因为期望是等于0的


$$
E[ \bigtriangledown_\theta log \pi_\theta(\tau) b]=\int \pi_\theta(\tau) \bigtriangledown_\theta log\pi_\theta(\tau) b d\tau=\int  \bigtriangledown_\theta \pi_\theta(\tau)  bd\tau=b\bigtriangledown_\theta \int \pi_\theta(\tau) d\tau=0
$$


那么b应该取什么值呢？一开始引入Baseline的原因是为了降低variance，那么引入b之后的variance等于啥呢


$$
Var=E_{\tau \sim \pi_\theta(\tau)}[\left( \bigtriangledown_\theta log \pi_\theta(\tau)[ r(\tau) - b] \right)^2] - E_{\tau \sim \pi_\theta(\tau)}[\bigtriangledown_\theta log \pi_\theta(\tau)[ r(\tau) - b] ]^2
$$


为了最小化上面的式子，可以对b求导，可以得到下面的第一个式子，但是在实际使用中往往使用第二个，也就是第一个的不加权求和的版本。


$$
b=\frac{E\left[(\bigtriangledown_\theta log \pi_\theta(\tau))^2 r(\tau)\right]}{E\left[(\bigtriangledown_\theta log \pi_\theta(\tau))^2\right]}\\
b=\frac{1}{N}\sum_{i=1}^N r(\tau_i)
$$

### 5.2 Off-policy policy gradients

之前说到的policy gradient是on-policy的，我们使用源于同一个policy下的样本来更新policy本身。当然，我们也可以使用off-policy的方式。on-policy的方式，一旦policy改变了，就必须在下一次迭代的时候。重新采样新的episode，折痕inefficient！off-policy是从别的策略（叫做behaviour policy）中采样，来更新我们想要的策略（target policy），这种方式最大的好处就在于可以做policy exploration，而不是仅仅的policy exploitation。啥意思？RL最大的bug就在于policy和value funciton的搜索空间实在是太大了，在寻找最优解的时候，我们要竟可能的去探索未知（exploration），同时需要在已经探索过的地方，充分挖掘，找出最好（exploitation）。这两者之间的矛盾就好比晚饭吃去过的最好餐馆还是去尝试新馆子，新馆子当然会有好有坏。总的来说，off-policy提供了一个behavior policy，以便我们用别的知识来接入。

从别的地方采样，用于自己的任务，这种方式就是大家熟悉而喜爱的Importance Sampling干的事。简单的说，重要性采样提供了直接近似期望的框架，它假定直接从目标分布P(X)中采样无法完成，但是对于任意给定的X值，我们可以很容易地计算P(X)的值。如此，为了计算其期望$E[f(X)]=\int f(X)P(X)dX$，我们希望对P(X)值大（质量大）的区域中尽可能多的进行采样，因为这部分的样本对期望的计算发挥的作用更大。重要性采样，把容易被采样的新的分布Q(X)作为目标采样分布，并且赋予这些采样的样本一定的权重，整个样本上的不同大小的权重值的则构成了真实的P(X)的采样数据，如下所示。


$$
E_{x \sim p(x)}[f(x)]=\int p(x)f(x)dx=\int q(x)\frac{p(x)}{q(x)}f(x)dx=E_{x \sim q(x)}[\frac{p(x)}{q(x)}f(x)]
$$


那么应用到RL的目标中就是


$$
J(\theta) = E_{\tau \sim \pi'(\tau)} [\frac{\pi_\theta(\tau)}{\pi'(\tau)}r(\tau)]\\
\frac{\pi_\theta(\tau)}{\pi'(\tau)}=\frac{p(s_1) \prod_{t=1}^T \pi_\theta(a_t\mid s_t)p(s_{t+1}\mid s_t,a_t)}{p(s_1) \prod_{t=1}^T \pi'(a_t\mid s_t)p(s_{t+1}\mid s_t,a_t)} = \prod_{t=1}^T \frac{\pi_\theta(a_t\mid s_t)}{\pi'(a_t\mid s_t)}
$$


对应的梯度是下式，其中第三个式子，是在第二个式子的基础上考虑到Causality之后得到的：第三项表示现在的动作不会影响以前的reward，第二项是表示当前的动作仅仅与之前的策略有关。


$$
\bigtriangledown_\theta J(\theta) = E_{\tau \sim \pi'(\tau)} [\frac{\pi_\theta(\tau)}{\pi'(\tau)} \bigtriangledown_\theta log \pi_\theta(\tau) r(\tau)]\\
=E_{\tau \sim \pi'(\tau)} \left[  \left(\prod_{t=1}^T \frac{\pi_\theta(a_t\mid s_t)}{\pi'(a_t\mid s_t)}\right)\left(\sum_{t=1}^T \bigtriangledown_\theta log \pi_\theta(a_{i,t}\mid s_{i,t}) \right) \left( \sum_{t‘=1}^T \gamma^{t'-1} r(s_{i,t'},a_{i,t'})\right)\right] \\
=E_{\tau \sim \pi'(\tau)} \left[ \sum_{t=1}^T \bigtriangledown_\theta log \pi_\theta(a_{i,t}\mid s_{i,t})  \left(\prod_{t'=1}^t \frac{\pi_\theta(a_t'\mid s_t')}{\pi'(a_t'\mid s_t')}\right) \left( \sum_{t‘=t}^T \gamma^{t'-1} r(s_{i,t'},a_{i,t'})\right)\right]
$$



### 5.3 Other Policy Gradients Methods

前面都是要求policy是可导的，但是如果policy实在不可导怎么办？有一个简单的办法就是去一维一维的近似梯度。这个方式很低效而且也会有很大error，但是也算是一种策略嘛


$$
\frac{\partial J(\theta)}{\partial \theta_k} \approx \frac{J(\theta+\epsilon u_k)-J(\theta)}{\epsilon}
$$


对于REINFORCE算法，他的梯度计算方式下式。这里的$r(\tau)$是整个episode的累计reward，这也就意味着，必须要等到episode执行完了之后才能更新


$$
\bigtriangledown_\theta J(\theta)=E_{\tau \sim \pi_\theta(\tau)} [\bigtriangledown_\theta log \pi_\theta(\tau) r(\tau)]
$$


这之外还有很多其他的算法针对$r(\tau)$这项做变化


$$
\bigtriangledown_\theta J(\theta)=E_{\tau \sim \pi_\theta(\tau)} [\bigtriangledown_\theta log \pi_\theta(\tau) r(\tau)] \rightarrow  \mbox{   REINFORCE algorithm} \\
\bigtriangledown_\theta J(\theta)==E_{\tau \sim \pi_\theta(\tau)} [\bigtriangledown_\theta log \pi_\theta(\tau) Q_w(\tau)] \rightarrow\mbox{   Q Actor-Critic algorithm}\\
\bigtriangledown_\theta J(\theta)==E_{\tau \sim \pi_\theta(\tau)} [\bigtriangledown_\theta log \pi_\theta(\tau) A_w(\tau)]\rightarrow\mbox{   Advantage Actor-Critic algorithm}\\ 
\bigtriangledown_\theta J(\theta)==E_{\tau \sim \pi_\theta(\tau)} [\bigtriangledown_\theta log \pi_\theta(\tau) \delta ]\rightarrow\mbox{   TD Actor-Critic algorithm}\\ 
\bigtriangledown_\theta J(\theta)==E_{\tau \sim \pi_\theta(\tau)} [\bigtriangledown_\theta log \pi_\theta(\tau) \delta e]\rightarrow\mbox{   TD(lambda) Actor-Critic algorithm}\\
...
$$

**TO BE CONTINUE**




























