---
layout:     post
title:      机器学习中的那些优化算法
subtitle:   优化算法の笔记（中） - Duality Method
date:       2021-07-31
author:     HC
header-img: img/ff43a4526beeb829ba673ab77c808890.jpeg
catalog: true
tags:
    - Optimization in machine learning
    - Duality Method
---

> 历史笔记迁移系列
>
> Word转markdown真的好累

# 0x02 Duality Method

## 0x02_1. Duality in Linear Programs and General Form

​	事物都有两面性，给定一个标准的凸优化问题，我们希望去构造一个点序列，通过不断的进行迭代，以逼近最小值。然而我们也可以想办法找出原问题的一个下界，然后不断的提升这个下界，以逼近最优值，这和EM算法的思想有一点相似，而**对偶（Duality）问题**就是基于这样的一个想法。下面以一些具体的例子进行入手。给定下面这个优化问题


$$
\min_{x,y} x+3y\\
s.t. x+y \ge 2\\
\quad x,y\ge 0
$$


​	那么，$x+3y$的下界是什么？当然是2，因为$x+3y=(x+y)+(2y)≥2$，所以Lower Bound是2，这个问题很简单，让我们在看一个更一般的例子，给定下面这个优化问题


$$
\min_{x,y} px+qy \quad where \quad p,q \ge 0\\
s.t. x+y \ge 2\\
\quad x,y\ge 0
$$


​	现在，我们令$a+b=p,a+c=q$,   $a,b,c≥0$ ，那么我们有$px+qy=(a+b)x+(a+c)y \ge 2a$。事实上，现在我们这个方法得到的下界2a，比上一个例子的方法得到的下界（2p或者2q，取决于p,q的大小）更加松弛，但是这也并不太影响，我们只需要不断地提升这个下界2a，我们把问题转化成下式（右）。原问题A就转化成了一个对偶问题B

![](/img/youhua21.png)

我们可以得到以下的性质

1. 值$A \ge B$，即**对偶问题的解一定是原问题解的一个下界**，具体的证明将在后面说明
2. 对偶问题的变量个数 等于 原问题的约束个数（包括等式约束与不等式约束）
3. 对偶问题的不等式约束个数 等于 原问题不等式约束个数
4. 如果两个问题的最优值满足A=B，则称他们是**强对偶的(Strong Duality)**，否则他们的差称为**Duality Gap**



​	那么对于一般的问题来说，如何求得所对应的对偶问题呢？这个回答当然是**Lagrange Duality**，这也是比较常用的方法。给定下式原问题，其中$c\in R^{n}, A \in R^{m,n}, b\in R^{m}, G\in R^{r,n}$


$$
A = \min_{x \in R^{n}} c^Tx\\
s.t. \quad Ax=b\\
\quad Gx \le h
$$


​	首先构造他的拉格朗日乘子$L(x,u,v)=c^{T}x+u(Ax-b)+v(Gx-h)$，这里的$v,u$满足$v\ge 0$，若$x_{0}$是满足原问题约束条件的可行点，则必有下式成立


$$
c^Tx_0 \ge L(x_0,u,v) = c^Tx_0 + \underset{等于零}{u(Ax_0-b)} + \underset{小于零}{v(Gx_0-h)} \ge \min_x L(x,u,v) = L(u,v)
$$


​	也就是说，通过拉格朗日法构造出来的$L(u,v)$一定是原问题的下界，如果我们用$x^*$来表示最优解，那么有下式成立。


$$
c^Tx^* \ge L(x^*,u,v) \ge \min_x L(x,u,v) = L(u,v)
$$


我们把它形式化成对偶的样子，先把拉格朗日乘子合并同类项一下，然后对$x$求min


$$
L(x,u,v)=(A^Tu+c+G^Tv)x - b^Tu-h^Tv\\
L(u,v) = \min_x L(x,u,v) = \left\{\begin{matrix}
-b^Tu-h^Tv \quad if \quad c=-A^Tu-G^Tv
\\ 
-\infty \quad otherwise
\end{matrix}\right.
$$


得到对偶问题，其中$u\in R^{m},v \in R^{r}$。注意这里是max，因为我们要提升下界


$$
\max_{u,v} -b^Tu-h^tv \\
s.t. c=-A^Tu-G^Tv\\
\quad u \ge 0
$$


总的来说，对于一个general的优化问题来说


$$
\min f(x)\\
s.t. \quad h_i(x) \le 0, i=1,2,...,m\\
\quad l_j(x)=0,j=1,2,...,r
$$




​	我们定义**Lagrangian**为

$$
L(x,u,v)=f(x)+\sum^{m}_{i=1} u_{i}h_{i}(x)+\sum_{j=1}^{r}v_{j}l_{j}(x)，注意u_{i} \ge 0, v_{j} \in R
$$

对于任意可行点x，我们都有$L(x,u,v) \le f(x)$。定义**Lagrange dual function**为$g(u,v)=\min_{x} L(x,u,v)$，我们有


$$
f^* \ge L(x,u,v) \ge \min_x L(x,u,v) = g(u,v)
$$


其对偶形式是下式，所以对偶形式是关于Lagrangian的min-max问题


$$
\max_{u,v} g(u,v) \quad s.t. \quad u \ge 0
$$


​	需要注意的是，**对偶问题一定是凸的**，因为他是关于$u,v$的函数，而不是$x$。现在，我们假设上式的最优值是$g^\*$，这里可以保证$f^\* \ge g^\*$，这个定理称为**weak duality**，即使原问题不是凸的，也是可以保证的。当然，有了weak duality，一定就有**strong duality**，即满足$f^\* =g^\*$，这时候原问题的解，就等价于对偶问题的解啦。为了要得到这个strong duality，需要函数满足一个条件**Slater’s Condition**，这个条件还算比较仁慈，他说的是：如果原问题是凸的，并且他的dom中至少有一点$x$是严格可行的（即$h_{i} (x)<0，l_{i} (x)=0,   对任意 i成立$），那么这个问题能保证strong duality。

​	另外，对于LP问题来说，对偶问题的对偶是原问题，而且，如果原问题是可行的，或者对偶问题是可行的话，Strong duality是可以保证的。

​	要是strong duality不能保证，那么给定原问题的可行点x和对偶问题的可行点u,v，称$f(x)-g(u,v)$是**duality gap**。又由于$f(x^* )≥g(u,v)$，所以有


$$
f(x) - f(x^*) \le f(x) - g(u,v)
$$


​	因此，当duality gap是0的时候，我们就达到了原问题的最优解。当duality gap不等于0的时候，上面这个不等式可以为我们提供一个迭代准则，即当$f(x)-g(u,v)≤ε$，我们能大概得知与最优值的差距。现在来看2个例子，分别是很常用到的SVM问题和Dual Norms问题。

​	**例子1-SVM**

支持向量机SVM是一个过分经典的模型了，这里就不赘述。给定SVM的目标函数


$$
\min_{\beta,\beta_0,\delta} \frac{1}{2} \|\beta\|_2^2 + C \sum_{i=1}^n \delta_{i} \\
s.t. \quad -\delta_i \le 0,i=1,2,...,n\\
\quad 1-\delta_i-y_i(x_i^T\beta + \beta_0) \le 0, i=1,2,...,n
$$


现在来求他的对偶，引入dual变量$v_{i} \ge 0,w_{i} \ge 0$，整理一下


$$
L(\beta,\beta_0,\delta,v,w) = \frac{1}{2} \|\beta\|^2_2 + C\sum_{i=1}^n\delta_i - \sum_{i=1}^nv_i \delta_i + \sum_{i=1}^{n} w_i (1-\delta_i-y_i(x_i^T\beta+\beta_0)) \\
\quad = \frac{1}{2} \|\beta\|^2_2 - \sum_{i=1}^n w_iy_ix_i\beta - \sum_{i=1}^n w_iy_i\beta_0 + \sum_{i=1}^n (C-v_i-w_i)\delta_i + \sum_{i=1}^n w_i
$$


现在，为了要求Lagrange dual function $g(w,v)$，我们需要对$β,β_{0},δ$求最小，首先看来$\beta$。


$$
\min_{\beta} \frac{1}{2} \|\beta\|^2_2 - \sum_{i=1}^n w_iy_ix_i\beta \\
= \frac{1}{2} \beta^T \beta- W^T \hat{X} \beta \quad where \quad \hat{X}的第i行是y_ix_i
$$


可以得到上式的解是$\beta=\hat{X}^{T}W$，在最小化下式（这里是关于$\delta_{i}$求导，而不是$\delta$）


$$
\min_{\beta_0} -\sum_{i=1}^n w_i y_i \beta_0 \\
\min_{\delta} \sum_{i=1}^n (C-v_i-w_i)\delta_i
$$


得到$\sum_{i=1}^{n}w_{i}y_{i}=0, C-v_{i}-w_{i}=0$，由于$v_{i} \ge 0$，所以$w_{i} \le C$。现在把所有的东西都带回到$L$函数中，得到


$$
g(w,v) = g(w) = -\frac{1}{2}W^T\hat{X}\hat{X}^TW + W^T1
$$


所以，SVM的对偶形式是这个：


$$
\max_{w} -\frac{1}{2}W^T\hat{X}\hat{X}^TW + W^T1 \\
s.t. \quad 0\le w_i \le C, i=1,2,...,n\\
\quad W^Ty=0
$$


​	可以看出这个问题是满足Slater’s Condition的，比如$w_{i}=0$，所以对偶问题的解，就是原问题的解，而且这个形式好解多了，这也是对偶的带来的便利。值得一说的是，在求$g(w,v)$的过程中所得到的那些条件，如$β=\hat{X}^{T} W$，也正是后面要说的KKT条件的一部分，这其实并不是巧合。

​	**例子2-Dual Norm**

​	给定任意Norm记为$\|*\|_{+}$，那么dual norm定义为：

$$
\|x\|_{d}=\max_{\|z\|_{+} \le 1} z^{T}x
$$

可以看下面的两个例子，更加的直观

![](/img/youhua22.png)

​	至于norm的对偶norm为啥是这个形式，这里就不证（黏）明（贴）了，它需要用到一个叫Holder不等式的东西。不过，关于Dual Norm还是有一些结论可以看下的

1. 如果$\|x\|$是一个norm，$\|x\|\_{d}$是它的对偶norm，那么满足$abs(z^{T}x) \le \|z\| \|x\|\_{d}$
2. $\|x\|_{dd} = \|x\|$
3. Sub-differential of Norm，其中$\theta$是一个norm，$\theta^{*}$对偶norm。{$z;asdf$}表示满足asdf的所有z


$$
\partial \theta(w) \left\{\begin{matrix}
\{z; \theta^*(z) \le 1\} \quad if \quad w=0
\\ 
\{z; \theta^*(z) \le 1 \quad and\quad z^Tw=\theta(w)\} \quad otherwise
\end{matrix}\right.
$$


## 0x02_2 KKT conditions

先上结论，给定下面的优化问题：


$$
\min f(x)\\
s.t. \quad h_i(x) \le 0, i=1,2,...,m\\
\quad l_j(x)=0,j=1,2,...,r
$$


它的**KKT条件（Karush-Kuhn-Tucker Conditions）**是由四个部分组成的，分别是


$$
Stationary：0 \in \partial f(x) + \sum_{i=1}^m u_i\partial h_i(x) + \sum_{j=1}^r v_i\partial l_i(x) \\
Complementary：u_{i}h_{i}(x)=0 \quad \forall i\\
Primal\quad Feasibility：h_i(x) \le 0, l_j(x)=0, \quad \forall i,j \\
Dual\quad Feasibility：u_i \ge 0 \quad \forall i
$$


​	如果说这个**优化问题具有强对偶性，那么有，符合KKT条件的点$x,u,v \leftrightarrow x,u,v$是原问题和对偶问题的解**。原因在于，强对偶性保证了其duality gap为零，则有


$$
f(x^*) = g(u^*,v^*) = \min_{x}f(x) + \sum_{i=1}^m u_i^* h_i(x) + \sum_{j=1}^r v_j^*l_j(x) \\
\le f(x^*) + \sum_{i=1}^m u_i^* h_i(x^*) + \sum_{j=1}^r v_j^*l_j(x^*) \\
\le f(x^*)
$$


​	因为上式中的不等式都应该是等式。所以，第一个不等式告诉我们，$x^\*$能最小化$L(x ,u^\*,v^\*)$，$即0 \in \partial_{x} L(x^{\*},u^{\*},v^{\*})$，也就是stationary condition成立；第二个不等式告诉我们，$u_{i}^{\*} h_{i} (x^{\*})=0$，也就是complementary condition，最后primal/dual feasibility condition当然也成立，不然解都不是可行的了。反过来，当满足KKT条件的时候，也可以反推出上式成立。另外当原问题是无约束优化的时候，KKT条件就相当于是subgradient optimality condition。

​	回一下上一节提到的SVM例子，我们在求Dual的时候已经得到Stationary，现在再补充其他条件


$$
\min_{\beta,\beta_0,\delta} \frac{1}{2} \|\beta\|_2^2 + C \sum_{i=1}^n \delta_{i} \\
s.t. \quad -\delta_i \le 0,i=1,2,...,n\\
\quad 1-\delta_i-y_i(x_i^T\beta + \beta_0) \le 0, i=1,2,...,n
$$


Stationary：$\sum_{i=1}^{n}w_{i}y_{i}=0, C-v_{i}-w_{i}=0, \beta=\hat{X}^{T}W=\sum_{i=1}^{n}w_{i}x_{i}y_{i}$

Complementary：$v_{i}\delta_{i}=0, w_{i}(1-\delta_{i}-y_{i}(x_{i}^{T}\beta+\beta_{0}))$

有条件可以看出，$w_{i} \in [0,C]$，那么可以定义满足这样的$w_{i}$对应的点叫做**支持向量**

![](/img/youhua23.png)

1. 如果$0<\delta_{i}<1$，则，$v_{i}=0,w_{i}=C,1-\delta_{i}-y_{i}(x_{i}^{T}\beta+\beta_{0})=0$，也就是说，$y_{i}(x_{i}^{T}\beta+\beta_{0}) < 1$，点i落在间隔边界和超平面之间，分类正确
2. 如果$\delta_{i}=0$，则$w_{i} \in [0,C]$
   1. 若$v_{i} > 0, w_{i} \in (0, C]$，则$1-\delta_{i}-y_{i}(x_{i}^{T}\beta+\beta_{0})=0 \rightarrow y_{i}(x_{i}^{T}\beta+\beta_{0}) = 1$，点在间隔边界上面
   2. 若$v_{i} = 0, w_{i} =0$，则$1-\delta_{i}-y_{i}(x_{i}^{T}\beta+\beta_{0})=0 \rightarrow y_{i}(x_{i}^{T}\beta+\beta_{0}) > 1$，点不是支持向量
3. 如果$\delta_{i}=1$，则$v_{i}=0,w_{i}=C,y_{i}(x_{i}^{T}\beta+\beta_{0}) = 1$，点在超平面上
4. 如果$\delta_{i}>1$，则$v_{i}=0,w_{i}=C,y_{i}(x_{i}^{T}\beta+\beta_{0}) <0$，点在误分类。



​	所以，KKT条件不仅仅用于find solution，也能让我们全面的分析问题的解。值得一提的是，对于软间隔SVM问题来说，原问题和对偶问题都不是严格凸函数，所以这种情况下的SVM的解b是不唯一的，但是w是唯一的，而对于线性可分SVM来说，原问题是严格凸函数，对偶问题是非严格凸函数（这里可以看到，原问题是严格凸函数，并不意味着对偶问题是严格凸函数），解w和b都是唯一的。（喜欢的朋友，可以去看下这篇NIPS的论文，Uniqueness of the SVM Solution）

![](/img/youhua24.png)



## 0x02_3 Duality Uses and Correspondences

​	从上一节可以看出对偶的作用有两点，一个是借用dual gap来做算法迭代的停止准则，另一个是依赖在强对偶的情况下的KKT的条件。这一节旨在引入另一个概念 共轭函数（Conjugate Function），可以用来辅助对偶形式的构造。那啥是共轭函数呢。给定一个函数$f$，他的共轭函数定义为：


$$
f^*(y) = \max_{x} y^Tx - f(x)
$$


​	由于共轭函数是关于$y$的一些凸函数的逐点上确界，所以无论函数$f$是不是凸函数，他的共轭函数一定是凸函数。对于共轭函数而言，可以把它看做是函数$f(x)$与函数$y^{T} x$之间的最大间隔。如果$f$可微，在满足$y=∇f(x)$的点$x$处的差值最大。假设这个点为$x_{0}$，则有$y=\triangledown f(x\_{0}), f^\*(y) =  yx\_{0} - f(x\_{0})=\triangledown f(x_{0}) x\_{0} - f(x\_{0}) $，进一步化解为$\triangledown f(x_{0}) =\frac{f(x\_{0}) - (-f^\*(y))}{x\_{0} - 0}$

![](/img/youhua25.png)

​	我们已经说了，共轭函数的作用就在于 他能使我们更快的得到对偶问题！那我们先看看共轭函数有没有什么好的性质可以使用

1. 对于任意的$x,y$，满足$f(x) + f^{*}(y) \le x^{T}y$

2. 对于任意函数$f$，他的共轭函数的共轭函数，值不大于原函数：$f^{\*\*}(x) \le f(x)$

3. 如果函数$f$是凸的或者凹的，那上面两个性质都可以取等号

4. 指示函数的共轭函数是支持函数（Support Function），即$f(x)=I_{C}(x),f^{\*}=\max_{x \in C} y^{T} x$

5. 范数的共轭是指示函数

   ​
   $$
   f(x)=\|x\|, f^{*}(y)=\left\{\begin{matrix} 0 \quad if \quad \|y\|_* < 1 \\ \infty \quad otherwise \end{matrix}\right. = I_{\|y\|_* < 1}(y)\\
   where\quad \|y\|_* 是dual norm
   $$




​	那么conjugate function到底有啥好的呢，如何加快求解dual？下面是一个Lasso Dual的小例子，给定$y \in R^{n}, X \in R^{n,p}$


$$
\min_{\beta \in R^p} \frac{1}{2} \|y-X\beta \|_2^2 + \lambda \|\beta\|_1
$$


​	注意到，这里是无约束的问题，无法通过拉格朗日引入对偶问题，事实上这里会用到一个dual trick，引入额外的等式约束


$$
\min_{\beta \in R^p} \frac{1}{2} \|y-z \|_2^2 + \lambda \|\beta\|_1 \\
s.t. X\beta = z
$$


​	这样就可以搞拉格朗日咯，那就搞起来


$$
\min_{z,\beta} L(z,\beta,u) = \frac{1}{2} \|y-z\|_2^2 + \lambda \|\beta\|_1 + u^T(z-X\beta)\\
= \min_{z} \{ \frac{1}{2} \|y-z\|_2^2 + u^Tz \} + \min_{\beta} \{ \lambda \|\beta\|_1 - u^TX\beta \}
$$


​	第一项的min可以直接求导，完美，那就求呗，反正最后是$y-u=z$，带入得下式，（另外可以看出lasso的解一定会满足y-u=z=Xβ，而这个等式也是KKT条件之一）


$$
\frac{1}{2} \| y\|_2^2 - \frac{1}{2} \|y-u\|_2^2
$$


​	第二项，有lasso，完了，要求subgradient吗？当然大可不必，我们把这个式子捣鼓一下，就成了conjugate function咯


$$
\min_{\beta} \{ \lambda \|\beta\|_1 - u^TX\beta \} = -\lambda \max_{\beta} \frac{u^TX\beta}{\lambda} - \|\beta\|_1\\
= -\lambda \max_{\beta} \frac{(X^Tu)^T}{\lambda}\beta - \|\beta\|_1
$$


​	这是L1的conjugate function，当然等于一个指示函数，最后得


$$
\frac{1}{2} \| y\|_2^2 - \frac{1}{2} \|y-u\|_2^2 - I_{\|X^Tu\|_{\infty} \le \lambda}
$$


​	所以Lasso的对偶就是下式


$$
\max_u \frac{1}{2} \|y\|_2^2 -\frac{1}{2}\|y-u\|^2_2\\
s.t. \|X^Tu\|_{\infty} = (X^T)^{-1} \|u\|_{\infty}  \le \lambda
$$


​	上式中的约束条件可以看成是一个polyhedron $C$，则Lasso对偶问题转化为最小化点y到polyhedron $C$的最短距离，如下所示，其中$C$就是那个约束条件

![](/img/youhua26.png)



​	事实上，对于更一般的问题，共轭函数也有很好的特性


$$
\min_x f(x) + g(x) \leftrightarrow \min_{z,x} f(x) + g(z) \quad s.t. \quad z=x
$$


​	其对偶就可以直接写成下式


$$
\min_{z,x} L(x,z,u) = \min_{z,x} f(x) + g(z) + u^T(z-x) = -f^*(u) - g^*(-u) \\
\rightarrow \max_{u} -f^*(u) - g^*(-u)
$$




# 0x03 Second-order Method

## 0x03_1 Newton Method

## 0x03_2 Barrier Method

## 0x03_3 Primal-dual Interior Point Method

## 0x03_4 Proximal Newton Method



# 0x04 Others and Application

## 0x04_1 Generalized Lasso Problem

## 0x04_2 Dual Methods and ADMM

dual subgradient



dual deconposition



Augmented Lagrangian Method



ADMM



例子：

- Norm regularization
- lasso

## 0x04_3 Distributed  ADMM 

## 0x04_4 Coordinate Descent

例子

1. linear regression

2. lasso regression

3. box-constrained regression

4. svm

   ​

## 0x04_5 Conditional Gradient Method

Frank-Wolfe Method

例子：

1. losso
2. Lp norm

## 0x04_6 Cutting-plane Method

## 0x04_7 Non-convex Problem

